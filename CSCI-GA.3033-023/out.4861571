0 2.8349838256835938
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.834984
1 2.8139705657958984
2 2.779695749282837
3 2.7229530811309814
4 2.6821911334991455
5 2.618096351623535
6 2.508167266845703
7 2.4551498889923096
8 2.323610305786133
9 2.2258358001708984
10 2.0297491550445557
11 1.90312922000885
12 1.7518396377563477
13 1.946879267692566
14 1.857315182685852
15 1.9534492492675781
16 1.84088134765625
17 1.998367190361023
18 1.7189452648162842
19 1.8074144124984741
20 1.9379066228866577
21 1.842215895652771
22 1.5501981973648071
23 1.7874653339385986
24 1.6700294017791748
25 1.7612224817276
26 1.8401572704315186
27 1.7992645502090454
28 1.494808316230774
29 1.6690237522125244
30 1.6553318500518799
31 1.6985015869140625
32 1.6324049234390259
33 1.6104011535644531
34 1.64345383644104
35 1.7472217082977295
36 1.5463865995407104
37 1.6456109285354614
38 1.5548664331436157
39 1.5716521739959717
40 1.5723717212677002
41 1.6078927516937256
42 1.6521286964416504
43 1.544348955154419
44 1.4334895610809326
45 1.4679386615753174
46 1.7221388816833496
47 1.6196740865707397
48 1.687981367111206
49 1.608161211013794
50 1.529526948928833
51 1.5375639200210571
52 1.5823156833648682
53 1.504951000213623
54 1.3132973909378052
55 1.565920352935791
56 1.7744346857070923
57 1.5757801532745361
58 1.4717656373977661
59 1.6207537651062012
60 1.6792718172073364
61 1.6212775707244873
62 1.5663177967071533
63 1.5397541522979736
64 1.6154735088348389
65 1.5941047668457031
66 1.4684008359909058
67 1.5418928861618042
68 1.510713815689087
69 1.5584814548492432
70 1.4289841651916504
71 1.6273034811019897
72 1.4472802877426147
73 1.4270577430725098
74 1.3873730897903442
75 1.407716989517212
76 1.4055871963500977
77 1.3350905179977417
78 1.4947564601898193
79 1.527816891670227
80 1.3696266412734985
81 1.4536230564117432
82 1.5462325811386108
83 1.4914363622665405
84 1.2565674781799316
85 1.5141024589538574
86 1.6327136754989624
87 1.4704759120941162
88 1.3415474891662598
89 1.4410170316696167
90 1.547331690788269
91 1.6309415102005005
92 1.604035496711731
93 1.4737257957458496
94 1.464537501335144
95 1.341659665107727
96 1.4166030883789062
97 1.6396191120147705
98 1.5971635580062866
99 1.4159255027770996
100 1.3710317611694336
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.371032
101 1.5271693468093872
102 1.3968722820281982
103 1.3255575895309448
104 1.498123288154602
105 1.5834044218063354
106 1.430937647819519
107 1.4699761867523193
108 1.3889250755310059
109 1.5675891637802124
110 1.478916883468628
111 1.3772627115249634
112 1.5634952783584595
113 1.4885375499725342
114 1.2301042079925537
115 1.4253926277160645
116 1.3632904291152954
117 1.4252794981002808
118 1.2770826816558838
119 1.2780671119689941
120 1.4979166984558105
121 1.4411615133285522
122 1.3629441261291504
123 1.5100791454315186
124 1.2624921798706055
125 1.3565940856933594
126 1.3390804529190063
127 1.4456324577331543
128 1.1896189451217651
129 1.4151841402053833
130 1.276400089263916
131 1.3352487087249756
132 1.276821255683899
133 1.3017358779907227
134 1.3325423002243042
135 1.2223212718963623
136 1.2579143047332764
137 1.1547657251358032
138 1.4244258403778076
139 1.1328037977218628
140 1.3381009101867676
141 1.3787897825241089
142 1.4332693815231323
143 1.3338841199874878
144 1.3099567890167236
145 1.3313604593276978
146 1.3425700664520264
147 1.3158230781555176
148 1.4430034160614014
149 1.2144825458526611
150 1.20492422580719
151 1.3479527235031128
152 1.3337881565093994
153 1.2596889734268188
154 1.3564980030059814
155 1.3928375244140625
156 1.4218201637268066
157 1.205651044845581
158 1.4123272895812988
159 1.2169289588928223
160 1.3488800525665283
161 1.1583280563354492
162 1.1269510984420776
163 1.1460204124450684
164 1.2464462518692017
165 1.1914023160934448
166 1.1306812763214111
167 1.2668412923812866
168 1.213431477546692
169 1.208827018737793
170 1.344123125076294
171 1.2046104669570923
172 1.241241455078125
173 1.1762797832489014
174 1.1459932327270508
175 1.1401664018630981
176 1.313284158706665
177 1.439234733581543
178 1.358941674232483
179 1.3492059707641602
180 1.2300978899002075
181 1.1949586868286133
182 1.2373883724212646
183 1.333231806755066
184 1.23927640914917
185 1.1930558681488037
186 1.1278338432312012
187 1.2223683595657349
188 1.253204584121704
189 1.3242107629776
190 1.2691373825073242
191 1.2774436473846436
192 1.3924391269683838
193 1.3830742835998535
194 1.3010640144348145
195 1.3132761716842651
196 1.0883374214172363
197 1.281349539756775
198 1.1951905488967896
199 1.3527177572250366
.............
32.294456565752625 78.86354801058769 20.45674203336239
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.35112726688385
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.351127
1 1.4143704175949097
2 1.3390839099884033
3 1.2575284242630005
4 1.2406914234161377
5 1.4818652868270874
6 1.0972416400909424
7 1.3519781827926636
8 1.190126657485962
9 1.3631689548492432
10 1.0798380374908447
11 1.2671912908554077
12 1.181706190109253
13 1.2019691467285156
14 1.213903784751892
15 1.2814298868179321
16 1.276615023612976
17 1.2629650831222534
18 1.3460866212844849
19 1.1996170282363892
20 1.3701227903366089
21 1.278530240058899
22 1.193921685218811
23 1.3579925298690796
24 1.311549425125122
25 1.1629313230514526
26 1.3373630046844482
27 1.2827138900756836
28 1.1638247966766357
29 1.1455836296081543
30 1.1667670011520386
31 1.2495514154434204
32 1.2188007831573486
33 1.259829044342041
34 1.1814802885055542
35 1.3177382946014404
36 1.1446716785430908
37 1.1733062267303467
38 1.1413975954055786
39 1.3188329935073853
40 1.0840058326721191
41 1.2822996377944946
42 1.397383451461792
43 1.0925291776657104
44 1.0888209342956543
45 1.208065152168274
46 1.336106538772583
47 1.208848476409912
48 1.445095181465149
49 1.2826634645462036
50 1.3006657361984253
51 1.2021175622940063
52 1.2676390409469604
53 1.1933881044387817
54 1.0310536623001099
55 1.093570351600647
56 1.3107551336288452
57 1.1849453449249268
58 1.2522590160369873
59 1.2802841663360596
60 1.296776533126831
61 1.1961297988891602
62 1.310053825378418
63 1.1955246925354004
64 1.1799317598342896
65 1.1923933029174805
66 1.152347207069397
67 1.4194563627243042
68 1.1324583292007446
69 1.267045021057129
70 1.24177885055542
71 1.4489208459854126
72 1.215160608291626
73 1.1248911619186401
74 1.2355222702026367
75 1.260265588760376
76 1.2036194801330566
77 1.031738519668579
78 1.2571543455123901
79 1.2871159315109253
80 1.3176748752593994
81 1.2631580829620361
82 1.257592797279358
83 1.240012764930725
84 0.9842888116836548
85 1.2979919910430908
86 1.4097410440444946
87 1.19978666305542
88 1.0353727340698242
89 1.2316768169403076
90 1.3461560010910034
91 1.4258135557174683
92 1.352473497390747
93 1.2477161884307861
94 1.3036073446273804
95 1.1119306087493896
96 1.1661993265151978
97 1.3864920139312744
98 1.2161552906036377
99 1.248652458190918
100 1.1240204572677612
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.124020
101 1.3243622779846191
102 1.0866131782531738
103 1.1068285703659058
104 1.2722254991531372
105 1.243263602256775
106 1.1807260513305664
107 1.2485243082046509
108 1.1441251039505005
109 1.346197247505188
110 1.2614777088165283
111 1.166092872619629
112 1.347510814666748
113 1.2376559972763062
114 1.1132506132125854
115 1.2984493970870972
116 1.2151857614517212
117 1.227349042892456
118 1.1875921487808228
119 1.108963966369629
120 1.3005539178848267
121 1.2989252805709839
122 1.236443281173706
123 1.3804343938827515
124 1.142309546470642
125 1.2139095067977905
126 1.2228760719299316
127 1.3088377714157104
128 1.0079325437545776
129 1.3188894987106323
130 1.1231378316879272
131 1.1896342039108276
132 1.1592096090316772
133 1.1252336502075195
134 1.1012022495269775
135 1.0445735454559326
136 1.1624330282211304
137 1.0030261278152466
138 1.3174524307250977
139 1.0329911708831787
140 1.12775456905365
141 1.233658790588379
142 1.2740978002548218
143 1.238165259361267
144 1.192623257637024
145 1.1806467771530151
146 1.254478931427002
147 1.1554975509643555
148 1.3190242052078247
149 1.0355465412139893
150 1.0946125984191895
151 1.1984288692474365
152 1.2187527418136597
153 1.135817289352417
154 1.1749145984649658
155 1.1853036880493164
156 1.3532779216766357
157 1.1212345361709595
158 1.2428702116012573
159 1.1045372486114502
160 1.225945234298706
161 1.0428128242492676
162 0.973480761051178
163 1.0354784727096558
164 1.135330319404602
165 1.1053802967071533
166 1.0265578031539917
167 1.130547285079956
168 1.0722090005874634
169 1.0311546325683594
170 1.2399983406066895
171 1.1244043111801147
172 1.078945279121399
173 1.0915082693099976
174 1.0350582599639893
175 1.0796204805374146
176 1.1947407722473145
177 1.2952911853790283
178 1.294465184211731
179 1.2790271043777466
180 1.1212948560714722
181 1.0627976655960083
182 1.1453803777694702
183 1.2029201984405518
184 1.0311229228973389
185 1.0767145156860352
186 1.0042861700057983
187 1.1381062269210815
188 1.149216651916504
189 1.1830500364303589
190 1.1469882726669312
191 1.0971875190734863
192 1.1883448362350464
193 1.2921669483184814
194 1.2855068445205688
195 1.03667414188385
196 0.9685086011886597
197 1.1586648225784302
198 1.0586719512939453
199 1.1994919776916504
.............
99.24794217571616 241.30270192027092 63.06493450701237
.............
0 1.1814216375350952
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.181422
1 1.312868595123291
2 1.2573275566101074
3 1.1455930471420288
4 1.152788519859314
5 1.3739917278289795
6 0.928009033203125
7 1.228987693786621
8 1.0410951375961304
9 1.2660133838653564
10 0.9697620868682861
11 1.1439294815063477
12 1.1087427139282227
13 1.0764977931976318
14 1.1544300317764282
15 1.1280251741409302
16 1.1488858461380005
17 1.02518892288208
18 1.2867203950881958
19 1.116712212562561
20 1.2493867874145508
21 1.1456456184387207
22 1.0374383926391602
23 1.236385703086853
24 1.1555768251419067
25 1.0607006549835205
26 1.2121847867965698
27 1.1617785692214966
28 1.0889137983322144
29 1.0690666437149048
30 1.059682011604309
31 1.0920509099960327
32 1.0719704627990723
33 1.1528332233428955
34 1.1085941791534424
35 1.226036787033081
36 0.9932266473770142
37 1.0956029891967773
38 1.0346903800964355
39 1.203912615776062
40 1.0074834823608398
41 1.1791069507598877
42 1.352015495300293
43 0.9831626415252686
44 0.9775986671447754
45 1.1255148649215698
46 1.2969616651535034
47 1.1548110246658325
48 1.3323239088058472
49 1.23219895362854
50 1.2454023361206055
51 1.1384209394454956
52 1.1973177194595337
53 1.0920411348342896
54 0.9239152669906616
55 1.0580445528030396
56 1.2333935499191284
57 1.1138696670532227
58 1.1936075687408447
59 1.2245205640792847
60 1.2317898273468018
61 1.1702758073806763
62 1.2207084894180298
63 1.0136390924453735
64 1.1069058179855347
65 1.156174898147583
66 1.0033609867095947
67 1.1971486806869507
68 1.0747218132019043
69 1.1573078632354736
70 1.0354610681533813
71 1.3109121322631836
72 1.1283135414123535
73 1.037992238998413
74 1.2019329071044922
75 1.091115951538086
76 1.01481294631958
77 0.9602344036102295
78 1.1985639333724976
79 1.1426790952682495
80 1.140222191810608
81 1.1053202152252197
82 1.1719717979431152
83 1.038944959640503
84 0.9255683422088623
85 1.1775282621383667
86 1.280439853668213
87 1.0395519733428955
88 0.9100548624992371
89 1.09257972240448
90 1.2640303373336792
91 1.3747341632843018
92 1.2835285663604736
93 1.1273216009140015
94 1.1809451580047607
95 1.015010118484497
96 1.1218525171279907
97 1.3502850532531738
98 1.126937985420227
99 1.1910741329193115
100 1.003322720527649
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.003323
101 1.2627573013305664
102 1.0872719287872314
103 1.0989673137664795
104 1.1696829795837402
105 1.162163496017456
106 1.1521188020706177
107 1.1547027826309204
108 1.0102605819702148
109 1.3003628253936768
110 1.1377179622650146
111 1.0532995462417603
112 1.183220624923706
113 1.2169182300567627
114 0.9788327813148499
115 1.1859219074249268
116 1.207308292388916
117 1.177424430847168
118 0.9893443584442139
119 1.0128787755966187
120 1.2459713220596313
121 1.257710337638855
122 1.112450122833252
123 1.2991350889205933
124 1.0761088132858276
125 1.1981829404830933
126 1.2145354747772217
127 1.2370952367782593
128 0.9625606536865234
129 1.2331503629684448
130 1.1129451990127563
131 1.0837138891220093
132 1.0877104997634888
133 1.0334224700927734
134 1.04410982131958
135 1.001961588859558
136 1.1310955286026
137 0.9139085412025452
138 1.2689452171325684
139 1.1185097694396973
140 1.0674549341201782
141 1.2235349416732788
142 1.1972193717956543
143 1.153695821762085
144 1.1951475143432617
145 1.1416667699813843
146 1.2064653635025024
147 1.048787236213684
148 1.2136527299880981
149 0.939839780330658
150 1.1295580863952637
151 1.119064211845398
152 1.1655926704406738
153 1.108530044555664
154 1.139129638671875
155 1.126380205154419
156 1.3286216259002686
157 1.063273310661316
158 1.1634336709976196
159 1.0668535232543945
160 1.1764243841171265
161 0.9610035419464111
162 0.8968501091003418
163 0.9744666814804077
164 1.0545275211334229
165 1.064716100692749
166 0.9491729140281677
167 1.0572489500045776
168 0.9836098551750183
169 0.9698740243911743
170 1.1668047904968262
171 1.0218089818954468
172 1.0182417631149292
173 1.022459864616394
174 1.000058650970459
175 0.9723759293556213
176 1.1527204513549805
177 1.2090747356414795
178 1.2055118083953857
179 1.1727137565612793
180 1.0610147714614868
181 1.0199671983718872
182 1.0870846509933472
183 1.1272159814834595
184 0.983253002166748
185 1.0251277685165405
186 0.9357529282569885
187 1.0787689685821533
188 1.070042610168457
189 1.1369638442993164
190 1.0726698637008667
191 1.0331759452819824
192 1.137565016746521
193 1.2502315044403076
194 1.2226296663284302
195 0.9578873515129089
196 0.9258736371994019
197 1.0815470218658447
198 1.0127851963043213
199 1.1624324321746826
.............
231.2935018055141 562.8214914761484 146.1732344571501
.............
0 1.116159439086914
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.116159
1 1.2846564054489136
2 1.2174208164215088
3 1.126334309577942
4 1.1004730463027954
5 1.3275916576385498
6 0.8990021347999573
7 1.1730608940124512
8 0.9845726490020752
9 1.2085769176483154
10 0.9136856198310852
11 1.0725325345993042
12 1.0543391704559326
13 1.014754295349121
14 1.1382346153259277
15 1.0992767810821533
16 1.0897352695465088
17 0.9462970495223999
18 1.270951509475708
19 1.0585099458694458
20 1.2035737037658691
21 1.0882346630096436
22 0.9699497222900391
23 1.1795527935028076
24 1.105432391166687
25 1.011794924736023
26 1.1942431926727295
27 1.097435712814331
28 1.0952030420303345
29 1.0638272762298584
30 1.0320028066635132
31 1.1095447540283203
32 1.028281569480896
33 1.1257458925247192
34 1.0711572170257568
35 1.180689811706543
36 1.0236420631408691
37 1.0931367874145508
38 1.009434461593628
39 1.1527312994003296
40 0.9660208225250244
41 1.1475555896759033
42 1.3134268522262573
43 0.9133365750312805
44 0.9567238092422485
45 1.0979375839233398
46 1.2286242246627808
47 1.1143871545791626
48 1.284005880355835
49 1.1415355205535889
50 1.1473078727722168
51 1.0733306407928467
52 1.1859108209609985
53 1.0256978273391724
54 0.9143027663230896
55 1.0249611139297485
56 1.1457101106643677
57 1.0562796592712402
58 1.115809440612793
59 1.1672978401184082
60 1.1756091117858887
61 1.1220637559890747
62 1.15016508102417
63 1.0013762712478638
64 1.0725942850112915
65 1.1006944179534912
66 0.9652680158615112
67 1.1947526931762695
68 1.0251471996307373
69 1.1287102699279785
70 1.0321277379989624
71 1.2842005491256714
72 1.0994491577148438
73 1.0497208833694458
74 1.1749345064163208
75 1.0840080976486206
76 0.9726477861404419
77 0.9445958137512207
78 1.1925020217895508
79 1.111201286315918
80 1.0700243711471558
81 1.0770169496536255
82 1.1391701698303223
83 0.9853507280349731
84 0.9529995918273926
85 1.1409379243850708
86 1.2661765813827515
87 1.0037875175476074
88 0.9018897414207458
89 1.0753799676895142
90 1.262702465057373
91 1.3603860139846802
92 1.2674520015716553
93 1.12539803981781
94 1.1729462146759033
95 0.9951879382133484
96 1.1280851364135742
97 1.2886701822280884
98 1.095253348350525
99 1.1650656461715698
100 0.9762798547744751
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.976280
101 1.2228814363479614
102 1.0702635049819946
103 1.1062012910842896
104 1.137436032295227
105 1.1239523887634277
106 1.1008901596069336
107 1.1256986856460571
108 0.9955002665519714
109 1.2852510213851929
110 1.0999970436096191
111 1.030924677848816
112 1.143937349319458
113 1.1858935356140137
114 0.9516034722328186
115 1.15744948387146
116 1.1617748737335205
117 1.1336168050765991
118 0.9772520661354065
119 0.9872006177902222
120 1.2036343812942505
121 1.2349495887756348
122 1.0708341598510742
123 1.263149619102478
124 1.043940544128418
125 1.1807440519332886
126 1.2023507356643677
127 1.2168655395507812
128 0.9429718852043152
129 1.181776762008667
130 1.098503828048706
131 1.0431249141693115
132 1.0366288423538208
133 0.9853404760360718
134 1.0276260375976562
135 0.9805697798728943
136 1.1296674013137817
137 0.8816764950752258
138 1.2438911199569702
139 1.1069271564483643
140 1.044014811515808
141 1.2338504791259766
142 1.1703267097473145
143 1.1201181411743164
144 1.1908267736434937
145 1.132340431213379
146 1.193191647529602
147 1.0061733722686768
148 1.1876552104949951
149 0.9110822081565857
150 1.1213641166687012
151 1.151482343673706
152 1.1562702655792236
153 1.0766401290893555
154 1.0948264598846436
155 1.0921080112457275
156 1.3017849922180176
157 1.0712286233901978
158 1.1532270908355713
159 1.0955299139022827
160 1.1864241361618042
161 0.9539468884468079
162 0.913788378238678
163 1.0079346895217896
164 1.0558652877807617
165 1.0424085855484009
166 0.9277479648590088
167 1.0366138219833374
168 0.9853162169456482
169 0.9570299386978149
170 1.1597847938537598
171 1.009737491607666
172 0.9901202321052551
173 0.9930698275566101
174 0.9956744313240051
175 0.9768866896629333
176 1.1596531867980957
177 1.1856938600540161
178 1.1876803636550903
179 1.1695846319198608
180 1.0482250452041626
181 1.009456753730774
182 1.0606480836868286
183 1.0944255590438843
184 0.9642437696456909
185 1.0173088312149048
186 0.9182445406913757
187 1.0702180862426758
188 1.0515254735946655
189 1.123671531677246
190 1.0506975650787354
191 1.0143986940383911
192 1.1115350723266602
193 1.2056477069854736
194 1.1911654472351074
195 0.929365336894989
196 0.9117799997329712
197 1.0467989444732666
198 0.9876419305801392
199 1.1393208503723145
.............
495.4978642500937 1205.1908991262317 312.6886066198349
.............
0 1.0874850749969482
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.087485
1 1.273638367652893
2 1.1955833435058594
3 1.1134326457977295
4 1.0573025941848755
5 1.2956205606460571
6 0.882676362991333
7 1.1547092199325562
8 0.9687942266464233
9 1.1830570697784424
10 0.9078060388565063
11 1.044121265411377
12 1.0406999588012695
13 1.0060951709747314
14 1.1293230056762695
15 1.068315863609314
16 1.0572251081466675
17 0.9153820872306824
18 1.2658179998397827
19 1.0229315757751465
20 1.185661792755127
21 1.0622128248214722
22 0.9341286420822144
23 1.1491485834121704
24 1.0863585472106934
25 0.9833414554595947
26 1.1890645027160645
27 1.0520812273025513
28 1.0603021383285522
29 1.0457533597946167
30 0.9708526134490967
31 1.0443294048309326
32 1.002506136894226
33 1.082506775856018
34 1.0331858396530151
35 1.1693841218948364
36 0.9713838696479797
37 1.0663182735443115
38 0.9900521039962769
39 1.1761287450790405
40 0.9428536295890808
41 1.0921504497528076
42 1.283664584159851
43 0.8905021548271179
44 0.9377678632736206
45 1.0773911476135254
46 1.2438699007034302
47 1.0453944206237793
48 1.2754863500595093
49 1.1204721927642822
50 1.1819947957992554
51 1.0852187871932983
52 1.1150151491165161
53 1.0236241817474365
54 0.905873715877533
55 0.9973457455635071
56 1.1177163124084473
57 1.0502510070800781
58 1.1186774969100952
59 1.143489122390747
60 1.137800931930542
61 1.1208745241165161
62 1.1481226682662964
63 0.9588740468025208
64 1.0382933616638184
65 1.0545724630355835
66 0.9430887699127197
67 1.1604913473129272
68 1.013304591178894
69 1.0889567136764526
70 0.9691368341445923
71 1.238255262374878
72 1.0888062715530396
73 1.0167357921600342
74 1.1406711339950562
75 1.048397183418274
76 0.9667053818702698
77 0.9306052327156067
78 1.1627726554870605
79 1.0724679231643677
80 1.053184151649475
81 1.060103416442871
82 1.120134949684143
83 0.9623132348060608
84 0.8887362480163574
85 1.1099121570587158
86 1.2099957466125488
87 0.968909502029419
88 0.8897189497947693
89 1.0441136360168457
90 1.2340761423110962
91 1.3334277868270874
92 1.2513779401779175
93 1.0858832597732544
94 1.1422057151794434
95 0.9588344097137451
96 1.0777150392532349
97 1.2181217670440674
98 1.0781629085540771
99 1.12260103225708
100 0.9316902160644531
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.931690
101 1.180586338043213
102 1.0221937894821167
103 1.0568053722381592
104 1.1112457513809204
105 1.086121916770935
106 1.0523197650909424
107 1.098594307899475
108 0.9827809929847717
109 1.2622714042663574
110 1.077288031578064
111 1.0119116306304932
112 1.0863161087036133
113 1.182775855064392
114 0.9348872303962708
115 1.1336079835891724
116 1.1234982013702393
117 1.0876038074493408
118 0.9613239765167236
119 0.946908175945282
120 1.1390191316604614
121 1.2343446016311646
122 1.0279866456985474
123 1.1771905422210693
124 1.0308310985565186
125 1.145033359527588
126 1.1657167673110962
127 1.1562331914901733
128 0.9442753791809082
129 1.1070663928985596
130 1.0685356855392456
131 1.0218517780303955
132 0.9660627245903015
133 0.9456556439399719
134 0.9871747493743896
135 0.957632303237915
136 1.098517656326294
137 0.8583700060844421
138 1.209584355354309
139 1.0364270210266113
140 0.9916578531265259
141 1.2156364917755127
142 1.1371781826019287
143 1.0872390270233154
144 1.1353415250778198
145 1.0844261646270752
146 1.1685357093811035
147 0.9777191281318665
148 1.1731088161468506
149 0.8739719390869141
150 1.0626475811004639
151 1.1284488439559937
152 1.0958822965621948
153 1.0627425909042358
154 1.091120719909668
155 1.0346509218215942
156 1.2792621850967407
157 1.0632429122924805
158 1.083738088607788
159 1.0672842264175415
160 1.1516624689102173
161 0.9377526640892029
162 0.8977667093276978
163 1.0346425771713257
164 1.0378183126449585
165 1.0126254558563232
166 0.9482372999191284
167 1.0326805114746094
168 0.9532087445259094
169 0.9423429369926453
170 1.130174994468689
171 0.9805261492729187
172 0.9797597527503967
173 0.9519914984703064
174 0.9689602851867676
175 0.9213676452636719
176 1.128282904624939
177 1.17445707321167
178 1.1595849990844727
179 1.1351605653762817
180 1.0278911590576172
181 0.9625641703605652
182 1.0649257898330688
183 1.0689080953598022
184 0.9587392210960388
185 1.0022996664047241
186 0.8814801573753357
187 1.031156301498413
188 1.027842402458191
189 1.0809102058410645
190 1.0157113075256348
191 0.9907625317573547
192 1.0637032985687256
193 1.169419527053833
194 1.1471242904663086
195 0.907162606716156
196 0.8891313672065735
197 1.005811095237732
198 0.9576302170753479
199 1.0974202156066895
.............
1025.8742565978318 2492.7455469798297 646.1556837633252
.............
4 1025.8742565978318 2492.7455469798297 646.1556837633252
.............
