0 2.8547451496124268
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.854745
1 2.831246852874756
2 2.799067974090576
3 2.729199171066284
4 2.703643798828125
5 2.611001491546631
6 2.484163999557495
7 2.408348321914673
8 2.2422759532928467
9 2.177797794342041
10 1.9708356857299805
11 1.8760368824005127
12 1.7500925064086914
13 2.0043039321899414
14 1.8766587972640991
15 1.9150794744491577
16 1.7784823179244995
17 1.876211404800415
18 1.7956559658050537
19 1.8848044872283936
20 1.9845455884933472
21 1.793753981590271
22 1.514443039894104
23 1.8624681234359741
24 1.6720541715621948
25 1.8250536918640137
26 1.879948377609253
27 1.7926268577575684
28 1.4947707653045654
29 1.6880961656570435
30 1.671898603439331
31 1.6871241331100464
32 1.6826478242874146
33 1.6359245777130127
34 1.6355226039886475
35 1.7537596225738525
36 1.5544097423553467
37 1.6544538736343384
38 1.5388952493667603
39 1.5771862268447876
40 1.5919520854949951
41 1.6280819177627563
42 1.6639362573623657
43 1.540949821472168
44 1.4330497980117798
45 1.4799154996871948
46 1.7312415838241577
47 1.6123582124710083
48 1.6855961084365845
49 1.6034479141235352
50 1.5254935026168823
51 1.532446026802063
52 1.581598162651062
53 1.4826604127883911
54 1.290101170539856
55 1.5832901000976562
56 1.777174949645996
57 1.5586766004562378
58 1.4727294445037842
59 1.6590520143508911
60 1.688461184501648
61 1.5993624925613403
62 1.5806453227996826
63 1.5841035842895508
64 1.6665817499160767
65 1.6266604661941528
66 1.462510347366333
67 1.551700234413147
68 1.5311449766159058
69 1.6030020713806152
70 1.4491366147994995
71 1.6317219734191895
72 1.4382522106170654
73 1.4365066289901733
74 1.4087882041931152
75 1.410588026046753
76 1.4107776880264282
77 1.3363782167434692
78 1.4886436462402344
79 1.5199896097183228
80 1.364722728729248
81 1.4423017501831055
82 1.5314266681671143
83 1.4762322902679443
84 1.2494373321533203
85 1.4994003772735596
86 1.623932957649231
87 1.4619128704071045
88 1.327690839767456
89 1.4248487949371338
90 1.5380412340164185
91 1.6194003820419312
92 1.5941342115402222
93 1.4589070081710815
94 1.4575793743133545
95 1.3254361152648926
96 1.3962242603302002
97 1.6258995532989502
98 1.5792036056518555
99 1.4007686376571655
100 1.3524318933486938
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.352432
101 1.5104925632476807
102 1.3695423603057861
103 1.30829656124115
104 1.4807610511779785
105 1.5562494993209839
106 1.4051820039749146
107 1.4556055068969727
108 1.3750278949737549
109 1.547883152961731
110 1.4522337913513184
111 1.3480526208877563
112 1.5518486499786377
113 1.4749021530151367
114 1.2043172121047974
115 1.3964505195617676
116 1.3455685377120972
117 1.404017448425293
118 1.2593722343444824
119 1.2560275793075562
120 1.4741356372833252
121 1.4119068384170532
122 1.3428955078125
123 1.4816100597381592
124 1.2422945499420166
125 1.337754487991333
126 1.3201910257339478
127 1.4238760471343994
128 1.1496249437332153
129 1.4042913913726807
130 1.2628780603408813
131 1.3067713975906372
132 1.2735055685043335
133 1.2877840995788574
134 1.3098022937774658
135 1.1982781887054443
136 1.2388432025909424
137 1.1331642866134644
138 1.4035354852676392
139 1.128847360610962
140 1.304410457611084
141 1.3600956201553345
142 1.4122473001480103
143 1.3257821798324585
144 1.290470004081726
145 1.314782977104187
146 1.3308197259902954
147 1.288766622543335
148 1.4318519830703735
149 1.1880091428756714
150 1.1966384649276733
151 1.3252301216125488
152 1.3130912780761719
153 1.2383952140808105
154 1.3282414674758911
155 1.3714278936386108
156 1.4103487730026245
157 1.1867706775665283
158 1.4029067754745483
159 1.2025063037872314
160 1.329230546951294
161 1.1394480466842651
162 1.104548692703247
163 1.1338616609573364
164 1.2298160791397095
165 1.176835536956787
166 1.1120680570602417
167 1.2509751319885254
168 1.198256492614746
169 1.1821383237838745
170 1.3300973176956177
171 1.1962658166885376
172 1.2157933712005615
173 1.1752030849456787
174 1.1451847553253174
175 1.1328164339065552
176 1.2992652654647827
177 1.4287132024765015
178 1.346508502960205
179 1.3394099473953247
180 1.2131810188293457
181 1.190588116645813
182 1.2307604551315308
183 1.3110002279281616
184 1.212890386581421
185 1.1790167093276978
186 1.1059776544570923
187 1.2140833139419556
188 1.246894359588623
189 1.3325645923614502
190 1.2654415369033813
191 1.2594177722930908
192 1.382256269454956
193 1.3689802885055542
194 1.298896074295044
195 1.2884469032287598
196 1.0814682245254517
197 1.2892857789993286
198 1.178644061088562
199 1.3339482545852661
.............
95.10973144136369 77.82671753503382 14.89171478152275
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3278188705444336
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.327819
1 1.3924262523651123
2 1.3278535604476929
3 1.262292504310608
4 1.2298117876052856
5 1.4950405359268188
6 1.0883092880249023
7 1.3353160619735718
8 1.172274112701416
9 1.355656385421753
10 1.0698473453521729
11 1.2620681524276733
12 1.1751459836959839
13 1.1976984739303589
14 1.2034496068954468
15 1.2571579217910767
16 1.2624192237854004
17 1.2189382314682007
18 1.3447641134262085
19 1.1839326620101929
20 1.380692481994629
21 1.2618992328643799
22 1.1820557117462158
23 1.337855339050293
24 1.2738794088363647
25 1.1504422426223755
26 1.3294129371643066
27 1.273625373840332
28 1.14788818359375
29 1.1419258117675781
30 1.148053765296936
31 1.205074667930603
32 1.1938055753707886
33 1.2373450994491577
34 1.181926965713501
35 1.300155520439148
36 1.1472656726837158
37 1.1708093881607056
38 1.126206874847412
39 1.2769514322280884
40 1.0774341821670532
41 1.2704837322235107
42 1.3848717212677002
43 1.0754607915878296
44 1.0754194259643555
45 1.1939793825149536
46 1.3136510848999023
47 1.182055115699768
48 1.4137535095214844
49 1.2802296876907349
50 1.2893809080123901
51 1.2004585266113281
52 1.254796028137207
53 1.171999454498291
54 0.9955525398254395
55 1.089924693107605
56 1.3150837421417236
57 1.1795560121536255
58 1.2424010038375854
59 1.267093539237976
60 1.2839816808700562
61 1.1847350597381592
62 1.2878386974334717
63 1.1785402297973633
64 1.1566036939620972
65 1.1743292808532715
66 1.1451232433319092
67 1.3816219568252563
68 1.0962586402893066
69 1.2550091743469238
70 1.2303597927093506
71 1.4201409816741943
72 1.1860153675079346
73 1.0972594022750854
74 1.2333807945251465
75 1.2403216361999512
76 1.1479264497756958
77 1.0061780214309692
78 1.2562334537506104
79 1.277105450630188
80 1.2756383419036865
81 1.2021914720535278
82 1.2702182531356812
83 1.2476595640182495
84 0.9595208764076233
85 1.2478991746902466
86 1.3784570693969727
87 1.1817901134490967
88 0.9999507665634155
89 1.1683120727539062
90 1.3293124437332153
91 1.4312171936035156
92 1.3336377143859863
93 1.2137036323547363
94 1.2753716707229614
95 1.1034356355667114
96 1.1720799207687378
97 1.3889474868774414
98 1.1960551738739014
99 1.2400327920913696
100 1.1000018119812012
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.100002
101 1.3065241575241089
102 1.0940934419631958
103 1.102060079574585
104 1.245354413986206
105 1.217893362045288
106 1.1641613245010376
107 1.217379093170166
108 1.101118564605713
109 1.344545602798462
110 1.2423769235610962
111 1.1307276487350464
112 1.2942723035812378
113 1.2281320095062256
114 1.1152106523513794
115 1.264182448387146
116 1.1947654485702515
117 1.2322537899017334
118 1.1644760370254517
119 1.0709604024887085
120 1.276757001876831
121 1.3102099895477295
122 1.2164716720581055
123 1.3453494310379028
124 1.116460919380188
125 1.2068324089050293
126 1.231837272644043
127 1.2944700717926025
128 1.0034679174423218
129 1.2681277990341187
130 1.1072582006454468
131 1.167952299118042
132 1.1084119081497192
133 1.0701391696929932
134 1.1209440231323242
135 1.029617190361023
136 1.1467583179473877
137 0.9694004058837891
138 1.2903987169265747
139 1.0845204591751099
140 1.1151520013809204
141 1.2283936738967896
142 1.2572287321090698
143 1.1940416097640991
144 1.1957924365997314
145 1.1633365154266357
146 1.2204234600067139
147 1.1207419633865356
148 1.283095359802246
149 0.9975737929344177
150 1.1204991340637207
151 1.1636695861816406
152 1.2012161016464233
153 1.112283706665039
154 1.1806151866912842
155 1.1753478050231934
156 1.3413664102554321
157 1.0855202674865723
158 1.2245160341262817
159 1.1134345531463623
160 1.2023147344589233
161 1.0114516019821167
162 0.9521118402481079
163 1.0128216743469238
164 1.097019076347351
165 1.1049180030822754
166 1.0052591562271118
167 1.1166704893112183
168 1.0310043096542358
169 1.0064655542373657
170 1.1858203411102295
171 1.0677093267440796
172 1.066705346107483
173 1.0706853866577148
174 1.0078126192092896
175 1.0023036003112793
176 1.1655632257461548
177 1.276272177696228
178 1.2526183128356934
179 1.2193900346755981
180 1.0926851034164429
181 1.0379918813705444
182 1.141360878944397
183 1.1697207689285278
184 1.0129326581954956
185 1.053271770477295
186 0.9756372570991516
187 1.1020212173461914
188 1.108716607093811
189 1.1648582220077515
190 1.1107383966445923
191 1.0767016410827637
192 1.1849411725997925
193 1.2894972562789917
194 1.2634987831115723
195 1.020682692527771
196 0.9496012926101685
197 1.1413484811782837
198 1.045497179031372
199 1.1804172992706299
.............
346.1107767894864 198.51473977975547 47.530011696740985
.............
0 1.1620337963104248
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.162034
1 1.2985541820526123
2 1.2580180168151855
3 1.1457566022872925
4 1.1412431001663208
5 1.3638768196105957
6 0.9220048785209656
7 1.2030816078186035
8 1.0154410600662231
9 1.24455988407135
10 0.9431495070457458
11 1.1179555654525757
12 1.079803705215454
13 1.0417332649230957
14 1.1438565254211426
15 1.1307505369186401
16 1.1335183382034302
17 0.9961528182029724
18 1.280989408493042
19 1.102790355682373
20 1.2459192276000977
21 1.1262290477752686
22 1.0146894454956055
23 1.2191798686981201
24 1.1270736455917358
25 1.0500309467315674
26 1.2192035913467407
27 1.140371322631836
28 1.0999808311462402
29 1.069126009941101
30 1.056215763092041
31 1.1114503145217896
32 1.0662126541137695
33 1.1459667682647705
34 1.0896129608154297
35 1.20658540725708
36 1.0117563009262085
37 1.098387360572815
38 1.0193167924880981
39 1.1576980352401733
40 0.981889545917511
41 1.166093111038208
42 1.3402070999145508
43 0.9296725392341614
44 0.9593536853790283
45 1.1135916709899902
46 1.2404135465621948
47 1.140203833580017
48 1.3015894889831543
49 1.1895287036895752
50 1.1530224084854126
51 1.0993062257766724
52 1.2366212606430054
53 1.028862476348877
54 0.920914888381958
55 1.0563467741012573
56 1.2048537731170654
57 1.0772534608840942
58 1.139808177947998
59 1.2126684188842773
60 1.2331945896148682
61 1.138423204421997
62 1.185550570487976
63 1.0510692596435547
64 1.1145198345184326
65 1.1623033285140991
66 0.9911855459213257
67 1.2247804403305054
68 1.060879111289978
69 1.1766566038131714
70 1.0724000930786133
71 1.3260111808776855
72 1.1180241107940674
73 1.070789098739624
74 1.195134162902832
75 1.1164191961288452
76 0.9927483201026917
77 0.954230785369873
78 1.2143357992172241
79 1.1336151361465454
80 1.1054162979125977
81 1.1066687107086182
82 1.1591770648956299
83 1.0099411010742188
84 0.9637933373451233
85 1.1645442247390747
86 1.3264237642288208
87 1.0322303771972656
88 0.9027012586593628
89 1.105574369430542
90 1.2624129056930542
91 1.3745527267456055
92 1.273389220237732
93 1.1328670978546143
94 1.1728520393371582
95 1.0330857038497925
96 1.134358525276184
97 1.3723182678222656
98 1.1239224672317505
99 1.193560242652893
100 0.9999716281890869
Train Epoch: 3 [10000/20000 (50%)]	Loss: 0.999972
101 1.2492918968200684
102 1.0989598035812378
103 1.0993825197219849
104 1.162621259689331
105 1.145025610923767
106 1.1361769437789917
107 1.1451921463012695
108 1.0067124366760254
109 1.308772325515747
110 1.1421505212783813
111 1.050839900970459
112 1.1760499477386475
113 1.2433116436004639
114 0.9749704003334045
115 1.1927838325500488
116 1.214887022972107
117 1.1683117151260376
118 0.974393367767334
119 1.0338318347930908
120 1.242937445640564
121 1.2338709831237793
122 1.126562237739563
123 1.314061164855957
124 1.1030540466308594
125 1.2294557094573975
126 1.2173584699630737
127 1.2437407970428467
128 0.9418440461158752
129 1.2491276264190674
130 1.1181186437606812
131 1.0957773923873901
132 1.1195790767669678
133 1.0450210571289062
134 1.0600130558013916
135 0.9876068234443665
136 1.1371930837631226
137 0.9118570685386658
138 1.2627196311950684
139 1.1387947797775269
140 1.0672425031661987
141 1.2312947511672974
142 1.2058502435684204
143 1.139510989189148
144 1.1938071250915527
145 1.1372272968292236
146 1.2003954648971558
147 1.043940782546997
148 1.2087000608444214
149 0.9451724290847778
150 1.102478265762329
151 1.1201434135437012
152 1.1677842140197754
153 1.0871455669403076
154 1.115717887878418
155 1.1178531646728516
156 1.3175631761550903
157 1.0680608749389648
158 1.166670322418213
159 1.079512596130371
160 1.1649518013000488
161 0.9532670378684998
162 0.9153497219085693
163 0.9837848544120789
164 1.0547590255737305
165 1.060682773590088
166 0.9377713799476624
167 1.0533394813537598
168 0.9859527349472046
169 0.9603456258773804
170 1.1723803281784058
171 1.0327831506729126
172 1.0012935400009155
173 1.0216684341430664
174 0.9983943700790405
175 0.9944829344749451
176 1.1578948497772217
177 1.2079086303710938
178 1.2294209003448486
179 1.1795560121536255
180 1.0623492002487183
181 1.0345120429992676
182 1.0854473114013672
183 1.1292299032211304
184 0.985502302646637
185 1.021101713180542
186 0.9410125017166138
187 1.076866865158081
188 1.064257264137268
189 1.1328660249710083
190 1.0735622644424438
191 1.0329288244247437
192 1.1348901987075806
193 1.2306599617004395
194 1.2229419946670532
195 0.9648764729499817
196 0.9209553599357605
197 1.0712714195251465
198 1.012069821357727
199 1.1541813611984253
.............
846.6004131063819 440.3901259303093 112.46030484326184
.............
0 1.114806890487671
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.114807
1 1.2763313055038452
2 1.2131731510162354
3 1.1357394456863403
4 1.082682490348816
5 1.3293856382369995
6 0.892090916633606
7 1.167860507965088
8 0.9789196252822876
9 1.2023978233337402
10 0.9093189835548401
11 1.0700263977050781
12 1.0498876571655273
13 1.00807523727417
14 1.1377460956573486
15 1.1076773405075073
16 1.0895824432373047
17 0.9418638348579407
18 1.2755804061889648
19 1.0539860725402832
20 1.2038123607635498
21 1.0840686559677124
22 0.9706800580024719
23 1.1839570999145508
24 1.0998023748397827
25 1.0169110298156738
26 1.2003544569015503
27 1.087847113609314
28 1.0934348106384277
29 1.0640913248062134
30 1.0253716707229614
31 1.1036862134933472
32 1.0253056287765503
33 1.1045305728912354
34 1.070146918296814
35 1.1760374307632446
36 1.0195657014846802
37 1.0919643640518188
38 1.007087230682373
39 1.148726224899292
40 0.9631191492080688
41 1.1473768949508667
42 1.3021912574768066
43 0.904245913028717
44 0.9606900215148926
45 1.0902520418167114
46 1.22512686252594
47 1.1079792976379395
48 1.2803667783737183
49 1.1457643508911133
50 1.1419577598571777
51 1.0749162435531616
52 1.192394733428955
53 1.0206618309020996
54 0.9091469645500183
55 1.0273232460021973
56 1.1422722339630127
57 1.046931266784668
58 1.1050513982772827
59 1.1584330797195435
60 1.1762789487838745
61 1.1193963289260864
62 1.1413978338241577
63 1.001393437385559
64 1.0631135702133179
65 1.1008861064910889
66 0.9664533734321594
67 1.197721004486084
68 1.0203547477722168
69 1.128764271736145
70 1.0265421867370605
71 1.289781093597412
72 1.102756142616272
73 1.0556256771087646
74 1.1668568849563599
75 1.0862582921981812
76 0.9697060585021973
77 0.9419500231742859
78 1.194826602935791
79 1.1058701276779175
80 1.0720925331115723
81 1.0807210206985474
82 1.1383068561553955
83 0.9851996898651123
84 0.9539711475372314
85 1.1366938352584839
86 1.2820826768875122
87 1.0035544633865356
88 0.8982585072517395
89 1.0799657106399536
90 1.2624328136444092
91 1.3591514825820923
92 1.2642438411712646
93 1.13388192653656
94 1.1788440942764282
95 0.9917739033699036
96 1.1377792358398438
97 1.2896236181259155
98 1.1025394201278687
99 1.169614315032959
100 0.9769567251205444
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.976957
101 1.216872215270996
102 1.0800267457962036
103 1.1054143905639648
104 1.138685703277588
105 1.1218056678771973
106 1.1024253368377686
107 1.124438762664795
108 0.984853208065033
109 1.2739415168762207
110 1.0952649116516113
111 1.029436469078064
112 1.1494767665863037
113 1.1918034553527832
114 0.9563479423522949
115 1.151538372039795
116 1.1569998264312744
117 1.1361143589019775
118 0.9833575487136841
119 0.9840973019599915
120 1.1976057291030884
121 1.2321789264678955
122 1.0703202486038208
123 1.2776515483856201
124 1.0480377674102783
125 1.1707578897476196
126 1.1889744997024536
127 1.2158610820770264
128 0.9478525519371033
129 1.1863036155700684
130 1.0975700616836548
131 1.0506263971328735
132 1.0342535972595215
133 0.9963778853416443
134 1.0284526348114014
135 0.9735603332519531
136 1.1341869831085205
137 0.8816928267478943
138 1.247693419456482
139 1.1147555112838745
140 1.053640365600586
141 1.23661208152771
142 1.1734222173690796
143 1.113376259803772
144 1.1932690143585205
145 1.1350374221801758
146 1.206443428993225
147 1.0059086084365845
148 1.1775128841400146
149 0.920799970626831
150 1.1077868938446045
151 1.1344654560089111
152 1.1727747917175293
153 1.0757343769073486
154 1.0667475461959839
155 1.0888582468032837
156 1.305795431137085
157 1.0900498628616333
158 1.1570768356323242
159 1.0903301239013672
160 1.184842586517334
161 0.9433181881904602
162 0.9214348793029785
163 1.0061603784561157
164 1.049504280090332
165 1.0424878597259521
166 0.9197449684143066
167 1.033728837966919
168 0.9828435778617859
169 0.9562458992004395
170 1.158616304397583
171 1.0182344913482666
172 0.9771140813827515
173 0.9940900206565857
174 1.0009547472000122
175 0.9956081509590149
176 1.1711220741271973
177 1.1773290634155273
178 1.2044514417648315
179 1.169793725013733
180 1.053647518157959
181 1.0239561796188354
182 1.061632752418518
183 1.1000654697418213
184 0.9687219262123108
185 1.0048823356628418
186 0.9165931940078735
187 1.073185920715332
188 1.0548282861709595
189 1.134657621383667
190 1.0540963411331177
191 1.0165704488754272
192 1.1199121475219727
193 1.201358437538147
194 1.2075079679489136
195 0.9371951818466187
196 0.9089295268058777
197 1.0395457744598389
198 0.991719663143158
199 1.1362457275390625
.............
1859.8585394453257 924.2614262644202 243.3971180934459
.............
0 1.0964951515197754
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.096495
1 1.2651137113571167
2 1.1929965019226074
3 1.1267578601837158
4 1.0515483617782593
5 1.294495701789856
6 0.8828642964363098
7 1.1498444080352783
8 0.9627503752708435
9 1.1847448348999023
10 0.9013044238090515
11 1.0524252653121948
12 1.0416395664215088
13 1.0043870210647583
14 1.12998628616333
15 1.0827068090438843
16 1.06781804561615
17 0.9081115126609802
18 1.2684149742126465
19 1.0240534543991089
20 1.186173439025879
21 1.057898759841919
22 0.9411191344261169
23 1.1568818092346191
24 1.093873381614685
25 0.9889568090438843
26 1.1903678178787231
27 1.0613453388214111
28 1.0592342615127563
29 1.0554707050323486
30 0.9730219841003418
31 1.0483115911483765
32 0.9954218864440918
33 1.0722368955612183
34 1.034833312034607
35 1.1671801805496216
36 0.9751595854759216
37 1.0627923011779785
38 0.988145112991333
39 1.1677354574203491
40 0.9501579403877258
41 1.0887473821640015
42 1.2698736190795898
43 0.8922204375267029
44 0.9398573040962219
45 1.081183910369873
46 1.2495187520980835
47 1.0466572046279907
48 1.2782913446426392
49 1.1251921653747559
50 1.1804312467575073
51 1.0829503536224365
52 1.1236562728881836
53 1.022477626800537
54 0.9026003479957581
55 0.9994618892669678
56 1.107834815979004
57 1.0365699529647827
58 1.117974042892456
59 1.1332659721374512
60 1.1365923881530762
61 1.1151702404022217
62 1.1394915580749512
63 0.9510751366615295
64 1.0394599437713623
65 1.0547332763671875
66 0.9359695911407471
67 1.1671748161315918
68 1.0179389715194702
69 1.0822923183441162
70 0.9743964672088623
71 1.244328260421753
72 1.087795615196228
73 1.0279288291931152
74 1.1442663669586182
75 1.0485299825668335
76 0.9646807312965393
77 0.9237315654754639
78 1.1685185432434082
79 1.079067349433899
80 1.0632234811782837
81 1.0544452667236328
82 1.128028154373169
83 0.9650658965110779
84 0.9068331122398376
85 1.1070671081542969
86 1.219050645828247
87 0.971998393535614
88 0.8920145630836487
89 1.0437343120574951
90 1.2341197729110718
91 1.3402752876281738
92 1.2506791353225708
93 1.0995359420776367
94 1.1574463844299316
95 0.9588527083396912
96 1.0826410055160522
97 1.2113230228424072
98 1.0884863138198853
99 1.1256293058395386
100 0.9419737458229065
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.941974
101 1.1825485229492188
102 1.0332708358764648
103 1.0510108470916748
104 1.1184996366500854
105 1.0953353643417358
106 1.0555042028427124
107 1.0940967798233032
108 0.9612260460853577
109 1.2492613792419434
110 1.0769667625427246
111 1.0155386924743652
112 1.0969771146774292
113 1.1899641752243042
114 0.938395082950592
115 1.134160041809082
116 1.1252830028533936
117 1.0981204509735107
118 0.9702827334403992
119 0.9510784149169922
120 1.1418430805206299
121 1.2397055625915527
122 1.0397236347198486
123 1.1926435232162476
124 1.0268175601959229
125 1.1534597873687744
126 1.1696767807006836
127 1.170800805091858
128 0.950838029384613
129 1.1021937131881714
130 1.0720964670181274
131 1.0237407684326172
132 0.9645184874534607
133 0.9545392394065857
134 0.9912364482879639
135 0.9496614933013916
136 1.0990487337112427
137 0.8571062684059143
138 1.209303617477417
139 1.0407546758651733
140 1.0033189058303833
141 1.2119892835617065
142 1.140060305595398
143 1.0849072933197021
144 1.1449085474014282
145 1.090631127357483
146 1.1740639209747314
147 0.9695260524749756
148 1.1659691333770752
149 0.8774476647377014
150 1.0695369243621826
151 1.1227771043777466
152 1.1152211427688599
153 1.0505348443984985
154 1.062251091003418
155 1.0306800603866577
156 1.2842011451721191
157 1.0910613536834717
158 1.110571026802063
159 1.0509448051452637
160 1.1636958122253418
161 0.9397086501121521
162 0.9093557596206665
163 1.0564059019088745
164 1.0753183364868164
165 1.018121600151062
166 0.9006808400154114
167 1.012807846069336
168 0.9619840979576111
169 0.9504441022872925
170 1.1476752758026123
171 1.0181454420089722
172 0.9785349369049072
173 0.9554072618484497
174 0.9783415198326111
175 0.9728502035140991
176 1.1565861701965332
177 1.1569589376449585
178 1.1797375679016113
179 1.156102180480957
180 1.0474181175231934
181 1.0073684453964233
182 1.0431162118911743
183 1.060865879058838
184 0.9525479078292847
185 0.9738063216209412
186 0.8978334069252014
187 1.0510140657424927
188 1.0300283432006836
189 1.119531512260437
190 1.0296876430511475
191 0.9967143535614014
192 1.0814076662063599
193 1.1684341430664062
194 1.1668157577514648
195 0.9134336113929749
196 0.8907337784767151
197 1.0079727172851562
198 0.9592389464378357
199 1.1116762161254883
.............
3889.002924863249 1892.4649569932371 504.9587300308049
.............
7 3889.002924863249 1892.4649569932371 504.9587300308049
.............
