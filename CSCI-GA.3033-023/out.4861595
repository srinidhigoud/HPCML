0 2.8513801097869873
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.851380
1 2.8352184295654297
2 2.7946290969848633
3 2.737671136856079
4 2.7080416679382324
5 2.6225717067718506
6 2.494379997253418
7 2.435222864151001
8 2.2756261825561523
9 2.195629835128784
10 2.0000858306884766
11 1.9053544998168945
12 1.7835578918457031
13 2.0545825958251953
14 1.9219999313354492
15 1.968625545501709
16 1.8286688327789307
17 1.9553943872451782
18 1.7370407581329346
19 1.8528012037277222
20 2.0068366527557373
21 1.851529598236084
22 1.549829363822937
23 1.7898974418640137
24 1.6475458145141602
25 1.7646809816360474
26 1.8683454990386963
27 1.8198550939559937
28 1.4990873336791992
29 1.6857889890670776
30 1.6682783365249634
31 1.7495535612106323
32 1.6203351020812988
33 1.6295756101608276
34 1.6365340948104858
35 1.746030569076538
36 1.5538170337677002
37 1.6548101902008057
38 1.5677146911621094
39 1.577208399772644
40 1.5725048780441284
41 1.6044871807098389
42 1.6412450075149536
43 1.5363492965698242
44 1.4384918212890625
45 1.4659672975540161
46 1.7273478507995605
47 1.6306456327438354
48 1.6926085948944092
49 1.6091238260269165
50 1.5272207260131836
51 1.5424914360046387
52 1.5821794271469116
53 1.514153242111206
54 1.3047056198120117
55 1.5466132164001465
56 1.756909966468811
57 1.5705424547195435
58 1.4742008447647095
59 1.5944305658340454
60 1.6596640348434448
61 1.618299961090088
62 1.5575377941131592
63 1.52866792678833
64 1.5893553495407104
65 1.5600460767745972
66 1.4432607889175415
67 1.5332276821136475
68 1.5023020505905151
69 1.5480822324752808
70 1.4091567993164062
71 1.607619285583496
72 1.42618727684021
73 1.4144703149795532
74 1.3884153366088867
75 1.402177333831787
76 1.391945481300354
77 1.3136883974075317
78 1.4791256189346313
79 1.515854001045227
80 1.3678501844406128
81 1.4372438192367554
82 1.5198745727539062
83 1.4606845378875732
84 1.2524189949035645
85 1.4967973232269287
86 1.6204208135604858
87 1.4491626024246216
88 1.303114414215088
89 1.420191764831543
90 1.536067247390747
91 1.6118649244308472
92 1.5803462266921997
93 1.4504404067993164
94 1.4481828212738037
95 1.316106915473938
96 1.3805195093154907
97 1.6290833950042725
98 1.5725857019424438
99 1.3994755744934082
100 1.3456944227218628
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.345694
101 1.5037367343902588
102 1.356089472770691
103 1.3000885248184204
104 1.471036434173584
105 1.5413936376571655
106 1.3866560459136963
107 1.463849663734436
108 1.393428087234497
109 1.546638011932373
110 1.4437713623046875
111 1.3441663980484009
112 1.5592644214630127
113 1.4756784439086914
114 1.1908090114593506
115 1.3890169858932495
116 1.3556878566741943
117 1.3976140022277832
118 1.2638856172561646
119 1.2550078630447388
120 1.4777615070343018
121 1.401979684829712
122 1.3389828205108643
123 1.486401081085205
124 1.238349199295044
125 1.3272781372070312
126 1.3213809728622437
127 1.405713677406311
128 1.133823275566101
129 1.4026875495910645
130 1.2502633333206177
131 1.3066039085388184
132 1.2929459810256958
133 1.279387354850769
134 1.3180510997772217
135 1.2089416980743408
136 1.2341055870056152
137 1.1385862827301025
138 1.401816725730896
139 1.1235547065734863
140 1.3039307594299316
141 1.3623698949813843
142 1.4103024005889893
143 1.3262830972671509
144 1.296811819076538
145 1.3119524717330933
146 1.3230483531951904
147 1.2937109470367432
148 1.431647539138794
149 1.177351713180542
150 1.191633701324463
151 1.332969069480896
152 1.3098987340927124
153 1.2380367517471313
154 1.3342416286468506
155 1.3736565113067627
156 1.4104255437850952
157 1.1844207048416138
158 1.407042384147644
159 1.1990050077438354
160 1.3368600606918335
161 1.1395459175109863
162 1.1072850227355957
163 1.1311033964157104
164 1.229024887084961
165 1.1751235723495483
166 1.1089179515838623
167 1.2485263347625732
168 1.195354700088501
169 1.1677470207214355
170 1.3297905921936035
171 1.2038782835006714
172 1.2182565927505493
173 1.1753122806549072
174 1.138817310333252
175 1.135781168937683
176 1.2962650060653687
177 1.4262824058532715
178 1.3592770099639893
179 1.334632158279419
180 1.2234420776367188
181 1.2032248973846436
182 1.2331492900848389
183 1.3228117227554321
184 1.226660966873169
185 1.1797624826431274
186 1.105921745300293
187 1.2095202207565308
188 1.2501658201217651
189 1.3348565101623535
190 1.2715823650360107
191 1.2724069356918335
192 1.399168848991394
193 1.3590320348739624
194 1.3279058933258057
195 1.3265708684921265
196 1.0800453424453735
197 1.295128583908081
198 1.2114243507385254
199 1.3428698778152466
.............
271.8240271471441 47.4893988519907 11.161412259563804
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.338015079498291
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.338015
1 1.394821047782898
2 1.331047534942627
3 1.2647396326065063
4 1.2274633646011353
5 1.482467532157898
6 1.1077914237976074
7 1.328545331954956
8 1.18022620677948
9 1.352685570716858
10 1.074783444404602
11 1.2624152898788452
12 1.172848105430603
13 1.2003196477890015
14 1.211199164390564
15 1.2611358165740967
16 1.2710680961608887
17 1.2310467958450317
18 1.3377188444137573
19 1.1906094551086426
20 1.3793206214904785
21 1.2759199142456055
22 1.1916344165802002
23 1.350555419921875
24 1.2966166734695435
25 1.151542067527771
26 1.321181058883667
27 1.2879146337509155
28 1.1528327465057373
29 1.1601307392120361
30 1.1713457107543945
31 1.2390960454940796
32 1.2046297788619995
33 1.2417025566101074
34 1.188565969467163
35 1.3071386814117432
36 1.1478073596954346
37 1.172989010810852
38 1.1411055326461792
39 1.2989307641983032
40 1.0747156143188477
41 1.291916012763977
42 1.389970302581787
43 1.081336259841919
44 1.0802452564239502
45 1.1974380016326904
46 1.3263827562332153
47 1.2005648612976074
48 1.4223942756652832
49 1.2818398475646973
50 1.284875512123108
51 1.1975290775299072
52 1.265522837638855
53 1.1863365173339844
54 1.0214999914169312
55 1.0884212255477905
56 1.3125355243682861
57 1.1768970489501953
58 1.2513556480407715
59 1.2910330295562744
60 1.2916749715805054
61 1.187649130821228
62 1.3126481771469116
63 1.2162318229675293
64 1.1589757204055786
65 1.1957069635391235
66 1.1670233011245728
67 1.4154183864593506
68 1.1304576396942139
69 1.2670259475708008
70 1.278931736946106
71 1.4364612102508545
72 1.1968075037002563
73 1.113821029663086
74 1.250882625579834
75 1.252997875213623
76 1.171433448791504
77 1.0207312107086182
78 1.2579927444458008
79 1.2869081497192383
80 1.3046722412109375
81 1.2203575372695923
82 1.2721706628799438
83 1.2568529844284058
84 0.9809488654136658
85 1.2625786066055298
86 1.3902801275253296
87 1.2060420513153076
88 1.0083426237106323
89 1.187028169631958
90 1.3367609977722168
91 1.436037302017212
92 1.3421931266784668
93 1.2296779155731201
94 1.291640281677246
95 1.1169328689575195
96 1.1856330633163452
97 1.3846766948699951
98 1.216688871383667
99 1.2450451850891113
100 1.1198229789733887
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.119823
101 1.3091963529586792
102 1.0868228673934937
103 1.1023945808410645
104 1.2528318166732788
105 1.2242863178253174
106 1.1687297821044922
107 1.2231338024139404
108 1.114782452583313
109 1.350624680519104
110 1.2456804513931274
111 1.1443191766738892
112 1.3145521879196167
113 1.2280879020690918
114 1.1025036573410034
115 1.2812594175338745
116 1.2054905891418457
117 1.2336888313293457
118 1.1749979257583618
119 1.086246132850647
120 1.2892749309539795
121 1.3133878707885742
122 1.234018325805664
123 1.3590701818466187
124 1.123166799545288
125 1.2113573551177979
126 1.2326037883758545
127 1.2964036464691162
128 1.0083707571029663
129 1.2938168048858643
130 1.118073582649231
131 1.1854363679885864
132 1.1300127506256104
133 1.0784616470336914
134 1.1234573125839233
135 1.0390735864639282
136 1.1626887321472168
137 0.9889413714408875
138 1.2928370237350464
139 1.057873010635376
140 1.1211494207382202
141 1.2240408658981323
142 1.2677276134490967
143 1.204188346862793
144 1.1874542236328125
145 1.170510172843933
146 1.2296291589736938
147 1.1417930126190186
148 1.3116941452026367
149 1.0088757276535034
150 1.1030224561691284
151 1.182854175567627
152 1.21274995803833
153 1.1221773624420166
154 1.179317593574524
155 1.1815415620803833
156 1.3475701808929443
157 1.0938727855682373
158 1.2493031024932861
159 1.1097190380096436
160 1.2155680656433105
161 1.0227967500686646
162 0.9600965976715088
163 1.0354385375976562
164 1.1200892925262451
165 1.095485806465149
166 1.0017061233520508
167 1.1208837032318115
168 1.0473917722702026
169 1.0099806785583496
170 1.1975337266921997
171 1.080378770828247
172 1.076319694519043
173 1.0843545198440552
174 1.0162562131881714
175 1.0270028114318848
176 1.172757625579834
177 1.2810852527618408
178 1.272270917892456
179 1.2430503368377686
180 1.1065760850906372
181 1.0471296310424805
182 1.1402827501296997
183 1.181196689605713
184 1.0269114971160889
185 1.0599764585494995
186 0.993079423904419
187 1.110235571861267
188 1.1207287311553955
189 1.1782467365264893
190 1.1222374439239502
191 1.0906248092651367
192 1.2050169706344604
193 1.3106689453125
194 1.274461030960083
195 1.0330678224563599
196 0.9568044543266296
197 1.162373661994934
198 1.0576218366622925
199 1.1935003995895386
.............
807.025571545586 142.11562109738588 33.376998618245125
.............
0 1.1770281791687012
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.177028
1 1.3046565055847168
2 1.2609953880310059
3 1.1385098695755005
4 1.1567186117172241
5 1.3658642768859863
6 0.9288383722305298
7 1.201204776763916
8 1.0273057222366333
9 1.2525798082351685
10 0.9490448832511902
11 1.1256964206695557
12 1.0925999879837036
13 1.0544339418411255
14 1.1404614448547363
15 1.133109211921692
16 1.1424283981323242
17 1.0059900283813477
18 1.2803400754928589
19 1.110750436782837
20 1.2482037544250488
21 1.1373902559280396
22 1.0269414186477661
23 1.2280830144882202
24 1.134316086769104
25 1.053037405014038
26 1.2166942358016968
27 1.151073932647705
28 1.088893175125122
29 1.0707954168319702
30 1.053554654121399
31 1.0954357385635376
32 1.0698570013046265
33 1.1421786546707153
34 1.0905894041061401
35 1.2122522592544556
36 1.0000348091125488
37 1.099575161933899
38 1.0243264436721802
39 1.172314167022705
40 0.982830286026001
41 1.1835299730300903
42 1.3439054489135742
43 0.9441214203834534
44 0.9652374982833862
45 1.1172575950622559
46 1.2507632970809937
47 1.142359733581543
48 1.3052335977554321
49 1.2112634181976318
50 1.1650538444519043
51 1.106887936592102
52 1.2489196062088013
53 1.0474357604980469
54 0.921393632888794
55 1.052869439125061
56 1.2122677564620972
57 1.0942273139953613
58 1.1526018381118774
59 1.2116150856018066
60 1.2430537939071655
61 1.1440587043762207
62 1.1856582164764404
63 1.057508111000061
64 1.1178778409957886
65 1.1743600368499756
66 1.012027621269226
67 1.2148991823196411
68 1.0621812343597412
69 1.171810269355774
70 1.0982940196990967
71 1.306288242340088
72 1.1099711656570435
73 1.0668257474899292
74 1.2057117223739624
75 1.0980000495910645
76 0.993460476398468
77 0.9491965770721436
78 1.1985313892364502
79 1.1335748434066772
80 1.1022809743881226
81 1.101181983947754
82 1.1673212051391602
83 1.0154623985290527
84 0.9970848560333252
85 1.1691522598266602
86 1.3321036100387573
87 1.0487926006317139
88 0.9051236510276794
89 1.1172751188278198
90 1.2781929969787598
91 1.3680986166000366
92 1.2746093273162842
93 1.1504886150360107
94 1.1916003227233887
95 1.0288739204406738
96 1.1623159646987915
97 1.348800539970398
98 1.1378098726272583
99 1.1940711736679077
100 1.0139849185943604
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.013985
101 1.2634546756744385
102 1.0827879905700684
103 1.1224476099014282
104 1.1682568788528442
105 1.1479191780090332
106 1.1376444101333618
107 1.1576831340789795
108 1.005553960800171
109 1.3133556842803955
110 1.1315327882766724
111 1.0470730066299438
112 1.1870652437210083
113 1.2106951475143433
114 0.969475507736206
115 1.1885695457458496
116 1.1957573890686035
117 1.1732287406921387
118 0.9873673915863037
119 1.0163589715957642
120 1.2497223615646362
121 1.23903489112854
122 1.1193805932998657
123 1.300999641418457
124 1.0895684957504272
125 1.2079176902770996
126 1.2080590724945068
127 1.2346118688583374
128 0.9530076384544373
129 1.2557518482208252
130 1.1223421096801758
131 1.0937347412109375
132 1.1112343072891235
133 1.0466089248657227
134 1.0519319772720337
135 0.9943060278892517
136 1.1418259143829346
137 0.9180055856704712
138 1.2640303373336792
139 1.1321769952774048
140 1.0717973709106445
141 1.2232452630996704
142 1.2055292129516602
143 1.1500914096832275
144 1.1977380514144897
145 1.1451458930969238
146 1.2003529071807861
147 1.050063967704773
148 1.214721918106079
149 0.9463480114936829
150 1.1185884475708008
151 1.122511863708496
152 1.1742393970489502
153 1.0987087488174438
154 1.1233750581741333
155 1.125909447669983
156 1.3336454629898071
157 1.0676665306091309
158 1.1725592613220215
159 1.0726999044418335
160 1.1670268774032593
161 0.9608557224273682
162 0.9080536365509033
163 0.9823530316352844
164 1.0621700286865234
165 1.0635052919387817
166 0.9383854866027832
167 1.0573017597198486
168 0.9821242690086365
169 0.9616556763648987
170 1.1685723066329956
171 1.0277820825576782
172 1.0116908550262451
173 1.021255373954773
174 1.0031650066375732
175 0.9926136136054993
176 1.1566672325134277
177 1.2054262161254883
178 1.2282567024230957
179 1.1799412965774536
180 1.0697448253631592
181 1.0373079776763916
182 1.0824605226516724
183 1.129194736480713
184 0.9913338422775269
185 1.022376298904419
186 0.9475817084312439
187 1.0820460319519043
188 1.0750021934509277
189 1.146255373954773
190 1.0774834156036377
191 1.0395413637161255
192 1.1505247354507446
193 1.2464100122451782
194 1.233337163925171
195 0.9624109864234924
196 0.9235605597496033
197 1.080950379371643
198 1.018251657485962
199 1.170334815979004
.............
1879.1157699059695 331.6313936021179 77.72725999727845
.............
0 1.1255792379379272
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.125579
1 1.278602957725525
2 1.216395616531372
3 1.129076361656189
4 1.0964456796646118
5 1.3275337219238281
6 0.9032513499259949
7 1.169184923171997
8 0.9865292310714722
9 1.2042949199676514
10 0.9103648662567139
11 1.0737708806991577
12 1.0547949075698853
13 1.0139946937561035
14 1.1353118419647217
15 1.1079363822937012
16 1.0945103168487549
17 0.9477797746658325
18 1.273391604423523
19 1.0590412616729736
20 1.1998488903045654
21 1.0864310264587402
22 0.9749554395675659
23 1.1832575798034668
24 1.1036767959594727
25 1.013273000717163
26 1.187952995300293
27 1.104731798171997
28 1.0921921730041504
29 1.0583958625793457
30 1.0308998823165894
31 1.1083736419677734
32 1.0263396501541138
33 1.122676134109497
34 1.071855902671814
35 1.18220055103302
36 1.0244648456573486
37 1.0949960947036743
38 1.0088330507278442
39 1.1528716087341309
40 0.9716427326202393
41 1.1498910188674927
42 1.3133749961853027
43 0.9129574298858643
44 0.9551473259925842
45 1.0961371660232544
46 1.2184804677963257
47 1.1115992069244385
48 1.2864441871643066
49 1.1448109149932861
50 1.1427134275436401
51 1.0725826025009155
52 1.1851922273635864
53 1.0278306007385254
54 0.9193597435951233
55 1.0193122625350952
56 1.1390337944030762
57 1.0538532733917236
58 1.11350679397583
59 1.1592199802398682
60 1.167027473449707
61 1.120138168334961
62 1.1408600807189941
63 0.9949591159820557
64 1.0535458326339722
65 1.1091707944869995
66 0.9759768843650818
67 1.193487286567688
68 1.0143206119537354
69 1.1323148012161255
70 1.039681077003479
71 1.2891335487365723
72 1.0981411933898926
73 1.0507285594940186
74 1.1703405380249023
75 1.086247444152832
76 0.9719178080558777
77 0.9404473304748535
78 1.1900458335876465
79 1.106316328048706
80 1.0739341974258423
81 1.071135401725769
82 1.1427898406982422
83 0.984853208065033
84 0.9686069488525391
85 1.1373423337936401
86 1.2832890748977661
87 1.005428433418274
88 0.8965620994567871
89 1.089198350906372
90 1.2684839963912964
91 1.356644868850708
92 1.267257809638977
93 1.1401077508926392
94 1.174912929534912
95 0.9976664185523987
96 1.1453765630722046
97 1.2800335884094238
98 1.0991653203964233
99 1.1665312051773071
100 0.9823238849639893
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.982324
101 1.2239208221435547
102 1.0688151121139526
103 1.1073168516159058
104 1.1376978158950806
105 1.124286413192749
106 1.0965279340744019
107 1.1281931400299072
108 0.9925538897514343
109 1.2848962545394897
110 1.0984888076782227
111 1.0321667194366455
112 1.149835467338562
113 1.1928216218948364
114 0.9581020474433899
115 1.1537411212921143
116 1.1591700315475464
117 1.1395918130874634
118 0.9941209554672241
119 0.9839050769805908
120 1.205090045928955
121 1.2381031513214111
122 1.0716357231140137
123 1.2647960186004639
124 1.0405694246292114
125 1.1710492372512817
126 1.1927658319473267
127 1.2134485244750977
128 0.95494544506073
129 1.1810009479522705
130 1.0977694988250732
131 1.0481114387512207
132 1.0287117958068848
133 0.9933103322982788
134 1.0244605541229248
135 0.9787024855613708
136 1.1296266317367554
137 0.882689893245697
138 1.2466857433319092
139 1.103861927986145
140 1.0498852729797363
141 1.2328007221221924
142 1.174399971961975
143 1.1228128671646118
144 1.1938741207122803
145 1.1357429027557373
146 1.1998438835144043
147 1.0092597007751465
148 1.1835784912109375
149 0.9158226847648621
150 1.1168206930160522
151 1.1461782455444336
152 1.170117735862732
153 1.0834671258926392
154 1.0870646238327026
155 1.094495415687561
156 1.3146352767944336
157 1.0778226852416992
158 1.168063998222351
159 1.0911033153533936
160 1.1788506507873535
161 0.946936845779419
162 0.9173284769058228
163 1.008567452430725
164 1.0597549676895142
165 1.043842077255249
166 0.9175670146942139
167 1.0346657037734985
168 0.9815224409103394
169 0.9594108462333679
170 1.1625880002975464
171 1.019076943397522
172 0.9895199537277222
173 0.9900057315826416
174 0.9930905103683472
175 1.00067138671875
176 1.1735490560531616
177 1.1746606826782227
178 1.2035224437713623
179 1.166978120803833
180 1.054427981376648
181 1.025156021118164
182 1.058497667312622
183 1.0951714515686035
184 0.9764910340309143
185 1.0074052810668945
186 0.9256536960601807
187 1.078064203262329
188 1.0555438995361328
189 1.1389888525009155
190 1.0532513856887817
191 1.0176700353622437
192 1.1226942539215088
193 1.2064114809036255
194 1.2082147598266602
195 0.9345502257347107
196 0.9075855016708374
197 1.0475599765777588
198 0.9959732294082642
199 1.1488245725631714
.............
4032.8288437183946 710.8981991838664 167.2323575913906
.............
0 1.1021746397018433
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.102175
1 1.2685949802398682
2 1.1952325105667114
3 1.1167033910751343
4 1.057300090789795
5 1.2951884269714355
6 0.8863640427589417
7 1.1421308517456055
8 0.9656593799591064
9 1.184563159942627
10 0.9016097187995911
11 1.0477113723754883
12 1.0412744283676147
13 1.007725715637207
14 1.130310297012329
15 1.0774060487747192
16 1.064499855041504
17 0.9141899943351746
18 1.2669981718063354
19 1.025234580039978
20 1.1807775497436523
21 1.062819242477417
22 0.9432743787765503
23 1.1590574979782104
24 1.089296579360962
25 0.9864336252212524
26 1.189476490020752
27 1.0619982481002808
28 1.0591870546340942
29 1.052789330482483
30 0.9745205044746399
31 1.0457217693328857
32 1.0016192197799683
33 1.0760514736175537
34 1.0336157083511353
35 1.1681536436080933
36 0.97468101978302
37 1.0649360418319702
38 0.9836488366127014
39 1.1661895513534546
40 0.9491138458251953
41 1.0815412998199463
42 1.2871642112731934
43 0.8919571042060852
44 0.9361716508865356
45 1.0794668197631836
46 1.2419788837432861
47 1.0511053800582886
48 1.2773046493530273
49 1.1281673908233643
50 1.1750423908233643
51 1.0775816440582275
52 1.1248143911361694
53 1.0225651264190674
54 0.9090232849121094
55 0.9945172071456909
56 1.1029114723205566
57 1.043848991394043
58 1.120755672454834
59 1.1328532695770264
60 1.140580415725708
61 1.1160931587219238
62 1.1367300748825073
63 0.9559106230735779
64 1.0445483922958374
65 1.0595779418945312
66 0.9443643689155579
67 1.1650044918060303
68 1.0199168920516968
69 1.0775917768478394
70 0.9812632203102112
71 1.2352168560028076
72 1.0844011306762695
73 1.012281894683838
74 1.1408820152282715
75 1.0505061149597168
76 0.9615674614906311
77 0.9220157861709595
78 1.1607164144515991
79 1.071499228477478
80 1.0525093078613281
81 1.048934817314148
82 1.128997802734375
83 0.9615778923034668
84 0.9086318016052246
85 1.112612009048462
86 1.2255054712295532
87 0.9700207710266113
88 0.8862438797950745
89 1.051365852355957
90 1.2376304864883423
91 1.3375757932662964
92 1.2455745935440063
93 1.0985394716262817
94 1.152681827545166
95 0.9645503163337708
96 1.07705819606781
97 1.2166420221328735
98 1.0841110944747925
99 1.126206636428833
100 0.9406526684761047
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.940653
101 1.1792235374450684
102 1.0267173051834106
103 1.0547846555709839
104 1.1126525402069092
105 1.0840175151824951
106 1.048790693283081
107 1.1030640602111816
108 0.9757235646247864
109 1.2652273178100586
110 1.0828944444656372
111 1.0140619277954102
112 1.0918922424316406
113 1.1902203559875488
114 0.9378525018692017
115 1.1348668336868286
116 1.1244587898254395
117 1.0952236652374268
118 0.9762001037597656
119 0.946219265460968
120 1.1432875394821167
121 1.2328884601593018
122 1.0296767950057983
123 1.1812467575073242
124 1.0155644416809082
125 1.1435092687606812
126 1.1576130390167236
127 1.1675339937210083
128 0.9468926787376404
129 1.1167802810668945
130 1.0783665180206299
131 1.0213408470153809
132 0.9640718698501587
133 0.9479301571846008
134 0.9965620636940002
135 0.9497976899147034
136 1.106134295463562
137 0.8546898365020752
138 1.213611125946045
139 1.0463167428970337
140 1.0019997358322144
141 1.2208961248397827
142 1.146964430809021
143 1.0910991430282593
144 1.1447523832321167
145 1.0928325653076172
146 1.158751130104065
147 0.98101407289505
148 1.176712155342102
149 0.8813859820365906
150 1.068165898323059
151 1.1457200050354004
152 1.1151715517044067
153 1.0508439540863037
154 1.0705758333206177
155 1.0327478647232056
156 1.295284628868103
157 1.0945754051208496
158 1.1084654331207275
159 1.0526108741760254
160 1.169235110282898
161 0.9461066722869873
162 0.8947465419769287
163 1.0568523406982422
164 1.0866492986679077
165 1.0234392881393433
166 0.9120847582817078
167 1.0241153240203857
168 0.972747266292572
169 0.9622762203216553
170 1.1494688987731934
171 1.0169610977172852
172 0.9912289381027222
173 0.957065761089325
174 0.9772354364395142
175 0.964353084564209
176 1.1511540412902832
177 1.1626099348068237
178 1.1736366748809814
179 1.1521553993225098
180 1.0445438623428345
181 0.998833417892456
182 1.0449632406234741
183 1.0664812326431274
184 0.9556812047958374
185 0.9783785939216614
186 0.9009072780609131
187 1.0549708604812622
188 1.0322480201721191
189 1.1165590286254883
190 1.0295850038528442
191 0.9962092041969299
192 1.0832757949829102
193 1.163987636566162
194 1.157505750656128
195 0.9115658402442932
196 0.8871974349021912
197 1.0082939863204956
198 0.966276228427887
199 1.1153351068496704
.............
8334.630842152983 1469.8093489520252 344.9313699323684
.............
12 8334.630842152983 1469.8093489520252 344.9313699323684
.............
