0 2.8475255966186523
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.847526
1 2.822195529937744
2 2.7798099517822266
3 2.7148618698120117
4 2.6684844493865967
5 2.601767063140869
6 2.4624667167663574
7 2.413991928100586
8 2.262146472930908
9 2.1693613529205322
10 1.9786006212234497
11 1.857969045639038
12 1.7472206354141235
13 1.979072093963623
14 1.8689639568328857
15 1.924121379852295
16 1.7797644138336182
17 1.8592724800109863
18 1.7386339902877808
19 1.8623237609863281
20 1.9950789213180542
21 1.804010033607483
22 1.5060287714004517
23 1.828249216079712
24 1.653674602508545
25 1.8043923377990723
26 1.881069540977478
27 1.7871425151824951
28 1.4825571775436401
29 1.6701557636260986
30 1.6655868291854858
31 1.6743230819702148
32 1.639012336730957
33 1.6091610193252563
34 1.645548701286316
35 1.7427364587783813
36 1.537656545639038
37 1.6295106410980225
38 1.5282301902770996
39 1.5694009065628052
40 1.5754894018173218
41 1.626752495765686
42 1.6573002338409424
43 1.5251266956329346
44 1.4195294380187988
45 1.4699536561965942
46 1.722611665725708
47 1.5976372957229614
48 1.6726853847503662
49 1.6016508340835571
50 1.514992356300354
51 1.5271551609039307
52 1.5728830099105835
53 1.4845439195632935
54 1.282370924949646
55 1.5567617416381836
56 1.7690925598144531
57 1.5546716451644897
58 1.4573163986206055
59 1.6167573928833008
60 1.6662983894348145
61 1.5938228368759155
62 1.556309461593628
63 1.549770712852478
64 1.6217985153198242
65 1.584950566291809
66 1.4483745098114014
67 1.5235024690628052
68 1.505237102508545
69 1.569264531135559
70 1.416689157485962
71 1.614192008972168
72 1.415494680404663
73 1.4130723476409912
74 1.398422122001648
75 1.4006285667419434
76 1.3862266540527344
77 1.305618166923523
78 1.4722013473510742
79 1.5079045295715332
80 1.359156847000122
81 1.4257011413574219
82 1.513516902923584
83 1.4477752447128296
84 1.237937331199646
85 1.4809094667434692
86 1.6117783784866333
87 1.4373878240585327
88 1.2960565090179443
89 1.399929404258728
90 1.5271803140640259
91 1.6045275926589966
92 1.5718055963516235
93 1.441919207572937
94 1.4475691318511963
95 1.305362343788147
96 1.3754082918167114
97 1.6174348592758179
98 1.5542587041854858
99 1.3847562074661255
100 1.3376520872116089
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.337652
101 1.4966317415237427
102 1.3397650718688965
103 1.2879329919815063
104 1.4603149890899658
105 1.5204659700393677
106 1.3789846897125244
107 1.448486328125
108 1.363884687423706
109 1.5374183654785156
110 1.4360407590866089
111 1.3302885293960571
112 1.5461585521697998
113 1.4563310146331787
114 1.1820878982543945
115 1.3873813152313232
116 1.3446791172027588
117 1.3935014009475708
118 1.2544044256210327
119 1.2450422048568726
120 1.471174716949463
121 1.3941279649734497
122 1.329379916191101
123 1.481380820274353
124 1.2313356399536133
125 1.3247500658035278
126 1.3153259754180908
127 1.3991895914077759
128 1.1281862258911133
129 1.395929217338562
130 1.2441706657409668
131 1.301485538482666
132 1.2735185623168945
133 1.271843433380127
134 1.3033478260040283
135 1.1941444873809814
136 1.228976845741272
137 1.1276860237121582
138 1.392919898033142
139 1.115097999572754
140 1.3005077838897705
141 1.357752799987793
142 1.4124846458435059
143 1.3054893016815186
144 1.2755855321884155
145 1.3018367290496826
146 1.316817283630371
147 1.2708629369735718
148 1.419355869293213
149 1.1708930730819702
150 1.193270206451416
151 1.3031830787658691
152 1.3022513389587402
153 1.2281683683395386
154 1.3212388753890991
155 1.3531036376953125
156 1.4146569967269897
157 1.1819369792938232
158 1.3806734085083008
159 1.1928287744522095
160 1.3259730339050293
161 1.1334508657455444
162 1.093969464302063
163 1.1353071928024292
164 1.2366492748260498
165 1.1680964231491089
166 1.0979397296905518
167 1.2417094707489014
168 1.1881165504455566
169 1.1731027364730835
170 1.32478666305542
171 1.1874935626983643
172 1.2171087265014648
173 1.1646009683609009
174 1.126945972442627
175 1.109466791152954
176 1.2952615022659302
177 1.409761667251587
178 1.3344480991363525
179 1.3233046531677246
180 1.2213720083236694
181 1.1749823093414307
182 1.2177973985671997
183 1.3135837316513062
184 1.2186484336853027
185 1.1765527725219727
186 1.1044596433639526
187 1.2087757587432861
188 1.2380138635635376
189 1.3051527738571167
190 1.2601286172866821
191 1.2643722295761108
192 1.366956353187561
193 1.3520466089248657
194 1.2927296161651611
195 1.2785956859588623
196 1.0648399591445923
197 1.2664223909378052
198 1.172389268875122
199 1.3268818855285645
.............
53.4152075573802 103.61008188128471 16.116437109187245
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3202331066131592
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.320233
1 1.3952240943908691
2 1.326869010925293
3 1.241959571838379
4 1.22185480594635
5 1.4821492433547974
6 1.0690399408340454
7 1.3254897594451904
8 1.1721975803375244
9 1.3421998023986816
10 1.064029335975647
11 1.2459425926208496
12 1.1603741645812988
13 1.180016040802002
14 1.2011319398880005
15 1.2606122493743896
16 1.2495415210723877
17 1.2346160411834717
18 1.3380383253097534
19 1.1869605779647827
20 1.3583838939666748
21 1.2692995071411133
22 1.1742852926254272
23 1.3429325819015503
24 1.3001261949539185
25 1.1517763137817383
26 1.327263593673706
27 1.270270824432373
28 1.1516220569610596
29 1.1395891904830933
30 1.1522334814071655
31 1.2267041206359863
32 1.1920018196105957
33 1.2378402948379517
34 1.1865949630737305
35 1.3058799505233765
36 1.1393709182739258
37 1.1675057411193848
38 1.1337226629257202
39 1.2968668937683105
40 1.0669828653335571
41 1.2655540704727173
42 1.3718888759613037
43 1.0773835182189941
44 1.0785837173461914
45 1.1919244527816772
46 1.3309134244918823
47 1.1899511814117432
48 1.420223355293274
49 1.2927460670471191
50 1.302046537399292
51 1.1977472305297852
52 1.2546939849853516
53 1.1890087127685547
54 1.0106478929519653
55 1.0891844034194946
56 1.2941861152648926
57 1.179456353187561
58 1.2359230518341064
59 1.2708885669708252
60 1.2917773723602295
61 1.1837936639785767
62 1.2957358360290527
63 1.1753551959991455
64 1.1662356853485107
65 1.1659971475601196
66 1.1467925310134888
67 1.3725625276565552
68 1.0904927253723145
69 1.2597899436950684
70 1.2252473831176758
71 1.4197064638137817
72 1.1795305013656616
73 1.0955663919448853
74 1.241196870803833
75 1.2371236085891724
76 1.1371132135391235
77 1.005818486213684
78 1.2641940116882324
79 1.282070517539978
80 1.2623913288116455
81 1.2013522386550903
82 1.28623366355896
83 1.2514050006866455
84 0.9668694138526917
85 1.2475000619888306
86 1.3825796842575073
87 1.1831539869308472
88 1.0175303220748901
89 1.1624083518981934
90 1.3380955457687378
91 1.4320178031921387
92 1.3391335010528564
93 1.2211964130401611
94 1.2790422439575195
95 1.1149133443832397
96 1.172134518623352
97 1.3916858434677124
98 1.200149655342102
99 1.2414840459823608
100 1.1075513362884521
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.107551
101 1.3061213493347168
102 1.088081955909729
103 1.1025936603546143
104 1.2510267496109009
105 1.225239634513855
106 1.172909140586853
107 1.2181313037872314
108 1.109503149986267
109 1.350643277168274
110 1.2361156940460205
111 1.1292668581008911
112 1.296354055404663
113 1.2223886251449585
114 1.0988718271255493
115 1.2650461196899414
116 1.2018070220947266
117 1.2336968183517456
118 1.155584454536438
119 1.0731321573257446
120 1.2824662923812866
121 1.313120722770691
122 1.2138179540634155
123 1.3528486490249634
124 1.1079449653625488
125 1.211674690246582
126 1.2382930517196655
127 1.2780368328094482
128 1.0078192949295044
129 1.2591267824172974
130 1.1065952777862549
131 1.1677488088607788
132 1.1066813468933105
133 1.0686339139938354
134 1.1211857795715332
135 1.0238292217254639
136 1.1485931873321533
137 0.9771431684494019
138 1.290440320968628
139 1.059743881225586
140 1.1147671937942505
141 1.2281967401504517
142 1.2614461183547974
143 1.195357084274292
144 1.1835484504699707
145 1.1605896949768066
146 1.2187680006027222
147 1.1140360832214355
148 1.2880351543426514
149 0.9997811913490295
150 1.1041367053985596
151 1.1665692329406738
152 1.202315330505371
153 1.11020827293396
154 1.1662001609802246
155 1.1673142910003662
156 1.3461719751358032
157 1.0987542867660522
158 1.2238094806671143
159 1.1047084331512451
160 1.2130602598190308
161 1.0199085474014282
162 0.9550961256027222
163 1.0333501100540161
164 1.1122541427612305
165 1.096462368965149
166 1.00482976436615
167 1.1175928115844727
168 1.0487803220748901
169 1.0043741464614868
170 1.187543511390686
171 1.06882905960083
172 1.0706748962402344
173 1.0842015743255615
174 1.0144728422164917
175 0.9996957182884216
176 1.1657956838607788
177 1.2781977653503418
178 1.2502739429473877
179 1.2271816730499268
180 1.099798560142517
181 1.0376213788986206
182 1.1393203735351562
183 1.1667155027389526
184 1.011440634727478
185 1.0531479120254517
186 0.9772189259529114
187 1.1049268245697021
188 1.1080113649368286
189 1.156204342842102
190 1.1123069524765015
191 1.0847684144973755
192 1.1773808002471924
193 1.279221773147583
194 1.2564839124679565
195 1.0207089185714722
196 0.953723669052124
197 1.1498163938522339
198 1.0419931411743164
199 1.1839244365692139
.............
261.4813045542687 249.37638859264553 55.76079761236906
.............
0 1.1613564491271973
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.161356
1 1.3019859790802002
2 1.2497529983520508
3 1.1426597833633423
4 1.1368805170059204
5 1.3717339038848877
6 0.9227787256240845
7 1.1964415311813354
8 1.0178638696670532
9 1.2468328475952148
10 0.9450929760932922
11 1.11270010471344
12 1.0832523107528687
13 1.0424091815948486
14 1.1401578187942505
15 1.1315429210662842
16 1.1316919326782227
17 0.9970661997795105
18 1.2757439613342285
19 1.098442792892456
20 1.2416568994522095
21 1.1286534070968628
22 1.016589879989624
23 1.2175734043121338
24 1.1259537935256958
25 1.0505937337875366
26 1.2186640501022339
27 1.1357022523880005
28 1.1006016731262207
29 1.0679922103881836
30 1.0603280067443848
31 1.1078157424926758
32 1.0663175582885742
33 1.1381824016571045
34 1.0946186780929565
35 1.2049412727355957
36 1.0123075246810913
37 1.0904443264007568
38 1.0207693576812744
39 1.1644989252090454
40 0.9780628681182861
41 1.1716821193695068
42 1.3300790786743164
43 0.9288872480392456
44 0.9560419321060181
45 1.1103397607803345
46 1.2430499792099
47 1.131816029548645
48 1.3031703233718872
49 1.1953773498535156
50 1.1561636924743652
51 1.0985724925994873
52 1.2243342399597168
53 1.022986888885498
54 0.9201823472976685
55 1.0603159666061401
56 1.2041268348693848
57 1.0791488885879517
58 1.1465542316436768
59 1.2184261083602905
60 1.2363367080688477
61 1.134006381034851
62 1.189945936203003
63 1.0585689544677734
64 1.1268864870071411
65 1.1472480297088623
66 0.987415075302124
67 1.2297040224075317
68 1.0629640817642212
69 1.1718170642852783
70 1.069612979888916
71 1.3265526294708252
72 1.1173806190490723
73 1.0764864683151245
74 1.1896189451217651
75 1.1183995008468628
76 0.995650053024292
77 0.9586047530174255
78 1.2143696546554565
79 1.1314043998718262
80 1.101468563079834
81 1.1098250150680542
82 1.1597412824630737
83 1.0080196857452393
84 0.9600903391838074
85 1.162716269493103
86 1.3162245750427246
87 1.027147889137268
88 0.9071880578994751
89 1.0982033014297485
90 1.2602152824401855
91 1.3671958446502686
92 1.2717058658599854
93 1.1313444375991821
94 1.1580638885498047
95 1.033522605895996
96 1.1332024335861206
97 1.3785592317581177
98 1.1216696500778198
99 1.18659508228302
100 1.0011327266693115
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.001133
101 1.244467854499817
102 1.0928057432174683
103 1.0998222827911377
104 1.1610358953475952
105 1.1458326578140259
106 1.1395177841186523
107 1.1487139463424683
108 1.0071074962615967
109 1.3127957582473755
110 1.1415907144546509
111 1.0544846057891846
112 1.1683162450790405
113 1.2367734909057617
114 0.9735992550849915
115 1.1968265771865845
116 1.214155673980713
117 1.1677515506744385
118 0.9754836559295654
119 1.0253651142120361
120 1.231581449508667
121 1.2363332509994507
122 1.1076489686965942
123 1.295346975326538
124 1.0845504999160767
125 1.2240294218063354
126 1.2187749147415161
127 1.2433416843414307
128 0.9388506412506104
129 1.213995337486267
130 1.1125249862670898
131 1.079094409942627
132 1.1107977628707886
133 1.0279182195663452
134 1.048622965812683
135 0.9818586111068726
136 1.1432123184204102
137 0.9054944515228271
138 1.263742208480835
139 1.1346269845962524
140 1.0486550331115723
141 1.2316868305206299
142 1.214561104774475
143 1.140811800956726
144 1.1834995746612549
145 1.1370047330856323
146 1.1928191184997559
147 1.0351158380508423
148 1.2234201431274414
149 0.952915370464325
150 1.0840359926223755
151 1.1321451663970947
152 1.1581510305404663
153 1.0816978216171265
154 1.134184718132019
155 1.1270816326141357
156 1.3164100646972656
157 1.0510724782943726
158 1.1408408880233765
159 1.0959131717681885
160 1.1716676950454712
161 0.956447422504425
162 0.9108524322509766
163 0.9635991454124451
164 1.0437076091766357
165 1.0763298273086548
166 0.9547638893127441
167 1.0611482858657837
168 0.9751007556915283
169 0.9712204933166504
170 1.1613411903381348
171 1.0164294242858887
172 1.001960039138794
173 1.024398922920227
174 1.0004938840866089
175 0.979587197303772
176 1.158273458480835
177 1.2103731632232666
178 1.2277400493621826
179 1.1696439981460571
180 1.0574030876159668
181 1.0190787315368652
182 1.0971815586090088
183 1.1156309843063354
184 0.9793624877929688
185 1.0152586698532104
186 0.9315696954727173
187 1.0746747255325317
188 1.0555535554885864
189 1.1405858993530273
190 1.064802646636963
191 1.0312403440475464
192 1.1287240982055664
193 1.2478246688842773
194 1.2210241556167603
195 0.9591450691223145
196 0.9169192314147949
197 1.0817406177520752
198 1.0103559494018555
199 1.1562193632125854
.............
711.4168952144682 540.5393824242055 139.9248143658042
.............
0 1.1123552322387695
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.112355
1 1.2780447006225586
2 1.1968412399291992
3 1.1253068447113037
4 1.0862432718276978
5 1.3257250785827637
6 0.8980876207351685
7 1.167145013809204
8 0.9830052852630615
9 1.202976107597351
10 0.9117289185523987
11 1.0628681182861328
12 1.0441986322402954
13 1.002726674079895
14 1.1357227563858032
15 1.0909816026687622
16 1.0821421146392822
17 0.939293384552002
18 1.275386929512024
19 1.050950527191162
20 1.1974581480026245
21 1.0847365856170654
22 0.9649467468261719
23 1.1733375787734985
24 1.093858242034912
25 1.0137145519256592
26 1.197516918182373
27 1.085327386856079
28 1.1025997400283813
29 1.0595148801803589
30 1.031756043434143
31 1.1092036962509155
32 1.0260334014892578
33 1.117375373840332
34 1.0625158548355103
35 1.1748992204666138
36 1.0319825410842896
37 1.092392921447754
38 1.007469654083252
39 1.1444964408874512
40 0.9662491679191589
41 1.1380314826965332
42 1.2980302572250366
43 0.89914870262146
44 0.952674388885498
45 1.0888289213180542
46 1.208255648612976
47 1.1072094440460205
48 1.2814730405807495
49 1.130894422531128
50 1.1388652324676514
51 1.0672831535339355
52 1.1711454391479492
53 1.0134042501449585
54 0.9139783382415771
55 1.0236420631408691
56 1.1400154829025269
57 1.0509004592895508
58 1.1197237968444824
59 1.1623308658599854
60 1.1655019521713257
61 1.1146408319473267
62 1.1501251459121704
63 1.0015645027160645
64 1.066146731376648
65 1.0848408937454224
66 0.959168553352356
67 1.2041488885879517
68 1.0219554901123047
69 1.1170506477355957
70 1.019109845161438
71 1.2986973524093628
72 1.103700041770935
73 1.0570372343063354
74 1.158697485923767
75 1.0992121696472168
76 0.9726036787033081
77 0.9464011192321777
78 1.1999698877334595
79 1.100337028503418
80 1.0665576457977295
81 1.0941749811172485
82 1.1348572969436646
83 0.980675220489502
84 0.9393356442451477
85 1.1424243450164795
86 1.2849087715148926
87 0.9941487908363342
88 0.8976092338562012
89 1.0761303901672363
90 1.2533748149871826
91 1.3497318029403687
92 1.2626875638961792
93 1.1207400560379028
94 1.136061668395996
95 0.9973580837249756
96 1.1332333087921143
97 1.3071712255477905
98 1.0867713689804077
99 1.1604336500167847
100 0.9712572693824768
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.971257
101 1.2147178649902344
102 1.0780519247055054
103 1.1090072393417358
104 1.130887508392334
105 1.1136854887008667
106 1.1055681705474854
107 1.1202501058578491
108 0.9965984225273132
109 1.2907713651657104
110 1.0997883081436157
111 1.034915566444397
112 1.1385316848754883
113 1.1936959028244019
114 0.957990288734436
115 1.1550767421722412
116 1.1598408222198486
117 1.1351581811904907
118 0.9814584255218506
119 0.9853096604347229
120 1.1930537223815918
121 1.239906668663025
122 1.0599453449249268
123 1.2538222074508667
124 1.0371350049972534
125 1.1872808933258057
126 1.2014570236206055
127 1.2177433967590332
128 0.9452729225158691
129 1.1571208238601685
130 1.098212718963623
131 1.0381152629852295
132 1.025822639465332
133 0.9818645715713501
134 1.0207245349884033
135 0.9721673727035522
136 1.1370731592178345
137 0.8775238990783691
138 1.2414851188659668
139 1.1037269830703735
140 1.0344561338424683
141 1.2321192026138306
142 1.1788132190704346
143 1.1127419471740723
144 1.1799510717391968
145 1.1318410634994507
146 1.1874529123306274
147 0.9966556429862976
148 1.1946399211883545
149 0.9182155132293701
150 1.0930001735687256
151 1.1485108137130737
152 1.150735855102539
153 1.061895728111267
154 1.0991629362106323
155 1.1010501384735107
156 1.3064848184585571
157 1.0578739643096924
158 1.125866413116455
159 1.1030147075653076
160 1.1877636909484863
161 0.9484661221504211
162 0.9200847148895264
163 0.9929537773132324
164 1.0339322090148926
165 1.0499513149261475
166 0.9503554701805115
167 1.0426748991012573
168 0.9796398878097534
169 0.9601791501045227
170 1.1465245485305786
171 0.998908281326294
172 0.9848582744598389
173 0.9991257190704346
174 1.0002784729003906
175 0.9595305919647217
176 1.1543177366256714
177 1.190570592880249
178 1.1906685829162598
179 1.1652309894561768
180 1.0475331544876099
181 1.0020068883895874
182 1.0701324939727783
183 1.0959546566009521
184 0.9658203721046448
185 1.0148757696151733
186 0.908417820930481
187 1.0640547275543213
188 1.044264316558838
189 1.1077879667282104
190 1.0447437763214111
191 1.0140159130096436
192 1.0972479581832886
193 1.2003414630889893
194 1.1816911697387695
195 0.9304577112197876
196 0.9074499011039734
197 1.0534276962280273
198 0.9818174839019775
199 1.1290674209594727
.............
1604.0038213077933 1123.0188792459667 307.4155269805342
.............
0 1.0832303762435913
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.083230
1 1.2750548124313354
2 1.1835665702819824
3 1.109250783920288
4 1.0501781702041626
5 1.3004461526870728
6 0.8843309879302979
7 1.1436951160430908
8 0.965117871761322
9 1.186669945716858
10 0.9057323336601257
11 1.0393012762069702
12 1.0343904495239258
13 0.9978269934654236
14 1.1264698505401611
15 1.069185733795166
16 1.0550713539123535
17 0.9121113419532776
18 1.2643336057662964
19 1.0191023349761963
20 1.1811152696609497
21 1.0607620477676392
22 0.9351499080657959
23 1.1494089365005493
24 1.080451488494873
25 0.9855474829673767
26 1.1962714195251465
27 1.0471421480178833
28 1.0699360370635986
29 1.0509660243988037
30 0.9717592000961304
31 1.0497019290924072
32 0.9997422099113464
33 1.0720655918121338
34 1.0352966785430908
35 1.1666513681411743
36 0.9683316946029663
37 1.0609612464904785
38 0.9879001379013062
39 1.1808186769485474
40 0.9486168026924133
41 1.0940921306610107
42 1.2836521863937378
43 0.8935479521751404
44 0.935396134853363
45 1.074406623840332
46 1.2456669807434082
47 1.0519918203353882
48 1.2764288187026978
49 1.1285344362258911
50 1.1843416690826416
51 1.089125156402588
52 1.1120538711547852
53 1.0259517431259155
54 0.9070573449134827
55 1.0046967267990112
56 1.126802682876587
57 1.050415277481079
58 1.122939109802246
59 1.1481168270111084
60 1.1423077583312988
61 1.1180357933044434
62 1.152531385421753
63 0.9588516354560852
64 1.033886432647705
65 1.0506598949432373
66 0.9388429522514343
67 1.1696398258209229
68 1.012883186340332
69 1.081338882446289
70 0.9715738892555237
71 1.2411794662475586
72 1.0872164964675903
73 1.0234971046447754
74 1.1416205167770386
75 1.0508522987365723
76 0.9661973714828491
77 0.928243100643158
78 1.1660236120224
79 1.0687769651412964
80 1.0424247980117798
81 1.0689796209335327
82 1.119850993156433
83 0.9640102982521057
84 0.8962979316711426
85 1.118971824645996
86 1.2283856868743896
87 0.9661203026771545
88 0.8847453594207764
89 1.0492260456085205
90 1.2386201620101929
91 1.3305531740188599
92 1.2501215934753418
93 1.0993191003799438
94 1.1423090696334839
95 0.9546873569488525
96 1.1004074811935425
97 1.2153910398483276
98 1.0696310997009277
99 1.1234996318817139
100 0.9484201669692993
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.948420
101 1.1864277124404907
102 1.027857780456543
103 1.0604292154312134
104 1.1183925867080688
105 1.0888056755065918
106 1.0583527088165283
107 1.107226848602295
108 0.9873769283294678
109 1.2564173936843872
110 1.070512294769287
111 1.0153887271881104
112 1.085446834564209
113 1.170735239982605
114 0.927421510219574
115 1.1392656564712524
116 1.130648136138916
117 1.0976616144180298
118 0.9564315676689148
119 0.9484593272209167
120 1.1364816427230835
121 1.2438240051269531
122 1.0248439311981201
123 1.1767486333847046
124 1.019656777381897
125 1.1666781902313232
126 1.1703811883926392
127 1.1609752178192139
128 0.9504646062850952
129 1.0941389799118042
130 1.0752757787704468
131 1.0185965299606323
132 0.9597445130348206
133 0.9392638206481934
134 0.9883351922035217
135 0.949093222618103
136 1.0963077545166016
137 0.8510398864746094
138 1.20480215549469
139 1.0255358219146729
140 0.9899210929870605
141 1.214027762413025
142 1.1459485292434692
143 1.0710891485214233
144 1.1231800317764282
145 1.0919196605682373
146 1.1664958000183105
147 0.9710363745689392
148 1.171741008758545
149 0.8852909207344055
150 1.0413992404937744
151 1.120840311050415
152 1.0995160341262817
153 1.0409034490585327
154 1.0701442956924438
155 1.0404683351516724
156 1.2832646369934082
157 1.0514942407608032
158 1.0701619386672974
159 1.0578032732009888
160 1.1689715385437012
161 0.937010645866394
162 0.891175389289856
163 1.0189512968063354
164 1.040583848953247
165 1.0171927213668823
166 0.9415028095245361
167 1.0214396715164185
168 0.9534081816673279
169 0.9484426379203796
170 1.126657247543335
171 0.986855685710907
172 0.9872657656669617
173 0.9566743969917297
174 0.9647969603538513
175 0.9206786155700684
176 1.1235320568084717
177 1.1726067066192627
178 1.1677417755126953
179 1.1244268417358398
180 1.0286853313446045
181 0.9666895866394043
182 1.058936595916748
183 1.0640373229980469
184 0.9507442712783813
185 0.9818813800811768
186 0.8806724548339844
187 1.023878812789917
188 1.0146374702453613
189 1.0710606575012207
190 1.0118259191513062
191 0.9865885376930237
192 1.051931619644165
193 1.1520754098892212
194 1.1379117965698242
195 0.8998085856437683
196 0.884748637676239
197 1.0050506591796875
198 0.9512811303138733
199 1.0891731977462769
.............
3395.589901542291 2288.2341064326465 643.4674040358514
.............
6 3395.589901542291 2288.2341064326465 643.4674040358514
.............
