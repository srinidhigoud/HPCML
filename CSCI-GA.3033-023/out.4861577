0 2.8354997634887695
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.835500
1 2.818171501159668
2 2.7894046306610107
3 2.7422266006469727
4 2.696711540222168
5 2.6618621349334717
6 2.548640251159668
7 2.5161092281341553
8 2.400688409805298
9 2.311786413192749
10 2.16827130317688
11 1.9855598211288452
12 1.8831794261932373
13 1.9590193033218384
14 1.888844609260559
15 1.9328292608261108
16 1.9592243432998657
17 1.9040989875793457
18 1.707597017288208
19 1.7379835844039917
20 1.912427544593811
21 1.8682513236999512
22 1.6570968627929688
23 1.7353157997131348
24 1.6645855903625488
25 1.6681675910949707
26 1.8156529664993286
27 1.7892369031906128
28 1.4898604154586792
29 1.6759501695632935
30 1.6525349617004395
31 1.7179962396621704
32 1.5849981307983398
33 1.5987664461135864
34 1.650869607925415
35 1.7602381706237793
36 1.5465812683105469
37 1.6461185216903687
38 1.5683330297470093
39 1.5678279399871826
40 1.5646395683288574
41 1.5710865259170532
42 1.633314847946167
43 1.517808437347412
44 1.440863847732544
45 1.4501442909240723
46 1.7257418632507324
47 1.618870735168457
48 1.6854889392852783
49 1.5945111513137817
50 1.5182406902313232
51 1.5337378978729248
52 1.5703030824661255
53 1.5078606605529785
54 1.3290011882781982
55 1.5267119407653809
56 1.7171554565429688
57 1.563631296157837
58 1.471548318862915
59 1.5752002000808716
60 1.642076015472412
61 1.6112279891967773
62 1.5498181581497192
63 1.5068773031234741
64 1.5647904872894287
65 1.5409795045852661
66 1.427762746810913
67 1.527820110321045
68 1.4834574460983276
69 1.5365773439407349
70 1.3904730081558228
71 1.5927451848983765
72 1.4197258949279785
73 1.3932605981826782
74 1.3802604675292969
75 1.3953185081481934
76 1.3779127597808838
77 1.293269157409668
78 1.471122145652771
79 1.508743405342102
80 1.3615745306015015
81 1.4370678663253784
82 1.513521671295166
83 1.4455522298812866
84 1.239615797996521
85 1.485426902770996
86 1.6138676404953003
87 1.436968207359314
88 1.2943893671035767
89 1.4035096168518066
90 1.5251346826553345
91 1.6000972986221313
92 1.5677320957183838
93 1.4437037706375122
94 1.452904462814331
95 1.3094383478164673
96 1.37032151222229
97 1.6109306812286377
98 1.5428903102874756
99 1.386484980583191
100 1.3335845470428467
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.333585
101 1.4974325895309448
102 1.3502583503723145
103 1.2906876802444458
104 1.4613547325134277
105 1.5348201990127563
106 1.3772175312042236
107 1.4592673778533936
108 1.383816123008728
109 1.5327208042144775
110 1.4325238466262817
111 1.3326395750045776
112 1.5516085624694824
113 1.4600296020507812
114 1.183927059173584
115 1.3882907629013062
116 1.3497297763824463
117 1.3936164379119873
118 1.2599806785583496
119 1.2468394041061401
120 1.4735572338104248
121 1.3966196775436401
122 1.333096981048584
123 1.4832969903945923
124 1.2294831275939941
125 1.3293585777282715
126 1.3215785026550293
127 1.4042181968688965
128 1.129140019416809
129 1.3922317028045654
130 1.245874047279358
131 1.3020260334014893
132 1.274886965751648
133 1.2670073509216309
134 1.3213027715682983
135 1.2011584043502808
136 1.2320479154586792
137 1.148249864578247
138 1.398966670036316
139 1.115604281425476
140 1.3028210401535034
141 1.3672412633895874
142 1.402881145477295
143 1.3357871770858765
144 1.3036786317825317
145 1.3081789016723633
146 1.325228214263916
147 1.291368007659912
148 1.4289789199829102
149 1.1813846826553345
150 1.197745680809021
151 1.3235712051391602
152 1.310855507850647
153 1.2344070672988892
154 1.32759428024292
155 1.3689864873886108
156 1.414562702178955
157 1.1881734132766724
158 1.4089045524597168
159 1.199073314666748
160 1.3320808410644531
161 1.1394976377487183
162 1.104423999786377
163 1.1327968835830688
164 1.2328335046768188
165 1.1759623289108276
166 1.106544017791748
167 1.248252511024475
168 1.195969581604004
169 1.179879903793335
170 1.3344422578811646
171 1.1985883712768555
172 1.2179392576217651
173 1.168519139289856
174 1.1376943588256836
175 1.1295030117034912
176 1.2992122173309326
177 1.421174168586731
178 1.3417460918426514
179 1.328883171081543
180 1.2181220054626465
181 1.1822841167449951
182 1.2238537073135376
183 1.3131240606307983
184 1.2233165502548218
185 1.1835609674453735
186 1.1091583967208862
187 1.2102700471878052
188 1.2440896034240723
189 1.3169500827789307
190 1.261365532875061
191 1.2638851404190063
192 1.3824458122253418
193 1.3673176765441895
194 1.2949351072311401
195 1.289742112159729
196 1.075305461883545
197 1.2739183902740479
198 1.1848387718200684
199 1.3341144323349
.............
99.82901367172599 87.69298169575632 12.335693823173642
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.331395149230957
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.331395
1 1.400811791419983
2 1.3250361680984497
3 1.2484930753707886
4 1.2269514799118042
5 1.482627511024475
6 1.0902765989303589
7 1.3337578773498535
8 1.1776878833770752
9 1.3445392847061157
10 1.0682183504104614
11 1.256748080253601
12 1.1687260866165161
13 1.1909061670303345
14 1.2087219953536987
15 1.2704774141311646
16 1.262384295463562
17 1.2444010972976685
18 1.3405593633651733
19 1.184630274772644
20 1.3605860471725464
21 1.2705916166305542
22 1.1817632913589478
23 1.3472179174423218
24 1.3005554676055908
25 1.1505855321884155
26 1.328843116760254
27 1.2593425512313843
28 1.15153968334198
29 1.138109803199768
30 1.1531184911727905
31 1.2219204902648926
32 1.2054831981658936
33 1.2469663619995117
34 1.172919511795044
35 1.3012466430664062
36 1.1359726190567017
37 1.168480634689331
38 1.1321492195129395
39 1.2974804639816284
40 1.0700390338897705
41 1.26592218875885
42 1.3756077289581299
43 1.0803070068359375
44 1.0831942558288574
45 1.1953142881393433
46 1.3235446214675903
47 1.1944324970245361
48 1.435740351676941
49 1.2792190313339233
50 1.2969211339950562
51 1.1958954334259033
52 1.2544602155685425
53 1.1854978799819946
54 1.0216174125671387
55 1.0878413915634155
56 1.2949233055114746
57 1.1784929037094116
58 1.2394012212753296
59 1.2677429914474487
60 1.282098412513733
61 1.1863644123077393
62 1.2992641925811768
63 1.1709901094436646
64 1.1592940092086792
65 1.17555570602417
66 1.1462039947509766
67 1.3872795104980469
68 1.0917367935180664
69 1.260787844657898
70 1.2279573678970337
71 1.424117922782898
72 1.1844905614852905
73 1.1004852056503296
74 1.2415003776550293
75 1.2470053434371948
76 1.1425024271011353
77 1.0123335123062134
78 1.2649837732315063
79 1.2838903665542603
80 1.270229458808899
81 1.2088576555252075
82 1.282014012336731
83 1.263127088546753
84 0.9740309715270996
85 1.2561208009719849
86 1.3862838745117188
87 1.1896793842315674
88 1.025339961051941
89 1.1817972660064697
90 1.3321272134780884
91 1.427986741065979
92 1.3443108797073364
93 1.2294479608535767
94 1.2857012748718262
95 1.1121735572814941
96 1.1709315776824951
97 1.380832552909851
98 1.1999151706695557
99 1.239904522895813
100 1.109259843826294
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.109260
101 1.3125518560409546
102 1.084130048751831
103 1.1022027730941772
104 1.2546343803405762
105 1.2308164834976196
106 1.1746078729629517
107 1.2275952100753784
108 1.119953989982605
109 1.3419815301895142
110 1.2386153936386108
111 1.1374058723449707
112 1.3078789710998535
113 1.2207245826721191
114 1.0933438539505005
115 1.2673214673995972
116 1.2052395343780518
117 1.2222850322723389
118 1.1627402305603027
119 1.085086464881897
120 1.2871028184890747
121 1.2986069917678833
122 1.2170878648757935
123 1.366365909576416
124 1.1175838708877563
125 1.2020183801651
126 1.2332462072372437
127 1.2837568521499634
128 1.001988410949707
129 1.2698299884796143
130 1.1056480407714844
131 1.1731812953948975
132 1.1214827299118042
133 1.0783096551895142
134 1.1193346977233887
135 1.0331507921218872
136 1.151829719543457
137 0.9814265966415405
138 1.2935681343078613
139 1.0500982999801636
140 1.1104471683502197
141 1.2313953638076782
142 1.259110689163208
143 1.204514503479004
144 1.1860748529434204
145 1.164615273475647
146 1.231866478919983
147 1.126018762588501
148 1.297774314880371
149 1.0083638429641724
150 1.1026464700698853
151 1.1784946918487549
152 1.2055561542510986
153 1.1141297817230225
154 1.1663146018981934
155 1.1710245609283447
156 1.3475241661071777
157 1.1082721948623657
158 1.232325792312622
159 1.1031779050827026
160 1.2154357433319092
161 1.0225330591201782
162 0.9559440612792969
163 1.0419306755065918
164 1.1274796724319458
165 1.0900912284851074
166 0.9982724785804749
167 1.1135351657867432
168 1.0601704120635986
169 1.0132501125335693
170 1.2130759954452515
171 1.0840297937393188
172 1.072174310684204
173 1.0854787826538086
174 1.0187954902648926
175 1.0269572734832764
176 1.1733871698379517
177 1.2803550958633423
178 1.2684687376022339
179 1.2451838254928589
180 1.104170322418213
181 1.0488107204437256
182 1.1394199132919312
183 1.1776491403579712
184 1.0151286125183105
185 1.054724931716919
186 0.9878605604171753
187 1.1203327178955078
188 1.120001196861267
189 1.1728140115737915
190 1.121951937675476
191 1.091763973236084
192 1.1924374103546143
193 1.294276237487793
194 1.2622249126434326
195 1.0256835222244263
196 0.9602080583572388
197 1.1591403484344482
198 1.0504755973815918
199 1.1932158470153809
.............
380.0450441967696 220.25730233825743 41.33993181027472
.............
0 1.166892647743225
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.166893
1 1.30796480178833
2 1.2434827089309692
3 1.1454674005508423
4 1.1465847492218018
5 1.374098300933838
6 0.9201175570487976
7 1.204564094543457
8 1.0240391492843628
9 1.2517976760864258
10 0.9510373473167419
11 1.118905782699585
12 1.091614007949829
13 1.0509934425354004
14 1.1435439586639404
15 1.1363948583602905
16 1.139221429824829
17 1.003206729888916
18 1.2775346040725708
19 1.099211573600769
20 1.2449339628219604
21 1.1275876760482788
22 1.0187610387802124
23 1.2264364957809448
24 1.1356323957443237
25 1.0539238452911377
26 1.222977638244629
27 1.129454255104065
28 1.0899795293807983
29 1.0664970874786377
30 1.0577682256698608
31 1.089715838432312
32 1.0638583898544312
33 1.134984016418457
34 1.0924710035324097
35 1.210506558418274
36 1.0022584199905396
37 1.0887264013290405
38 1.0274869203567505
39 1.1710485219955444
40 0.9773570895195007
41 1.1716375350952148
42 1.3254714012145996
43 0.9404154419898987
44 0.9615263938903809
45 1.1128008365631104
46 1.249143123626709
47 1.122317910194397
48 1.3091641664505005
49 1.2067286968231201
50 1.179073452949524
51 1.1062625646591187
52 1.2246997356414795
53 1.0427836179733276
54 0.9096984267234802
55 1.0538538694381714
56 1.1982378959655762
57 1.087646722793579
58 1.156972885131836
59 1.2139538526535034
60 1.2330749034881592
61 1.1345188617706299
62 1.2007502317428589
63 1.0630292892456055
64 1.1363235712051392
65 1.1287668943405151
66 0.9933409094810486
67 1.2395039796829224
68 1.062522053718567
69 1.1575675010681152
70 1.082339882850647
71 1.3130159378051758
72 1.1222316026687622
73 1.0871795415878296
74 1.1856259107589722
75 1.097143530845642
76 1.0087131261825562
77 0.9622380137443542
78 1.2152981758117676
79 1.1346641778945923
80 1.0906497240066528
81 1.1192736625671387
82 1.177668571472168
83 1.0125720500946045
84 0.975298285484314
85 1.1741544008255005
86 1.3159998655319214
87 1.038543939590454
88 0.9163199067115784
89 1.098802089691162
90 1.2745637893676758
91 1.3776527643203735
92 1.2795172929763794
93 1.1510483026504517
94 1.1922756433486938
95 1.017259955406189
96 1.1589125394821167
97 1.3569188117980957
98 1.1302016973495483
99 1.1938115358352661
100 1.0043535232543945
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.004354
101 1.2488609552383423
102 1.1018273830413818
103 1.112270474433899
104 1.166926622390747
105 1.1464406251907349
106 1.135645866394043
107 1.1555273532867432
108 0.9993627667427063
109 1.3076062202453613
110 1.125848650932312
111 1.045487403869629
112 1.1740792989730835
113 1.1971288919448853
114 0.9682766199111938
115 1.1807905435562134
116 1.197291374206543
117 1.167790412902832
118 0.9837480187416077
119 1.0187489986419678
120 1.2264279127120972
121 1.2413225173950195
122 1.1065558195114136
123 1.3013516664505005
124 1.0821677446365356
125 1.1977330446243286
126 1.2156550884246826
127 1.2418265342712402
128 0.9486138224601746
129 1.2118620872497559
130 1.0954359769821167
131 1.080228567123413
132 1.0924890041351318
133 1.0188087224960327
134 1.060570240020752
135 0.9921558499336243
136 1.127891182899475
137 0.9025188684463501
138 1.2696146965026855
139 1.1370965242385864
140 1.0649428367614746
141 1.2426904439926147
142 1.2014613151550293
143 1.1576000452041626
144 1.203568696975708
145 1.1378520727157593
146 1.1996177434921265
147 1.0372856855392456
148 1.2155858278274536
149 0.9418156147003174
150 1.1223266124725342
151 1.1423351764678955
152 1.1643275022506714
153 1.0909669399261475
154 1.1392852067947388
155 1.121890664100647
156 1.3149789571762085
157 1.0675022602081299
158 1.1564571857452393
159 1.092444896697998
160 1.167811632156372
161 0.9617462158203125
162 0.9184292554855347
163 0.9745621681213379
164 1.0512977838516235
165 1.0685181617736816
166 0.9420455098152161
167 1.0562688112258911
168 0.984353244304657
169 0.9695961475372314
170 1.1707098484039307
171 1.019741415977478
172 1.0029188394546509
173 1.0209534168243408
174 0.9926662445068359
175 0.9789397716522217
176 1.1542658805847168
177 1.2088240385055542
178 1.2205166816711426
179 1.174217939376831
180 1.0585951805114746
181 1.0167946815490723
182 1.0878006219863892
183 1.1222107410430908
184 0.9802675843238831
185 1.0147230625152588
186 0.9354674816131592
187 1.0758744478225708
188 1.060050129890442
189 1.137639045715332
190 1.0680880546569824
191 1.0337413549423218
192 1.1314949989318848
193 1.2424668073654175
194 1.207105278968811
195 0.9607179760932922
196 0.9202450513839722
197 1.078330159187317
198 1.0093646049499512
199 1.157943606376648
.............
933.0901894979179 485.5412388443947 98.21907327324152
.............
0 1.1093446016311646
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.109345
1 1.277001142501831
2 1.1974923610687256
3 1.1258854866027832
4 1.0796217918395996
5 1.3289088010787964
6 0.8918448686599731
7 1.1657048463821411
8 0.984032154083252
9 1.2079612016677856
10 0.9148257374763489
11 1.062034249305725
12 1.0451146364212036
13 1.0064048767089844
14 1.1378731727600098
15 1.0896451473236084
16 1.0785644054412842
17 0.9424833059310913
18 1.2724130153656006
19 1.0509568452835083
20 1.1931415796279907
21 1.0868701934814453
22 0.9650044441223145
23 1.1794120073318481
24 1.1011223793029785
25 1.009924054145813
26 1.1996873617172241
27 1.0807253122329712
28 1.0924725532531738
29 1.063214898109436
30 1.0217225551605225
31 1.0995951890945435
32 1.0248011350631714
33 1.1134450435638428
34 1.0678330659866333
35 1.1815415620803833
36 1.0241386890411377
37 1.0858646631240845
38 1.0110769271850586
39 1.1523964405059814
40 0.9602687954902649
41 1.1418392658233643
42 1.2951951026916504
43 0.911133885383606
44 0.956935703754425
45 1.096262812614441
46 1.2275233268737793
47 1.0940014123916626
48 1.2819528579711914
49 1.1490408182144165
50 1.1558945178985596
51 1.073751449584961
52 1.1865710020065308
53 1.0228009223937988
54 0.9086942076683044
55 1.0213524103164673
56 1.1327682733535767
57 1.055810809135437
58 1.1264386177062988
59 1.1638364791870117
60 1.1686762571334839
61 1.114234447479248
62 1.156707763671875
63 1.005290150642395
64 1.0771880149841309
65 1.079102635383606
66 0.9667882323265076
67 1.2097468376159668
68 1.0190993547439575
69 1.1183961629867554
70 1.0340346097946167
71 1.288265347480774
72 1.1081569194793701
73 1.0649982690811157
74 1.1577380895614624
75 1.0794577598571777
76 0.9806380271911621
77 0.9489085674285889
78 1.2003066539764404
79 1.1108494997024536
80 1.0609917640686035
81 1.0910351276397705
82 1.1518282890319824
83 0.9858564138412476
84 0.9529663324356079
85 1.1447852849960327
86 1.265378713607788
87 1.0030946731567383
88 0.9062110185623169
89 1.0708881616592407
90 1.2591183185577393
91 1.3667025566101074
92 1.2727985382080078
93 1.1362600326538086
94 1.1822799444198608
95 0.9911451935768127
96 1.1288707256317139
97 1.281343936920166
98 1.099027156829834
99 1.1635103225708008
100 0.978415310382843
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.978415
101 1.2126060724258423
102 1.0744446516036987
103 1.0856387615203857
104 1.1380435228347778
105 1.1192947626113892
106 1.0935269594192505
107 1.1239566802978516
108 0.990140974521637
109 1.2881454229354858
110 1.1022371053695679
111 1.027514100074768
112 1.1376214027404785
113 1.1887271404266357
114 0.9571311473846436
115 1.1537612676620483
116 1.1599798202514648
117 1.1355881690979004
118 0.9915565252304077
119 0.9894309043884277
120 1.1946206092834473
121 1.240874171257019
122 1.0618104934692383
123 1.2553850412368774
124 1.0312471389770508
125 1.184844970703125
126 1.1980891227722168
127 1.2222723960876465
128 0.9553424119949341
129 1.1505091190338135
130 1.093626856803894
131 1.0379642248153687
132 1.0128411054611206
133 0.9809885621070862
134 1.0217868089675903
135 0.9824966192245483
136 1.1260063648223877
137 0.8776013851165771
138 1.2430516481399536
139 1.0927295684814453
140 1.0367542505264282
141 1.2372697591781616
142 1.1684107780456543
143 1.1249090433120728
144 1.189108967781067
145 1.1264420747756958
146 1.1915249824523926
147 0.9975762963294983
148 1.1867929697036743
149 0.9148291945457458
150 1.1065105199813843
151 1.1417912244796753
152 1.1563925743103027
153 1.0616157054901123
154 1.0863032341003418
155 1.0908774137496948
156 1.3128184080123901
157 1.076869249343872
158 1.1321059465408325
159 1.1037993431091309
160 1.1983442306518555
161 0.9571854472160339
162 0.9106895327568054
163 1.0006550550460815
164 1.0448681116104126
165 1.0450820922851562
166 0.9370438456535339
167 1.0411885976791382
168 0.9863954782485962
169 0.9611219167709351
170 1.1588307619094849
171 0.9984177350997925
172 0.9957886338233948
173 0.9942172169685364
174 1.005966067314148
175 0.9577761888504028
176 1.1531680822372437
177 1.1823177337646484
178 1.1782710552215576
179 1.1636054515838623
180 1.0470317602157593
181 1.005629062652588
182 1.0611625909805298
183 1.0971400737762451
184 0.9677175283432007
185 1.020758867263794
186 0.9202466607093811
187 1.0687330961227417
188 1.0517100095748901
189 1.1111648082733154
190 1.0462473630905151
191 1.0118958950042725
192 1.1058685779571533
193 1.2064701318740845
194 1.1791592836380005
195 0.9342289566993713
196 0.9128892421722412
197 1.0441745519638062
198 0.9815597534179688
199 1.1366654634475708
.............
2049.4208376146853 1016.5171404778957 213.15602738410234
.............
0 1.082863211631775
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.082863
1 1.2715964317321777
2 1.1908107995986938
3 1.1153843402862549
4 1.0443204641342163
5 1.3012642860412598
6 0.8764597177505493
7 1.1505823135375977
8 0.9680851697921753
9 1.1918704509735107
10 0.9127960801124573
11 1.0401917695999146
12 1.0416254997253418
13 1.0075733661651611
14 1.1300885677337646
15 1.072287917137146
16 1.0617738962173462
17 0.914991021156311
18 1.265505313873291
19 1.020147681236267
20 1.1847026348114014
21 1.0687518119812012
22 0.9342105984687805
23 1.1557105779647827
24 1.089414119720459
25 0.9827226996421814
26 1.1894468069076538
27 1.0470689535140991
28 1.055039405822754
29 1.049254298210144
30 0.9712307453155518
31 1.029815435409546
32 0.999035120010376
33 1.07774817943573
34 1.0265862941741943
35 1.1652653217315674
36 0.9736630320549011
37 1.0661219358444214
38 0.992641806602478
39 1.175553798675537
40 0.9443824291229248
41 1.087039589881897
42 1.2751384973526
43 0.9058202505111694
44 0.9312635660171509
45 1.0799944400787354
46 1.2387285232543945
47 1.0565497875213623
48 1.2870515584945679
49 1.11516535282135
50 1.18346107006073
51 1.0886669158935547
52 1.1121618747711182
53 1.027300477027893
54 0.9141777753829956
55 1.000910758972168
56 1.1236281394958496
57 1.0496089458465576
58 1.1221771240234375
59 1.1521459817886353
60 1.1276956796646118
61 1.109350323677063
62 1.1528780460357666
63 0.9473159909248352
64 1.0242431163787842
65 1.0467394590377808
66 0.9408906698226929
67 1.1721400022506714
68 1.002988338470459
69 1.081516981124878
70 0.9631736278533936
71 1.248786211013794
72 1.0854716300964355
73 1.026307463645935
74 1.139465093612671
75 1.0480320453643799
76 0.9653257727622986
77 0.9217170476913452
78 1.1676753759384155
79 1.083315372467041
80 1.0511380434036255
81 1.0601366758346558
82 1.1232655048370361
83 0.966296911239624
84 0.8899590969085693
85 1.1095402240753174
86 1.2172738313674927
87 0.9696251749992371
88 0.8898299932479858
89 1.0493617057800293
90 1.2278363704681396
91 1.3399839401245117
92 1.2634183168411255
93 1.1030433177947998
94 1.1480348110198975
95 0.9599682688713074
96 1.0875080823898315
97 1.2082750797271729
98 1.0741493701934814
99 1.1208906173706055
100 0.9415810108184814
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.941581
101 1.1787081956863403
102 1.0261695384979248
103 1.0515735149383545
104 1.1143988370895386
105 1.0891258716583252
106 1.0612863302230835
107 1.1012523174285889
108 0.979465126991272
109 1.2541433572769165
110 1.0812331438064575
111 1.0210771560668945
112 1.0897750854492188
113 1.1878595352172852
114 0.9490617513656616
115 1.1260724067687988
116 1.1263034343719482
117 1.098069190979004
118 0.982684850692749
119 0.9612929821014404
120 1.1440308094024658
121 1.2599889039993286
122 1.0394635200500488
123 1.1846805810928345
124 1.025434970855713
125 1.16864013671875
126 1.1776366233825684
127 1.170273780822754
128 0.9518662095069885
129 1.1028169393539429
130 1.0662750005722046
131 1.0211507081985474
132 0.9547073245048523
133 0.9427651166915894
134 1.004479169845581
135 0.9547979235649109
136 1.100346326828003
137 0.8546905517578125
138 1.214199423789978
139 1.040000557899475
140 1.002819538116455
141 1.2237896919250488
142 1.1359052658081055
143 1.0832716226577759
144 1.150758147239685
145 1.1035430431365967
146 1.1778608560562134
147 0.9730058312416077
148 1.1711045503616333
149 0.8833537101745605
150 1.0720536708831787
151 1.1370253562927246
152 1.1135432720184326
153 1.0378888845443726
154 1.069141149520874
155 1.044324517250061
156 1.2969729900360107
157 1.0856763124465942
158 1.092728853225708
159 1.0634331703186035
160 1.1902029514312744
161 0.9580531120300293
162 0.8938780426979065
163 1.047046422958374
164 1.0685497522354126
165 1.022585391998291
166 0.9276108741760254
167 1.0163615942001343
168 0.9756537079811096
169 0.954947292804718
170 1.1472313404083252
171 1.0057834386825562
172 0.9963065385818481
173 0.9646614789962769
174 0.9687347412109375
175 0.9449743032455444
176 1.1316990852355957
177 1.1690794229507446
178 1.1786282062530518
179 1.1451905965805054
180 1.0427381992340088
181 0.9843166470527649
182 1.0440741777420044
183 1.0715186595916748
184 0.9596682190895081
185 0.984754204750061
186 0.9038078188896179
187 1.043228030204773
188 1.0280637741088867
189 1.0949398279190063
190 1.0255321264266968
191 0.9955047369003296
192 1.0730324983596802
193 1.1638661623001099
194 1.1481095552444458
195 0.9118486046791077
196 0.8895596861839294
197 1.0054649114608765
198 0.9553301930427551
199 1.1095048189163208
.............
4269.217597771436 2078.1304522883147 440.7046520356089
.............
8 4269.217597771436 2078.1304522883147 440.7046520356089
.............
