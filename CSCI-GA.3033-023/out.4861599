0 2.82865834236145
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.828658
1 2.8083155155181885
2 2.772550582885742
3 2.7235827445983887
4 2.6813085079193115
5 2.6251535415649414
6 2.5036349296569824
7 2.4695682525634766
8 2.3344271183013916
9 2.227205753326416
10 2.055305242538452
11 1.9298579692840576
12 1.8078335523605347
13 2.0200676918029785
14 1.9053356647491455
15 1.991386890411377
16 1.8669949769973755
17 2.0045626163482666
18 1.7107837200164795
19 1.8066176176071167
20 1.9535304307937622
21 1.8403674364089966
22 1.5909100770950317
23 1.7809137105941772
24 1.6544921398162842
25 1.7489941120147705
26 1.8246209621429443
27 1.7764970064163208
28 1.5179200172424316
29 1.682523488998413
30 1.6614065170288086
31 1.7180956602096558
32 1.609446406364441
33 1.6047817468643188
34 1.6495904922485352
35 1.754091739654541
36 1.5451393127441406
37 1.645603895187378
38 1.5568780899047852
39 1.5754845142364502
40 1.5718916654586792
41 1.5925265550613403
42 1.6401046514511108
43 1.5169111490249634
44 1.4299159049987793
45 1.4603368043899536
46 1.7250508069992065
47 1.6150000095367432
48 1.6904865503311157
49 1.6067419052124023
50 1.5308798551559448
51 1.5431486368179321
52 1.5726666450500488
53 1.4983742237091064
54 1.3072352409362793
55 1.5658135414123535
56 1.7656954526901245
57 1.5589925050735474
58 1.470130205154419
59 1.6263881921768188
60 1.6849288940429688
61 1.6152797937393188
62 1.565067172050476
63 1.5410308837890625
64 1.612497091293335
65 1.5938220024108887
66 1.4678971767425537
67 1.5437452793121338
68 1.499755859375
69 1.5600935220718384
70 1.4218674898147583
71 1.6251533031463623
72 1.4418880939483643
73 1.415049433708191
74 1.387158989906311
75 1.402660846710205
76 1.399909496307373
77 1.3254536390304565
78 1.485524296760559
79 1.5171916484832764
80 1.3642282485961914
81 1.4427273273468018
82 1.5388792753219604
83 1.4793996810913086
84 1.2497121095657349
85 1.5020047426223755
86 1.6222953796386719
87 1.4579776525497437
88 1.324373722076416
89 1.4310600757598877
90 1.5352110862731934
91 1.6176131963729858
92 1.591465950012207
93 1.461453914642334
94 1.4561389684677124
95 1.3305621147155762
96 1.397395133972168
97 1.61763334274292
98 1.5706754922866821
99 1.4005110263824463
100 1.3503488302230835
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.350349
101 1.5122275352478027
102 1.369890570640564
103 1.307407259941101
104 1.479581356048584
105 1.5569894313812256
106 1.4017261266708374
107 1.4624550342559814
108 1.3823268413543701
109 1.5488835573196411
110 1.4542274475097656
111 1.3475476503372192
112 1.5528678894042969
113 1.4815291166305542
114 1.2032270431518555
115 1.398902177810669
116 1.3487846851348877
117 1.407389521598816
118 1.2588573694229126
119 1.2518916130065918
120 1.474796175956726
121 1.4075050354003906
122 1.3408311605453491
123 1.479149341583252
124 1.2440526485443115
125 1.3360519409179688
126 1.3227441310882568
127 1.419764518737793
128 1.1474370956420898
129 1.4038351774215698
130 1.261841893196106
131 1.3078258037567139
132 1.2825816869735718
133 1.2866346836090088
134 1.3118335008621216
135 1.2052702903747559
136 1.2381309270858765
137 1.1395742893218994
138 1.4089018106460571
139 1.1293363571166992
140 1.3057132959365845
141 1.3648616075515747
142 1.4151437282562256
143 1.3203539848327637
144 1.286327838897705
145 1.3163334131240845
146 1.3285883665084839
147 1.2863037586212158
148 1.429964542388916
149 1.191013216972351
150 1.1954410076141357
151 1.3211050033569336
152 1.3118728399276733
153 1.2394391298294067
154 1.3270387649536133
155 1.3693885803222656
156 1.4190312623977661
157 1.1884431838989258
158 1.3991199731826782
159 1.2043063640594482
160 1.3331468105316162
161 1.1410458087921143
162 1.1091856956481934
163 1.1308280229568481
164 1.2282522916793823
165 1.1802786588668823
166 1.1155366897583008
167 1.2517883777618408
168 1.2019238471984863
169 1.1830443143844604
170 1.3300448656082153
171 1.2024123668670654
172 1.2133700847625732
173 1.1829601526260376
174 1.153550624847412
175 1.1305363178253174
176 1.2996208667755127
177 1.4273324012756348
178 1.348179578781128
179 1.335270881652832
180 1.2167869806289673
181 1.195072054862976
182 1.2304624319076538
183 1.315464973449707
184 1.224657416343689
185 1.184760332107544
186 1.1082149744033813
187 1.221247673034668
188 1.24821138381958
189 1.3262356519699097
190 1.2697187662124634
191 1.2664989233016968
192 1.3756481409072876
193 1.3630075454711914
194 1.2993664741516113
195 1.2834771871566772
196 1.078603744506836
197 1.284623622894287
198 1.1751552820205688
199 1.3380604982376099
.............
143.97679156064987 171.84065861999989 3.9017599802464247
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3283591270446777
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.328359
1 1.3956080675125122
2 1.3249883651733398
3 1.2549223899841309
4 1.2282730340957642
5 1.4995428323745728
6 1.0888346433639526
7 1.3398059606552124
8 1.176861047744751
9 1.3529772758483887
10 1.070379614830017
11 1.253149390220642
12 1.1754519939422607
13 1.1946560144424438
14 1.2079367637634277
15 1.268937587738037
16 1.265995979309082
17 1.2275804281234741
18 1.3461397886276245
19 1.186767816543579
20 1.3758304119110107
21 1.270356297492981
22 1.1845943927764893
23 1.340590000152588
24 1.2810043096542358
25 1.1527973413467407
26 1.3337457180023193
27 1.276587724685669
28 1.1503390073776245
29 1.1427963972091675
30 1.1530805826187134
31 1.2187649011611938
32 1.1945644617080688
33 1.2433661222457886
34 1.1857542991638184
35 1.3070875406265259
36 1.145380973815918
37 1.173751950263977
38 1.128708004951477
39 1.2879302501678467
40 1.0752058029174805
41 1.268245816230774
42 1.3858352899551392
43 1.074690818786621
44 1.075433373451233
45 1.1961897611618042
46 1.3219642639160156
47 1.186560034751892
48 1.4220181703567505
49 1.284779667854309
50 1.2989072799682617
51 1.201979160308838
52 1.255284309387207
53 1.182666301727295
54 1.005401372909546
55 1.0895133018493652
56 1.3108364343643188
57 1.1804982423782349
58 1.2418904304504395
59 1.2728768587112427
60 1.2869675159454346
61 1.1847314834594727
62 1.298190951347351
63 1.1803994178771973
64 1.1631040573120117
65 1.1748334169387817
66 1.1510809659957886
67 1.3866745233535767
68 1.09530770778656
69 1.264326810836792
70 1.234825849533081
71 1.4250786304473877
72 1.1892902851104736
73 1.1037704944610596
74 1.234666109085083
75 1.2384216785430908
76 1.1584285497665405
77 1.0098172426223755
78 1.2541276216506958
79 1.2732552289962769
80 1.2786779403686523
81 1.2077054977416992
82 1.2652033567428589
83 1.2432503700256348
84 0.9661781787872314
85 1.2537285089492798
86 1.3818954229354858
87 1.1822612285614014
88 1.0093168020248413
89 1.1816939115524292
90 1.3321784734725952
91 1.4281110763549805
92 1.3404262065887451
93 1.2215582132339478
94 1.2769087553024292
95 1.1035699844360352
96 1.1685059070587158
97 1.3907603025436401
98 1.200415015220642
99 1.245134949684143
100 1.1064138412475586
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.106414
101 1.3112397193908691
102 1.0902029275894165
103 1.1018307209014893
104 1.2478783130645752
105 1.2232953310012817
106 1.168882131576538
107 1.2242064476013184
108 1.1069977283477783
109 1.35011625289917
110 1.250328540802002
111 1.1356616020202637
112 1.3022308349609375
113 1.235274076461792
114 1.1197589635849
115 1.2733159065246582
116 1.2005633115768433
117 1.231837511062622
118 1.1736936569213867
119 1.0762230157852173
120 1.2807211875915527
121 1.3083745241165161
122 1.2199597358703613
123 1.3536713123321533
124 1.1229697465896606
125 1.2055058479309082
126 1.2344472408294678
127 1.297274112701416
128 1.0012794733047485
129 1.2660659551620483
130 1.10769522190094
131 1.1687980890274048
132 1.1208984851837158
133 1.0796923637390137
134 1.115382432937622
135 1.0322110652923584
136 1.1479190587997437
137 0.9748682379722595
138 1.2971514463424683
139 1.068423867225647
140 1.1148059368133545
141 1.2270536422729492
142 1.2615834474563599
143 1.1950169801712036
144 1.1868607997894287
145 1.163783311843872
146 1.2259633541107178
147 1.1272995471954346
148 1.290544867515564
149 1.0029056072235107
150 1.1124323606491089
151 1.1723079681396484
152 1.1981955766677856
153 1.1135568618774414
154 1.1817622184753418
155 1.1751759052276611
156 1.3475733995437622
157 1.0909236669540405
158 1.222740650177002
159 1.1126078367233276
160 1.210036039352417
161 1.0177316665649414
162 0.957435667514801
163 1.0218299627304077
164 1.1060709953308105
165 1.1066656112670898
166 1.010718584060669
167 1.1185599565505981
168 1.0409495830535889
169 1.0113729238510132
170 1.1895887851715088
171 1.0737433433532715
172 1.070723056793213
173 1.0805096626281738
174 1.004225492477417
175 1.0056331157684326
176 1.1643493175506592
177 1.2868598699569702
178 1.2586276531219482
179 1.2256544828414917
180 1.0979567766189575
181 1.0363128185272217
182 1.1493127346038818
183 1.1707721948623657
184 1.0139076709747314
185 1.0578970909118652
186 0.9763990640640259
187 1.1039760112762451
188 1.1112171411514282
189 1.1612365245819092
190 1.112420916557312
191 1.080374002456665
192 1.1859034299850464
193 1.2875568866729736
194 1.2543491125106812
195 1.0181593894958496
196 0.9553678631782532
197 1.1550790071487427
198 1.0391247272491455
199 1.1850734949111938
.............
464.9005296714604 565.2125212997198 23.049389054998755
.............
0 1.1568422317504883
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.156842
1 1.3059295415878296
2 1.2607039213180542
3 1.1453529596328735
4 1.139587640762329
5 1.386073112487793
6 0.9217912554740906
7 1.2081774473190308
8 1.017764687538147
9 1.2520843744277954
10 0.9439108967781067
11 1.1127418279647827
12 1.0818171501159668
13 1.041812777519226
14 1.144988775253296
15 1.1280757188796997
16 1.1352088451385498
17 0.9993152022361755
18 1.282875657081604
19 1.1007081270217896
20 1.2458468675613403
21 1.1355793476104736
22 1.0166569948196411
23 1.221828818321228
24 1.1272203922271729
25 1.052088737487793
26 1.2205688953399658
27 1.1322808265686035
28 1.0933679342269897
29 1.0727168321609497
30 1.0587579011917114
31 1.106072187423706
32 1.0696102380752563
33 1.137987732887268
34 1.094234585762024
35 1.2109572887420654
36 1.0117156505584717
37 1.0929639339447021
38 1.0215131044387817
39 1.1600570678710938
40 0.9790471792221069
41 1.1705214977264404
42 1.3251218795776367
43 0.9215921759605408
44 0.9594875574111938
45 1.113747000694275
46 1.2426979541778564
47 1.1302382946014404
48 1.3004176616668701
49 1.2003834247589111
50 1.1587119102478027
51 1.1001912355422974
52 1.2380454540252686
53 1.0282214879989624
54 0.9213590025901794
55 1.0563260316848755
56 1.1995259523391724
57 1.0799981355667114
58 1.143197774887085
59 1.2136707305908203
60 1.2317137718200684
61 1.1355412006378174
62 1.1926836967468262
63 1.055896282196045
64 1.1220293045043945
65 1.140559434890747
66 0.9904299378395081
67 1.2341324090957642
68 1.0625110864639282
69 1.1743192672729492
70 1.0748744010925293
71 1.3278628587722778
72 1.122210144996643
73 1.081949234008789
74 1.1849114894866943
75 1.113357663154602
76 1.0083507299423218
77 0.9629663228988647
78 1.2129524946212769
79 1.1278026103973389
80 1.1036397218704224
81 1.112005352973938
82 1.1716511249542236
83 1.0123497247695923
84 0.9709318280220032
85 1.1655346155166626
86 1.3159918785095215
87 1.0363366603851318
88 0.9096685647964478
89 1.096889615058899
90 1.263533353805542
91 1.3698399066925049
92 1.2755731344223022
93 1.1347311735153198
94 1.1757535934448242
95 1.0283887386322021
96 1.1408761739730835
97 1.3732472658157349
98 1.1237462759017944
99 1.189746618270874
100 1.003240942955017
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.003241
101 1.2519652843475342
102 1.0895414352416992
103 1.1107826232910156
104 1.1631661653518677
105 1.1518322229385376
106 1.1442915201187134
107 1.1505918502807617
108 1.0091710090637207
109 1.3095381259918213
110 1.134787917137146
111 1.0552088022232056
112 1.1752045154571533
113 1.2360576391220093
114 0.9786170125007629
115 1.1913472414016724
116 1.2133013010025024
117 1.1744781732559204
118 0.9799726009368896
119 1.0228184461593628
120 1.2406269311904907
121 1.239711046218872
122 1.1170529127120972
123 1.3040859699249268
124 1.0938527584075928
125 1.2159957885742188
126 1.2233158349990845
127 1.2412317991256714
128 0.9462249875068665
129 1.228075385093689
130 1.1117643117904663
131 1.0843013525009155
132 1.1063646078109741
133 1.0337700843811035
134 1.0516924858093262
135 0.9877520203590393
136 1.1300432682037354
137 0.9082058072090149
138 1.2701736688613892
139 1.127000093460083
140 1.0593140125274658
141 1.2328282594680786
142 1.2113527059555054
143 1.149014949798584
144 1.1911306381225586
145 1.13237464427948
146 1.193263292312622
147 1.0407239198684692
148 1.214474081993103
149 0.9453039765357971
150 1.113157868385315
151 1.1324270963668823
152 1.1548266410827637
153 1.0947140455245972
154 1.1413990259170532
155 1.119489312171936
156 1.3154313564300537
157 1.0569498538970947
158 1.1503870487213135
159 1.084476113319397
160 1.1694637537002563
161 0.9614464640617371
162 0.9028061032295227
163 0.968757152557373
164 1.0505470037460327
165 1.0743767023086548
166 0.9460195302963257
167 1.061213731765747
168 0.9736610651016235
169 0.9717478156089783
170 1.1681737899780273
171 1.01508367061615
172 1.0056949853897095
173 1.0201339721679688
174 0.9893258810043335
175 0.9733696579933167
176 1.1488779783248901
177 1.2119122743606567
178 1.218766689300537
179 1.1681263446807861
180 1.055883765220642
181 1.0164968967437744
182 1.0984293222427368
183 1.1177394390106201
184 0.9779442548751831
185 1.022694706916809
186 0.9307515621185303
187 1.072617769241333
188 1.0613651275634766
189 1.1395747661590576
190 1.0683090686798096
191 1.0300683975219727
192 1.1353883743286133
193 1.2503845691680908
194 1.2137317657470703
195 0.9639633893966675
196 0.9204363822937012
197 1.082400918006897
198 1.0105856657028198
199 1.1593741178512573
.............
1112.2055444326252 1360.4632598329335 61.65858293324709
.............
0 1.1071964502334595
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.107196
1 1.2789099216461182
2 1.2031948566436768
3 1.1258968114852905
4 1.0802780389785767
5 1.3353986740112305
6 0.8989689350128174
7 1.1707960367202759
8 0.9823570847511292
9 1.210323691368103
10 0.910484790802002
11 1.0619007349014282
12 1.0423524379730225
13 1.0032154321670532
14 1.1399662494659424
15 1.0886847972869873
16 1.0829591751098633
17 0.9433131217956543
18 1.278249740600586
19 1.0541990995407104
20 1.1970478296279907
21 1.0931847095489502
22 0.9666725993156433
23 1.1805176734924316
24 1.0985029935836792
25 1.013501763343811
26 1.1974058151245117
27 1.0835179090499878
28 1.085997223854065
29 1.0633476972579956
30 1.0288804769515991
31 1.098046064376831
32 1.0309298038482666
33 1.1105895042419434
34 1.06368887424469
35 1.1784796714782715
36 1.0303972959518433
37 1.0867959260940552
38 1.0096288919448853
39 1.1425516605377197
40 0.9624062180519104
41 1.1386128664016724
42 1.2972614765167236
43 0.9006056189537048
44 0.9535977840423584
45 1.095070242881775
46 1.2113490104675293
47 1.0967707633972168
48 1.285069227218628
49 1.1368430852890015
50 1.145105004310608
51 1.0730115175247192
52 1.1814993619918823
53 1.0211032629013062
54 0.9138836860656738
55 1.0231456756591797
56 1.138933539390564
57 1.0511153936386108
58 1.1241908073425293
59 1.158675193786621
60 1.1709442138671875
61 1.1126827001571655
62 1.1530183553695679
63 1.0098278522491455
64 1.072792887687683
65 1.082253098487854
66 0.9690331816673279
67 1.2136229276657104
68 1.0247246026992798
69 1.129908800125122
70 1.0351897478103638
71 1.289024829864502
72 1.1073731184005737
73 1.0718495845794678
74 1.1620959043502808
75 1.0829373598098755
76 0.9832127094268799
77 0.9526994228363037
78 1.200333595275879
79 1.1074060201644897
80 1.0727077722549438
81 1.084626317024231
82 1.15104341506958
83 0.9884380102157593
84 0.9600068926811218
85 1.1419928073883057
86 1.2729127407073975
87 1.0074018239974976
88 0.904812216758728
89 1.0738847255706787
90 1.259007453918457
91 1.360681176185608
92 1.2702480554580688
93 1.1323450803756714
94 1.181833267211914
95 0.993324875831604
96 1.1292344331741333
97 1.2999937534332275
98 1.0944563150405884
99 1.164133906364441
100 0.9812151193618774
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.981215
101 1.22308349609375
102 1.0791294574737549
103 1.1124176979064941
104 1.1382942199707031
105 1.1260713338851929
106 1.1073968410491943
107 1.1272943019866943
108 0.9936513304710388
109 1.287484884262085
110 1.0983189344406128
111 1.038219928741455
112 1.1454439163208008
113 1.197363018989563
114 0.9636715054512024
115 1.1532589197158813
116 1.1648284196853638
117 1.1444902420043945
118 0.98467618227005
119 0.9874469637870789
120 1.197373867034912
121 1.2400448322296143
122 1.0694258213043213
123 1.2694653272628784
124 1.0433170795440674
125 1.1830155849456787
126 1.2098138332366943
127 1.222381830215454
128 0.9513364434242249
129 1.173211932182312
130 1.100066065788269
131 1.039547085762024
132 1.026358723640442
133 0.9896495342254639
134 1.023959994316101
135 0.9771338701248169
136 1.1278653144836426
137 0.8803367018699646
138 1.2502135038375854
139 1.1019864082336426
140 1.0376958847045898
141 1.234722375869751
142 1.1793581247329712
143 1.1179556846618652
144 1.1794015169143677
145 1.1246258020401
146 1.1944597959518433
147 1.000264048576355
148 1.1822078227996826
149 0.9203670620918274
150 1.1077375411987305
151 1.1365597248077393
152 1.1467010974884033
153 1.071730136871338
154 1.0923305749893188
155 1.0893789529800415
156 1.3061916828155518
157 1.0590022802352905
158 1.1391971111297607
159 1.0927033424377441
160 1.176519751548767
161 0.9522958993911743
162 0.9125466346740723
163 0.9950973391532898
164 1.0407403707504272
165 1.0501731634140015
166 0.9339231848716736
167 1.0410887002944946
168 0.9760006666183472
169 0.9610186219215393
170 1.1599352359771729
171 0.9989147782325745
172 0.9826034307479858
173 0.9976291656494141
174 0.9951754212379456
175 0.9708298444747925
176 1.1555125713348389
177 1.1870696544647217
178 1.1980310678482056
179 1.1668592691421509
180 1.0429832935333252
181 1.0089870691299438
182 1.068798542022705
183 1.0943503379821777
184 0.961285412311554
185 1.0159578323364258
186 0.9135429859161377
187 1.0681028366088867
188 1.0439043045043945
189 1.1184968948364258
190 1.0485234260559082
191 1.0119673013687134
192 1.1111639738082886
193 1.210062026977539
194 1.178371787071228
195 0.9346228837966919
196 0.9074211120605469
197 1.0534493923187256
198 0.9815429449081421
199 1.137751579284668
.............
2391.1410541534424 2935.979319419712 137.60376904532313
.............
0 1.0831454992294312
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.083145
1 1.2735902070999146
2 1.1912181377410889
3 1.110998272895813
4 1.0520039796829224
5 1.307538628578186
6 0.8795303106307983
7 1.1459904909133911
8 0.9694817066192627
9 1.1853587627410889
10 0.901913046836853
11 1.0391042232513428
12 1.0337111949920654
13 0.9944028258323669
14 1.1280279159545898
15 1.0714529752731323
16 1.0616649389266968
17 0.9159137010574341
18 1.2710241079330444
19 1.0230690240859985
20 1.181700348854065
21 1.0668671131134033
22 0.9381958246231079
23 1.1573481559753418
24 1.0849230289459229
25 0.9891011118888855
26 1.189225196838379
27 1.0482977628707886
28 1.0582990646362305
29 1.0509718656539917
30 0.9791066646575928
31 1.0458309650421143
32 1.0064529180526733
33 1.0740140676498413
34 1.0334258079528809
35 1.1669435501098633
36 0.986145555973053
37 1.066213846206665
38 0.9968258142471313
39 1.1670536994934082
40 0.9448168277740479
41 1.1034103631973267
42 1.2781850099563599
43 0.8887057304382324
44 0.9374158382415771
45 1.0809398889541626
46 1.2222697734832764
47 1.0581941604614258
48 1.276809573173523
49 1.1215497255325317
50 1.1715214252471924
51 1.0755527019500732
52 1.1407084465026855
53 1.0269057750701904
54 0.9063819646835327
55 1.0022377967834473
56 1.1125574111938477
57 1.0462076663970947
58 1.1159331798553467
59 1.1331371068954468
60 1.1489754915237427
61 1.1079697608947754
62 1.134478211402893
63 0.9676046967506409
64 1.0521976947784424
65 1.0548527240753174
66 0.9418573975563049
67 1.1743072271347046
68 1.0166316032409668
69 1.0877963304519653
70 0.9919260144233704
71 1.248511791229248
72 1.0937238931655884
73 1.0366582870483398
74 1.1453608274459839
75 1.0521773099899292
76 0.9688162207603455
77 0.931286096572876
78 1.175082802772522
79 1.0789141654968262
80 1.0539121627807617
81 1.0598939657211304
82 1.1297153234481812
83 0.9698516726493835
84 0.9093560576438904
85 1.1203393936157227
86 1.2275563478469849
87 0.9754514098167419
88 0.8885844945907593
89 1.0500669479370117
90 1.2347909212112427
91 1.3429850339889526
92 1.260987401008606
93 1.106574296951294
94 1.1582914590835571
95 0.9610269665718079
96 1.08869469165802
97 1.2292070388793945
98 1.0778087377548218
99 1.1255571842193604
100 0.94742751121521
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.947428
101 1.1913435459136963
102 1.035068154335022
103 1.0699965953826904
104 1.1181641817092896
105 1.1007660627365112
106 1.0699412822723389
107 1.1018154621124268
108 0.9840537905693054
109 1.2597630023956299
110 1.0816466808319092
111 1.0239864587783813
112 1.0995312929153442
113 1.193116307258606
114 0.9499173760414124
115 1.1386058330535889
116 1.1384450197219849
117 1.1116032600402832
118 0.9768900275230408
119 0.9592118859291077
120 1.1522217988967896
121 1.251072883605957
122 1.042335867881775
123 1.206058382987976
124 1.0225387811660767
125 1.1718822717666626
126 1.1927179098129272
127 1.1751092672348022
128 0.9558395147323608
129 1.1182184219360352
130 1.0880589485168457
131 1.021033763885498
132 0.9696022868156433
133 0.9541762471199036
134 0.9976935386657715
135 0.9579228162765503
136 1.0964512825012207
137 0.8602813482284546
138 1.219895362854004
139 1.054214596748352
140 1.010347843170166
141 1.2240875959396362
142 1.1496354341506958
143 1.0917249917984009
144 1.1533950567245483
145 1.101242184638977
146 1.175746202468872
147 0.9727770090103149
148 1.1667778491973877
149 0.8914684057235718
150 1.0887662172317505
151 1.1338306665420532
152 1.1207373142242432
153 1.0462522506713867
154 1.0687875747680664
155 1.0541539192199707
156 1.3004873991012573
157 1.0796821117401123
158 1.1048545837402344
159 1.0711791515350342
160 1.1818389892578125
161 0.9532756209373474
162 0.9009636640548706
163 1.0467932224273682
164 1.0690749883651733
165 1.03012216091156
166 0.9199922680854797
167 1.0231482982635498
168 0.9750260710716248
169 0.9596039652824402
170 1.1502482891082764
171 1.0048468112945557
172 0.9823350310325623
173 0.9684721231460571
174 0.9809308052062988
175 0.9616773128509521
176 1.1496390104293823
177 1.1658252477645874
178 1.1759475469589233
179 1.155564785003662
180 1.043988823890686
181 1.001231074333191
182 1.0536764860153198
183 1.073459267616272
184 0.9545573592185974
185 0.9899248480796814
186 0.8990622758865356
187 1.0529396533966064
188 1.0252951383590698
189 1.098381519317627
190 1.029646396636963
191 0.9957243204116821
192 1.0867817401885986
193 1.174595832824707
194 1.1602262258529663
195 0.9107093811035156
196 0.8918765187263489
197 1.0181567668914795
198 0.9602372646331787
199 1.1153696775436401
.............
4963.730237074196 6102.241151502356 288.4158829636872
.............
14 4963.730237074196 6102.241151502356 288.4158829636872
.............
