0 2.7998850345611572
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.799885
1 2.7895865440368652
2 2.7467005252838135
3 2.6970937252044678
4 2.6593539714813232
5 2.585301160812378
6 2.4340381622314453
7 2.386293888092041
8 2.2276859283447266
9 2.1798622608184814
10 1.9946112632751465
11 1.9021902084350586
12 1.785987377166748
13 2.025228500366211
14 1.891048789024353
15 1.9133967161178589
16 1.7941186428070068
17 1.9088923931121826
18 1.7317813634872437
19 1.8574588298797607
20 1.9704567193984985
21 1.8168869018554688
22 1.5361847877502441
23 1.7945189476013184
24 1.6594613790512085
25 1.7492343187332153
26 1.8511111736297607
27 1.7924975156784058
28 1.4871413707733154
29 1.6729629039764404
30 1.6598303318023682
31 1.7240883111953735
32 1.6179625988006592
33 1.6159873008728027
34 1.6236655712127686
35 1.7370047569274902
36 1.5336860418319702
37 1.6358973979949951
38 1.5552153587341309
39 1.5700803995132446
40 1.5577034950256348
41 1.6028201580047607
42 1.6371676921844482
43 1.5313628911972046
44 1.4322541952133179
45 1.4682960510253906
46 1.7273668050765991
47 1.6193199157714844
48 1.6862115859985352
49 1.6026746034622192
50 1.5169460773468018
51 1.5296947956085205
52 1.5784552097320557
53 1.5063022375106812
54 1.3003997802734375
55 1.545203447341919
56 1.757083535194397
57 1.5682616233825684
58 1.4713536500930786
59 1.5891554355621338
60 1.6577566862106323
61 1.6219059228897095
62 1.5538389682769775
63 1.5179717540740967
64 1.5856326818466187
65 1.5623371601104736
66 1.445195198059082
67 1.5248498916625977
68 1.5004987716674805
69 1.5412176847457886
70 1.4081778526306152
71 1.6124789714813232
72 1.43013596534729
73 1.4163999557495117
74 1.381670355796814
75 1.4024686813354492
76 1.3917886018753052
77 1.3181437253952026
78 1.4848743677139282
79 1.5182996988296509
80 1.3688328266143799
81 1.440117597579956
82 1.5286674499511719
83 1.4772772789001465
84 1.249169111251831
85 1.5050982236862183
86 1.6258893013000488
87 1.4560050964355469
88 1.3194600343704224
89 1.4326993227005005
90 1.5420422554016113
91 1.6234748363494873
92 1.5930088758468628
93 1.4636574983596802
94 1.4550063610076904
95 1.3287224769592285
96 1.396872878074646
97 1.639771580696106
98 1.5902270078659058
99 1.411064624786377
100 1.366963505744934
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.366964
101 1.5163145065307617
102 1.384706735610962
103 1.3149847984313965
104 1.4886382818222046
105 1.5645374059677124
106 1.4131869077682495
107 1.4650496244430542
108 1.389563798904419
109 1.5654957294464111
110 1.469506025314331
111 1.3615913391113281
112 1.5525097846984863
113 1.4838224649429321
114 1.2147536277770996
115 1.4088070392608643
116 1.355556845664978
117 1.416288137435913
118 1.2699130773544312
119 1.2682613134384155
120 1.489723563194275
121 1.4265754222869873
122 1.3555619716644287
123 1.5011197328567505
124 1.251743197441101
125 1.34319269657135
126 1.3324085474014282
127 1.434159517288208
128 1.1730303764343262
129 1.4173901081085205
130 1.2718185186386108
131 1.322522759437561
132 1.2859458923339844
133 1.299473762512207
134 1.3250477313995361
135 1.2202483415603638
136 1.2503058910369873
137 1.1428580284118652
138 1.4171234369277954
139 1.135764241218567
140 1.322328805923462
141 1.3682092428207397
142 1.424034595489502
143 1.325933814048767
144 1.3016940355300903
145 1.3261457681655884
146 1.3337059020996094
147 1.304502248764038
148 1.4429826736450195
149 1.1988270282745361
150 1.1935014724731445
151 1.3397380113601685
152 1.3264161348342896
153 1.2570730447769165
154 1.352819561958313
155 1.3947780132293701
156 1.4174586534500122
157 1.1937447786331177
158 1.4124449491500854
159 1.2091633081436157
160 1.3532333374023438
161 1.1516194343566895
162 1.127756953239441
163 1.1338824033737183
164 1.2376461029052734
165 1.1934325695037842
166 1.124675989151001
167 1.2639182806015015
168 1.2103271484375
169 1.185892939567566
170 1.333654761314392
171 1.204850673675537
172 1.2304699420928955
173 1.1809921264648438
174 1.148626685142517
175 1.1427924633026123
176 1.3044102191925049
177 1.4334666728973389
178 1.3622064590454102
179 1.3519151210784912
180 1.2241536378860474
181 1.193013072013855
182 1.2430319786071777
183 1.3238469362258911
184 1.2173761129379272
185 1.1865371465682983
186 1.120088815689087
187 1.213802456855774
188 1.2493375539779663
189 1.3272205591201782
190 1.2664752006530762
191 1.2674919366836548
192 1.3977664709091187
193 1.378636360168457
194 1.3186697959899902
195 1.3339757919311523
196 1.0847088098526
197 1.2886677980422974
198 1.2099603414535522
199 1.3520021438598633
.............
88.8201899509877 141.07918679527938 8.985350135713816
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3404470682144165
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.340447
1 1.3966641426086426
2 1.3400965929031372
3 1.2769337892532349
4 1.2355860471725464
5 1.4811238050460815
6 1.109958291053772
7 1.3354828357696533
8 1.1782053709030151
9 1.3603214025497437
10 1.0741432905197144
11 1.2677537202835083
12 1.1757110357284546
13 1.2011480331420898
14 1.2162212133407593
15 1.2736310958862305
16 1.2698880434036255
17 1.24346125125885
18 1.345262050628662
19 1.1940243244171143
20 1.3836101293563843
21 1.281020164489746
22 1.1907129287719727
23 1.3532344102859497
24 1.2965342998504639
25 1.1600478887557983
26 1.331203818321228
27 1.2900716066360474
28 1.1592674255371094
29 1.151401162147522
30 1.164631724357605
31 1.2338097095489502
32 1.2041090726852417
33 1.2455142736434937
34 1.193684458732605
35 1.3113309144973755
36 1.148840308189392
37 1.1781882047653198
38 1.1347652673721313
39 1.3036636114120483
40 1.0763177871704102
41 1.2863409519195557
42 1.3952265977859497
43 1.082445502281189
44 1.0800793170928955
45 1.198410987854004
46 1.3294570446014404
47 1.1938199996948242
48 1.4256179332733154
49 1.2840825319290161
50 1.2872778177261353
51 1.2041370868682861
52 1.2619284391403198
53 1.1907529830932617
54 1.0165858268737793
55 1.094204306602478
56 1.3261239528656006
57 1.1823982000350952
58 1.250605583190918
59 1.2839480638504028
60 1.3011571168899536
61 1.1943470239639282
62 1.3013197183609009
63 1.213388442993164
64 1.1858277320861816
65 1.1917011737823486
66 1.1508723497390747
67 1.443266749382019
68 1.1460365056991577
69 1.2545655965805054
70 1.254427194595337
71 1.4454025030136108
72 1.2164983749389648
73 1.1153565645217896
74 1.2391855716705322
75 1.2592294216156006
76 1.1920877695083618
77 1.029157280921936
78 1.2560193538665771
79 1.285893201828003
80 1.3246181011199951
81 1.2517114877700806
82 1.2571494579315186
83 1.2436752319335938
84 0.9826070666313171
85 1.2949349880218506
86 1.3973501920700073
87 1.2010802030563354
88 1.0142991542816162
89 1.2214285135269165
90 1.336766242980957
91 1.4364625215530396
92 1.3422584533691406
93 1.2398314476013184
94 1.2975460290908813
95 1.1021281480789185
96 1.1748238801956177
97 1.3840429782867432
98 1.2178711891174316
99 1.2546947002410889
100 1.1257895231246948
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.125790
101 1.317781686782837
102 1.092310905456543
103 1.1033539772033691
104 1.2601052522659302
105 1.2318780422210693
106 1.173456072807312
107 1.234177827835083
108 1.1205121278762817
109 1.3498953580856323
110 1.2568533420562744
111 1.1511825323104858
112 1.3190869092941284
113 1.2338216304779053
114 1.1092314720153809
115 1.2908072471618652
116 1.206985592842102
117 1.2293204069137573
118 1.1813938617706299
119 1.0989288091659546
120 1.291869044303894
121 1.2989519834518433
122 1.2292674779891968
123 1.3581985235214233
124 1.1317799091339111
125 1.205224871635437
126 1.227607011795044
127 1.3075860738754272
128 1.0038111209869385
129 1.2951823472976685
130 1.118077039718628
131 1.175816535949707
132 1.145392656326294
133 1.106259822845459
134 1.1075044870376587
135 1.0425817966461182
136 1.1624045372009277
137 0.9850476980209351
138 1.303357720375061
139 1.0669739246368408
140 1.119009017944336
141 1.2225254774093628
142 1.2591325044631958
143 1.2179756164550781
144 1.1975815296173096
145 1.1777313947677612
146 1.2389143705368042
147 1.1482030153274536
148 1.3097072839736938
149 1.0140069723129272
150 1.1226210594177246
151 1.1911587715148926
152 1.2174876928329468
153 1.123014211654663
154 1.1859159469604492
155 1.1935847997665405
156 1.3607984781265259
157 1.1000572443008423
158 1.2400271892547607
159 1.1213299036026
160 1.2226769924163818
161 1.0332658290863037
162 0.9665570855140686
163 1.0290871858596802
164 1.1204214096069336
165 1.1126657724380493
166 1.0214393138885498
167 1.1301807165145874
168 1.05339777469635
169 1.0190802812576294
170 1.2048074007034302
171 1.080944538116455
172 1.0756434202194214
173 1.0885075330734253
174 1.0071895122528076
175 1.0270003080368042
176 1.1685518026351929
177 1.2933456897735596
178 1.2709953784942627
179 1.2423409223556519
180 1.1055161952972412
181 1.040326714515686
182 1.1474491357803345
183 1.1812857389450073
184 1.021630883216858
185 1.0665459632873535
186 0.9892714023590088
187 1.1117295026779175
188 1.1188770532608032
189 1.1751500368118286
190 1.1216788291931152
191 1.085055947303772
192 1.1986881494522095
193 1.3168469667434692
194 1.261968970298767
195 1.0304203033447266
196 0.9590729475021362
197 1.1597473621368408
198 1.0495933294296265
199 1.197798490524292
.............
266.28158641792834 421.8423309940845 26.52063099294901
.............
0 1.1717448234558105
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.171745
1 1.3055202960968018
2 1.264563798904419
3 1.1482245922088623
4 1.1379973888397217
5 1.3687303066253662
6 0.9304109811782837
7 1.2072768211364746
8 1.0287981033325195
9 1.2557494640350342
10 0.9514411091804504
11 1.127986192703247
12 1.0879441499710083
13 1.0627281665802002
14 1.151468276977539
15 1.1278327703475952
16 1.1460870504379272
17 1.0025442838668823
18 1.2874146699905396
19 1.1060726642608643
20 1.2474249601364136
21 1.1470623016357422
22 1.0277701616287231
23 1.2273931503295898
24 1.1420979499816895
25 1.054423451423645
26 1.2161750793457031
27 1.1467515230178833
28 1.0827468633651733
29 1.0706535577774048
30 1.0519912242889404
31 1.0881975889205933
32 1.0738909244537354
33 1.1398736238479614
34 1.0880682468414307
35 1.209877848625183
36 1.000491738319397
37 1.101513385772705
38 1.0187196731567383
39 1.1801996231079102
40 0.9769603610038757
41 1.188300371170044
42 1.3507704734802246
43 0.9417123198509216
44 0.9689425826072693
45 1.1135276556015015
46 1.2592345476150513
47 1.132694125175476
48 1.3058558702468872
49 1.2160847187042236
50 1.1711208820343018
51 1.1032158136367798
52 1.2435606718063354
53 1.0551432371139526
54 0.9167275428771973
55 1.0501371622085571
56 1.2137749195098877
57 1.0899497270584106
58 1.1646685600280762
59 1.2066971063613892
60 1.2478502988815308
61 1.151548147201538
62 1.1844364404678345
63 1.067343831062317
64 1.1480408906936646
65 1.1855748891830444
66 1.0097700357437134
67 1.2154474258422852
68 1.101336121559143
69 1.1649444103240967
70 1.087442398071289
71 1.304211139678955
72 1.1186107397079468
73 1.0735031366348267
74 1.2104756832122803
75 1.0951857566833496
76 0.9928417205810547
77 0.955191969871521
78 1.2078431844711304
79 1.134448528289795
80 1.1095081567764282
81 1.1037675142288208
82 1.1684753894805908
83 1.019848108291626
84 0.9745312333106995
85 1.165656566619873
86 1.3153952360153198
87 1.0420390367507935
88 0.9058874249458313
89 1.1064658164978027
90 1.2781891822814941
91 1.376833200454712
92 1.272770643234253
93 1.1517945528030396
94 1.2127257585525513
95 1.02628755569458
96 1.1541657447814941
97 1.3446485996246338
98 1.130683422088623
99 1.1960033178329468
100 1.0230605602264404
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.023061
101 1.2668102979660034
102 1.0980274677276611
103 1.1114250421524048
104 1.173690676689148
105 1.156693458557129
106 1.1302381753921509
107 1.1594934463500977
108 1.0043213367462158
109 1.3103127479553223
110 1.1309877634048462
111 1.0424834489822388
112 1.1821868419647217
113 1.208782434463501
114 0.9556600451469421
115 1.1945956945419312
116 1.1947245597839355
117 1.1656185388565063
118 0.9906517267227173
119 1.019494652748108
120 1.2438969612121582
121 1.2316460609436035
122 1.1095569133758545
123 1.3040889501571655
124 1.0807673931121826
125 1.204574704170227
126 1.2027933597564697
127 1.2424489259719849
128 0.9536964297294617
129 1.2406917810440063
130 1.1208834648132324
131 1.0885748863220215
132 1.0935429334640503
133 1.0348275899887085
134 1.0463958978652954
135 0.9996599555015564
136 1.148863434791565
137 0.9150843620300293
138 1.2674354314804077
139 1.1291379928588867
140 1.073581337928772
141 1.233937382698059
142 1.1969703435897827
143 1.1651089191436768
144 1.2103012800216675
145 1.1509004831314087
146 1.2093210220336914
147 1.051875114440918
148 1.2126976251602173
149 0.9462651610374451
150 1.1330132484436035
151 1.1238728761672974
152 1.1803642511367798
153 1.112197995185852
154 1.1332134008407593
155 1.130429744720459
156 1.3362727165222168
157 1.063244342803955
158 1.1813654899597168
159 1.0793397426605225
160 1.1717326641082764
161 0.9587172865867615
162 0.9125504493713379
163 0.9751362800598145
164 1.0596036911010742
165 1.067858099937439
166 0.9429352283477783
167 1.063598871231079
168 0.9855961799621582
169 0.9658830761909485
170 1.16678786277771
171 1.0217502117156982
172 1.0078823566436768
173 1.024786114692688
174 1.0019850730895996
175 0.9934272170066833
176 1.1616960763931274
177 1.208008885383606
178 1.2215532064437866
179 1.178974986076355
180 1.0661646127700806
181 1.0215051174163818
182 1.0843470096588135
183 1.1286228895187378
184 0.9863623976707458
185 1.0277906656265259
186 0.9411953687667847
187 1.0814319849014282
188 1.074733853340149
189 1.1502820253372192
190 1.0745046138763428
191 1.0328468084335327
192 1.1448581218719482
193 1.2589244842529297
194 1.2295032739639282
195 0.9641265273094177
196 0.9226346611976624
197 1.0797592401504517
198 1.0208253860473633
199 1.1724772453308105
.............
620.6713972762227 985.8490701299161 63.64664134941995
.............
0 1.1295907497406006
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.129591
1 1.2757681608200073
2 1.217634677886963
3 1.133263349533081
4 1.0912033319473267
5 1.3229575157165527
6 0.900357723236084
7 1.1691081523895264
8 0.9859700798988342
9 1.2065951824188232
10 0.9096828699111938
11 1.0736885070800781
12 1.0568819046020508
13 1.023512601852417
14 1.1370563507080078
15 1.100501537322998
16 1.0993009805679321
17 0.9427388906478882
18 1.2701961994171143
19 1.0599967241287231
20 1.2035564184188843
21 1.0853403806686401
22 0.9783283472061157
23 1.1857883930206299
24 1.1032088994979858
25 1.013960599899292
26 1.195641040802002
27 1.1043099164962769
28 1.097781777381897
29 1.0636777877807617
30 1.0203447341918945
31 1.1084803342819214
32 1.023993968963623
33 1.1230802536010742
34 1.0700956583023071
35 1.1840698719024658
36 1.0291297435760498
37 1.101879596710205
38 1.004205346107483
39 1.1541247367858887
40 0.9691044688224792
41 1.1567950248718262
42 1.312404751777649
43 0.9079506397247314
44 0.964931845664978
45 1.0947562456130981
46 1.2130070924758911
47 1.1053584814071655
48 1.2801601886749268
49 1.138778805732727
50 1.1427572965621948
51 1.067959189414978
52 1.1845619678497314
53 1.0228556394577026
54 0.9174240827560425
55 1.0203970670700073
56 1.1406395435333252
57 1.0512137413024902
58 1.1241424083709717
59 1.1574020385742188
60 1.1679177284240723
61 1.1200268268585205
62 1.145717740058899
63 0.9991933703422546
64 1.0706878900527954
65 1.10468327999115
66 0.9715939164161682
67 1.1855698823928833
68 1.03066086769104
69 1.1173824071884155
70 1.0318043231964111
71 1.2820967435836792
72 1.0958718061447144
73 1.0428353548049927
74 1.1711361408233643
75 1.0791387557983398
76 0.9673663377761841
77 0.9412831664085388
78 1.1845132112503052
79 1.1034438610076904
80 1.067072868347168
81 1.0717573165893555
82 1.1408910751342773
83 0.9874284863471985
84 0.9619322419166565
85 1.1345516443252563
86 1.2777959108352661
87 1.000386357307434
88 0.8932013511657715
89 1.082688331604004
90 1.2640084028244019
91 1.3598606586456299
92 1.2655432224273682
93 1.1372370719909668
94 1.1768162250518799
95 0.997883677482605
96 1.139114260673523
97 1.2872681617736816
98 1.0997036695480347
99 1.164596438407898
100 0.9885231256484985
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.988523
101 1.2254209518432617
102 1.0674359798431396
103 1.1044620275497437
104 1.1373510360717773
105 1.1192162036895752
106 1.088657259941101
107 1.1287543773651123
108 0.9941522479057312
109 1.2931029796600342
110 1.1000860929489136
111 1.0256799459457397
112 1.1433730125427246
113 1.1879990100860596
114 0.9500453472137451
115 1.156674861907959
116 1.1456928253173828
117 1.1288256645202637
118 0.9973326325416565
119 0.9854962229728699
120 1.1952505111694336
121 1.2266473770141602
122 1.055556058883667
123 1.2495951652526855
124 1.0339655876159668
125 1.174271583557129
126 1.1808279752731323
127 1.2159479856491089
128 0.9461473226547241
129 1.174839735031128
130 1.0987759828567505
131 1.042170524597168
132 1.0168296098709106
133 0.9858477711677551
134 1.0237175226211548
135 0.9803076982498169
136 1.1330598592758179
137 0.8851692080497742
138 1.2419966459274292
139 1.0983502864837646
140 1.0428557395935059
141 1.2358918190002441
142 1.1681040525436401
143 1.1306536197662354
144 1.194851279258728
145 1.1353694200515747
146 1.1931264400482178
147 1.011744499206543
148 1.187018871307373
149 0.9187783002853394
150 1.1260857582092285
151 1.145803689956665
152 1.1729604005813599
153 1.0832810401916504
154 1.0909456014633179
155 1.0891954898834229
156 1.3177679777145386
157 1.0783991813659668
158 1.172702431678772
159 1.1024584770202637
160 1.1844831705093384
161 0.9453650712966919
162 0.928388774394989
163 1.0140902996063232
164 1.059187650680542
165 1.0440839529037476
166 0.9264526963233948
167 1.041978120803833
168 0.981635570526123
169 0.9589470028877258
170 1.1627978086471558
171 1.0178543329238892
172 0.9904889464378357
173 0.9936405420303345
174 0.9975845813751221
175 1.0006444454193115
176 1.1778192520141602
177 1.1836518049240112
178 1.20179283618927
179 1.1685757637023926
180 1.0548064708709717
181 1.0180904865264893
182 1.056276559829712
183 1.0978126525878906
184 0.9777168035507202
185 1.0196609497070312
186 0.9266291856765747
187 1.0821336507797241
188 1.0644854307174683
189 1.1480591297149658
190 1.0586150884628296
191 1.013127326965332
192 1.1181690692901611
193 1.2127220630645752
194 1.2061721086502075
195 0.9393408298492432
196 0.9146945476531982
197 1.0459716320037842
198 0.9921689033508301
199 1.1526395082473755
.............
1330.898863101378 2112.9146983157843 135.77827492542565
.............
0 1.107901930809021
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.107902
1 1.2707486152648926
2 1.1992542743682861
3 1.1233623027801514
4 1.058686375617981
5 1.2901368141174316
6 0.8849667906761169
7 1.1577624082565308
8 0.971285343170166
9 1.1833657026290894
10 0.9031658172607422
11 1.055546522140503
12 1.0535345077514648
13 1.0172510147094727
14 1.1304265260696411
15 1.0651049613952637
16 1.0786200761795044
17 0.9143211841583252
18 1.271049976348877
19 1.0269544124603271
20 1.1823238134384155
21 1.0598160028457642
22 0.9471260905265808
23 1.1543501615524292
24 1.0956616401672363
25 0.9892523884773254
26 1.176934003829956
27 1.0605461597442627
28 1.0536712408065796
29 1.0375972986221313
30 0.9695925712585449
31 1.034887671470642
32 0.9983999729156494
33 1.0815986394882202
34 1.0279297828674316
35 1.1678147315979004
36 0.9665023684501648
37 1.071292519569397
38 0.983296811580658
39 1.1728160381317139
40 0.9535247683525085
41 1.0843753814697266
42 1.2904081344604492
43 0.8994114398956299
44 0.936687171459198
45 1.0816562175750732
46 1.2266528606414795
47 1.046783685684204
48 1.2788474559783936
49 1.122235894203186
50 1.1792336702346802
51 1.0833908319473267
52 1.1182938814163208
53 1.0198287963867188
54 0.9116271734237671
55 0.9995441436767578
56 1.112341284751892
57 1.0410447120666504
58 1.1257662773132324
59 1.1327956914901733
60 1.1297935247421265
61 1.1207985877990723
62 1.1470460891723633
63 0.9548618197441101
64 1.0342155694961548
65 1.0601928234100342
66 0.945357084274292
67 1.1570700407028198
68 1.0225803852081299
69 1.0819244384765625
70 0.9692384600639343
71 1.24367356300354
72 1.0828943252563477
73 1.0070726871490479
74 1.1438976526260376
75 1.0518920421600342
76 0.965370774269104
77 0.9201195240020752
78 1.1574811935424805
79 1.0750881433486938
80 1.0504299402236938
81 1.0516916513442993
82 1.1203134059906006
83 0.9670913219451904
84 0.9032077789306641
85 1.111443281173706
86 1.2262097597122192
87 0.9670892953872681
88 0.8835879564285278
89 1.047816514968872
90 1.2367689609527588
91 1.3371269702911377
92 1.2505427598953247
93 1.1030367612838745
94 1.155125379562378
95 0.9598249793052673
96 1.0794792175292969
97 1.2260013818740845
98 1.0890779495239258
99 1.1208460330963135
100 0.9485234022140503
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.948523
101 1.1910734176635742
102 1.0320355892181396
103 1.0522502660751343
104 1.115279197692871
105 1.0923051834106445
106 1.0519747734069824
107 1.1037437915802002
108 0.9739025831222534
109 1.2656570672988892
110 1.0835442543029785
111 1.0156995058059692
112 1.094388723373413
113 1.1887496709823608
114 0.9438884854316711
115 1.1349889039993286
116 1.114234209060669
117 1.0898610353469849
118 0.9915837645530701
119 0.9590739607810974
120 1.1305240392684937
121 1.2412099838256836
122 1.0388790369033813
123 1.1737457513809204
124 1.0319373607635498
125 1.1479122638702393
126 1.1578397750854492
127 1.1648211479187012
128 0.941038966178894
129 1.1044178009033203
130 1.057955265045166
131 1.0310996770858765
132 0.9647418856620789
133 0.9475926160812378
134 0.9966157674789429
135 0.953904926776886
136 1.099390983581543
137 0.8713936805725098
138 1.2192409038543701
139 1.0468372106552124
140 1.0036205053329468
141 1.2241299152374268
142 1.1414600610733032
143 1.0921059846878052
144 1.151244044303894
145 1.1057590246200562
146 1.161271333694458
147 0.9819288849830627
148 1.177606463432312
149 0.8939173817634583
150 1.070157527923584
151 1.1441428661346436
152 1.129513144493103
153 1.0507216453552246
154 1.0714797973632812
155 1.0373828411102295
156 1.309275507926941
157 1.093341588973999
158 1.0958950519561768
159 1.061537742614746
160 1.1778968572616577
161 0.9500108957290649
162 0.9024325609207153
163 1.0550708770751953
164 1.083726167678833
165 1.0213249921798706
166 0.9142590165138245
167 1.0274908542633057
168 0.9702072143554688
169 0.9591622948646545
170 1.1504789590835571
171 1.0137158632278442
172 0.9845144152641296
173 0.9528048038482666
174 0.9742507338523865
175 0.9624175429344177
176 1.1521210670471191
177 1.1691983938217163
178 1.1647504568099976
179 1.152235984802246
180 1.0545982122421265
181 0.9881575107574463
182 1.0455482006072998
183 1.0693472623825073
184 0.9674551486968994
185 0.9937551021575928
186 0.8933926224708557
187 1.0526065826416016
188 1.0370173454284668
189 1.1026630401611328
190 1.0262036323547363
191 0.9992266297340393
192 1.0794123411178589
193 1.1774470806121826
194 1.1687285900115967
195 0.908555269241333
196 0.888607919216156
197 1.0080982446670532
198 0.9676346778869629
199 1.1184884309768677
.............
2754.1436029449105 4370.403136022389 280.51977145671844
.............
10 2754.1436029449105 4370.403136022389 280.51977145671844
.............
