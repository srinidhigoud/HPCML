0 2.8241794109344482
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.824179
1 2.8015830516815186
2 2.7559919357299805
3 2.679588556289673
4 2.6577959060668945
5 2.5515427589416504
6 2.402538537979126
7 2.3249869346618652
8 2.1408112049102783
9 2.1031460762023926
10 1.8890148401260376
11 1.829634189605713
12 1.7322392463684082
13 2.0177767276763916
14 1.868910312652588
15 1.8999935388565063
16 1.7406655550003052
17 1.833237648010254
18 1.8015623092651367
19 1.8780218362808228
20 1.9608714580535889
21 1.7859758138656616
22 1.521700382232666
23 1.8591505289077759
24 1.667851209640503
25 1.812178611755371
26 1.8790453672409058
27 1.8032199144363403
28 1.4890379905700684
29 1.6935163736343384
30 1.6871682405471802
31 1.7178518772125244
32 1.6589913368225098
33 1.643060326576233
34 1.6518124341964722
35 1.751601219177246
36 1.5584591627120972
37 1.661753535270691
38 1.5685161352157593
39 1.5890105962753296
40 1.5941723585128784
41 1.6408573389053345
42 1.6621333360671997
43 1.559425950050354
44 1.448164701461792
45 1.4833920001983643
46 1.7373179197311401
47 1.6253527402877808
48 1.6950393915176392
49 1.6224597692489624
50 1.529191255569458
51 1.5340821743011475
52 1.5886057615280151
53 1.5192701816558838
54 1.312239408493042
55 1.5430711507797241
56 1.7622212171554565
57 1.5866299867630005
58 1.4871392250061035
59 1.5818207263946533
60 1.638827919960022
61 1.6257545948028564
62 1.5630042552947998
63 1.5288995504379272
64 1.5898154973983765
65 1.559008002281189
66 1.4455486536026
67 1.5321152210235596
68 1.5097870826721191
69 1.5436166524887085
70 1.4154435396194458
71 1.616387128829956
72 1.433571219444275
73 1.4259318113327026
74 1.3853204250335693
75 1.4095721244812012
76 1.400091290473938
77 1.3266104459762573
78 1.4893617630004883
79 1.5263034105300903
80 1.3780876398086548
81 1.4506862163543701
82 1.5317025184631348
83 1.4790836572647095
84 1.2624281644821167
85 1.5144531726837158
86 1.630197286605835
87 1.4615532159805298
88 1.3308939933776855
89 1.4380488395690918
90 1.5460896492004395
91 1.6256272792816162
92 1.593477487564087
93 1.4695236682891846
94 1.4616975784301758
95 1.3353767395019531
96 1.4044970273971558
97 1.6417371034622192
98 1.5929157733917236
99 1.4166804552078247
100 1.3698619604110718
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.369862
101 1.5211219787597656
102 1.3905534744262695
103 1.3207865953445435
104 1.491491436958313
105 1.571174144744873
106 1.4145954847335815
107 1.4816478490829468
108 1.4029165506362915
109 1.5690916776657104
110 1.4715858697891235
111 1.3677031993865967
112 1.5700703859329224
113 1.4951987266540527
114 1.2146300077438354
115 1.4081588983535767
116 1.3611558675765991
117 1.42292058467865
118 1.2751730680465698
119 1.2737098932266235
120 1.4837234020233154
121 1.4281560182571411
122 1.3551326990127563
123 1.4952067136764526
124 1.261696457862854
125 1.3474050760269165
126 1.3348621129989624
127 1.4352985620498657
128 1.1656162738800049
129 1.4172263145446777
130 1.2750245332717896
131 1.3268535137176514
132 1.2953838109970093
133 1.3042171001434326
134 1.3256289958953857
135 1.2250818014144897
136 1.2488586902618408
137 1.1506190299987793
138 1.4205529689788818
139 1.1359889507293701
140 1.3248852491378784
141 1.374101161956787
142 1.4226367473602295
143 1.3319447040557861
144 1.3026994466781616
145 1.3196254968643188
146 1.331802248954773
147 1.3086944818496704
148 1.4383834600448608
149 1.2024381160736084
150 1.2007685899734497
151 1.3365423679351807
152 1.3263435363769531
153 1.2547305822372437
154 1.3543325662612915
155 1.3891253471374512
156 1.4145148992538452
157 1.195752501487732
158 1.4070184230804443
159 1.203200101852417
160 1.3521316051483154
161 1.15184485912323
162 1.1276004314422607
163 1.141142725944519
164 1.2383469343185425
165 1.186087965965271
166 1.1223357915878296
167 1.254662275314331
168 1.2115308046340942
169 1.1842973232269287
170 1.3349087238311768
171 1.208289623260498
172 1.2251992225646973
173 1.1834968328475952
174 1.1569771766662598
175 1.1414575576782227
176 1.3004100322723389
177 1.4287965297698975
178 1.3591492176055908
179 1.3467334508895874
180 1.2215954065322876
181 1.2016501426696777
182 1.2401022911071777
183 1.322115182876587
184 1.222992181777954
185 1.1879388093948364
186 1.1165084838867188
187 1.2163127660751343
188 1.2540558576583862
189 1.3339427709579468
190 1.2760521173477173
191 1.2742493152618408
192 1.3944309949874878
193 1.3697693347930908
194 1.3115098476409912
195 1.3187603950500488
196 1.0839613676071167
197 1.2899928092956543
198 1.2008424997329712
199 1.3482704162597656
.............
246.98551416024566 44.76965318247676 12.1568160969764
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3330188989639282
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.333019
1 1.3960659503936768
2 1.3370866775512695
3 1.2685878276824951
4 1.2325820922851562
5 1.4880393743515015
6 1.1019412279129028
7 1.336319088935852
8 1.1787877082824707
9 1.3569515943527222
10 1.0734248161315918
11 1.2623893022537231
12 1.1727181673049927
13 1.2024435997009277
14 1.2090976238250732
15 1.260237693786621
16 1.2628334760665894
17 1.2357791662216187
18 1.3415721654891968
19 1.19394052028656
20 1.3804060220718384
21 1.2728488445281982
22 1.1878952980041504
23 1.351595163345337
24 1.292353868484497
25 1.1618297100067139
26 1.3340857028961182
27 1.2931923866271973
28 1.1561543941497803
29 1.1454983949661255
30 1.1625925302505493
31 1.2343238592147827
32 1.1996036767959595
33 1.242762565612793
34 1.1945488452911377
35 1.31081223487854
36 1.149547815322876
37 1.1741275787353516
38 1.132686972618103
39 1.3005181550979614
40 1.078596830368042
41 1.2782326936721802
42 1.3839014768600464
43 1.0842487812042236
44 1.0807852745056152
45 1.1976300477981567
46 1.3265610933303833
47 1.193636178970337
48 1.4257241487503052
49 1.2843067646026611
50 1.2984728813171387
51 1.2056853771209717
52 1.2616606950759888
53 1.1902217864990234
54 1.0100802183151245
55 1.089816927909851
56 1.3112993240356445
57 1.185499668121338
58 1.245906949043274
59 1.275302529335022
60 1.2933595180511475
61 1.1879475116729736
62 1.2936073541641235
63 1.1842482089996338
64 1.1759579181671143
65 1.175050139427185
66 1.1456382274627686
67 1.4035301208496094
68 1.1103805303573608
69 1.2527399063110352
70 1.233465552330017
71 1.4277325868606567
72 1.1981253623962402
73 1.1082983016967773
74 1.2332017421722412
75 1.2480318546295166
76 1.1710841655731201
77 1.01791250705719
78 1.2575886249542236
79 1.2856667041778564
80 1.2945724725723267
81 1.2231755256652832
82 1.2636959552764893
83 1.252349615097046
84 0.9752561450004578
85 1.265071153640747
86 1.3867267370224
87 1.1909006834030151
88 1.0189127922058105
89 1.1930334568023682
90 1.3302286863327026
91 1.4288108348846436
92 1.341627836227417
93 1.2302336692810059
94 1.2829082012176514
95 1.1002556085586548
96 1.1798173189163208
97 1.3837095499038696
98 1.2107387781143188
99 1.242653489112854
100 1.1109334230422974
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.110933
101 1.314584493637085
102 1.0878039598464966
103 1.100886344909668
104 1.2515827417373657
105 1.2241005897521973
106 1.1690210103988647
107 1.2287143468856812
108 1.111124038696289
109 1.3504616022109985
110 1.2507600784301758
111 1.1428351402282715
112 1.3094249963760376
113 1.2281720638275146
114 1.1091974973678589
115 1.2736032009124756
116 1.2044943571090698
117 1.229539394378662
118 1.1718857288360596
119 1.0795265436172485
120 1.2883661985397339
121 1.3078340291976929
122 1.2229312658309937
123 1.3557043075561523
124 1.1227095127105713
125 1.2054733037948608
126 1.231276273727417
127 1.2962340116500854
128 1.0031039714813232
129 1.2838016748428345
130 1.1131953001022339
131 1.176702857017517
132 1.1230480670928955
133 1.080737590789795
134 1.1185659170150757
135 1.034214735031128
136 1.156301736831665
137 0.9811253547668457
138 1.2916059494018555
139 1.0718744993209839
140 1.1170408725738525
141 1.2229171991348267
142 1.263778805732727
143 1.197803020477295
144 1.1898027658462524
145 1.16763174533844
146 1.2255886793136597
147 1.1305460929870605
148 1.2923359870910645
149 1.0039311647415161
150 1.1127680540084839
151 1.1743923425674438
152 1.2076832056045532
153 1.1167203187942505
154 1.1792452335357666
155 1.181575059890747
156 1.3470001220703125
157 1.0976269245147705
158 1.234523057937622
159 1.111747145652771
160 1.2106658220291138
161 1.0225545167922974
162 0.9612665772438049
163 1.0303418636322021
164 1.111857533454895
165 1.0971062183380127
166 1.0095239877700806
167 1.1213878393173218
168 1.0439226627349854
169 1.0099740028381348
170 1.1956900358200073
171 1.0747809410095215
172 1.0745302438735962
173 1.0809979438781738
174 1.013291835784912
175 1.013049602508545
176 1.1648656129837036
177 1.2849600315093994
178 1.25824773311615
179 1.2323975563049316
180 1.0971999168395996
181 1.0378614664077759
182 1.1391119956970215
183 1.1761184930801392
184 1.0155713558197021
185 1.0566420555114746
186 0.9841575622558594
187 1.104955792427063
188 1.1153709888458252
189 1.1626452207565308
190 1.1170954704284668
191 1.0843031406402588
192 1.193009376525879
193 1.2947520017623901
194 1.2634220123291016
195 1.0258855819702148
196 0.9525655508041382
197 1.1515915393829346
198 1.0490736961364746
199 1.1908142566680908
.............
739.3800197336823 134.03115905448794 36.44316875189543
.............
0 1.1617357730865479
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.161736
1 1.300594449043274
2 1.2580504417419434
3 1.1468485593795776
4 1.1484622955322266
5 1.3650380373001099
6 0.9253118634223938
7 1.2009921073913574
8 1.0181734561920166
9 1.2500133514404297
10 0.9450681209564209
11 1.1193270683288574
12 1.0831729173660278
13 1.0420184135437012
14 1.1412606239318848
15 1.1335808038711548
16 1.1341625452041626
17 0.9982118010520935
18 1.2778313159942627
19 1.1059863567352295
20 1.252760410308838
21 1.1318919658660889
22 1.0193082094192505
23 1.2260637283325195
24 1.1313117742538452
25 1.0521881580352783
26 1.2172762155532837
27 1.142805576324463
28 1.0954784154891968
29 1.0671459436416626
30 1.0603104829788208
31 1.1063381433486938
32 1.0703248977661133
33 1.1439307928085327
34 1.0908695459365845
35 1.2083568572998047
36 1.0145701169967651
37 1.0976091623306274
38 1.0206550359725952
39 1.1632908582687378
40 0.9806557297706604
41 1.1696852445602417
42 1.3364460468292236
43 0.9324065446853638
44 0.9563295245170593
45 1.1174548864364624
46 1.2320177555084229
47 1.1345863342285156
48 1.3057500123977661
49 1.1878896951675415
50 1.1556679010391235
51 1.1001324653625488
52 1.2358309030532837
53 1.032378077507019
54 0.9181978702545166
55 1.0492584705352783
56 1.1992628574371338
57 1.0787227153778076
58 1.1479145288467407
59 1.2119884490966797
60 1.2304556369781494
61 1.1347101926803589
62 1.195712685585022
63 1.0594993829727173
64 1.1243441104888916
65 1.1577295064926147
66 0.9929454326629639
67 1.2274709939956665
68 1.0687938928604126
69 1.1705820560455322
70 1.0825484991073608
71 1.3166401386260986
72 1.1153111457824707
73 1.0745667219161987
74 1.1941910982131958
75 1.1134889125823975
76 0.9946228265762329
77 0.9537423849105835
78 1.2088208198547363
79 1.1364997625350952
80 1.109106421470642
81 1.1021722555160522
82 1.1609848737716675
83 1.0139466524124146
84 0.9743655920028687
85 1.1657260656356812
86 1.3231778144836426
87 1.0356723070144653
88 0.9039854407310486
89 1.1079825162887573
90 1.2647768259048462
91 1.3716319799423218
92 1.2721447944641113
93 1.1406172513961792
94 1.178446888923645
95 1.0293103456497192
96 1.1499037742614746
97 1.3541303873062134
98 1.1244451999664307
99 1.1918978691101074
100 1.007283091545105
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.007283
101 1.255383014678955
102 1.0947656631469727
103 1.1059989929199219
104 1.1648122072219849
105 1.1457020044326782
106 1.144649863243103
107 1.1521199941635132
108 1.0024853944778442
109 1.301937460899353
110 1.1284691095352173
111 1.050380825996399
112 1.1799390316009521
113 1.2190868854522705
114 0.9653985500335693
115 1.1875361204147339
116 1.2070049047470093
117 1.1710363626480103
118 0.9709641337394714
119 1.0251249074935913
120 1.235275387763977
121 1.2378759384155273
122 1.1194772720336914
123 1.2880682945251465
124 1.1102149486541748
125 1.2220004796981812
126 1.2162531614303589
127 1.2549976110458374
128 0.9411440491676331
129 1.2268893718719482
130 1.105302333831787
131 1.1028798818588257
132 1.1101000308990479
133 1.0231910943984985
134 1.076898455619812
135 0.9897790551185608
136 1.1332188844680786
137 0.9084362983703613
138 1.2644376754760742
139 1.1455119848251343
140 1.0765416622161865
141 1.2403647899627686
142 1.2071608304977417
143 1.157450556755066
144 1.209826111793518
145 1.1452959775924683
146 1.198492169380188
147 1.048051118850708
148 1.216894268989563
149 0.9473250508308411
150 1.1238281726837158
151 1.1506242752075195
152 1.1738508939743042
153 1.0958430767059326
154 1.1404659748077393
155 1.1274738311767578
156 1.3196384906768799
157 1.0720282793045044
158 1.1745545864105225
159 1.0884168148040771
160 1.1654975414276123
161 0.9611375331878662
162 0.9228693246841431
163 0.9872682690620422
164 1.058547019958496
165 1.0627069473266602
166 0.9370642900466919
167 1.057446837425232
168 0.987224817276001
169 0.9664584398269653
170 1.1771657466888428
171 1.0338383913040161
172 0.9976853728294373
173 1.0206152200698853
174 0.9890446662902832
175 1.001512885093689
176 1.1524827480316162
177 1.2112832069396973
178 1.2405589818954468
179 1.1834287643432617
180 1.0607786178588867
181 1.0315552949905396
182 1.0859599113464355
183 1.1243726015090942
184 0.9839928150177002
185 1.0218483209609985
186 0.9416481256484985
187 1.0770354270935059
188 1.0655643939971924
189 1.132502555847168
190 1.0754985809326172
191 1.0362247228622437
192 1.1428029537200928
193 1.244033694267273
194 1.2239537239074707
195 0.9633679389953613
196 0.9194475412368774
197 1.0803385972976685
198 1.015884280204773
199 1.163092017173767
.............
1713.9233026411384 312.9794741421938 83.96013412624598
.............
0 1.1121270656585693
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.112127
1 1.2770973443984985
2 1.2111889123916626
3 1.136012077331543
4 1.0879727602005005
5 1.3280225992202759
6 0.8946516513824463
7 1.1665476560592651
8 0.9799180626869202
9 1.2076791524887085
10 0.9124114513397217
11 1.0673240423202515
12 1.0464524030685425
13 1.0058057308197021
14 1.1367624998092651
15 1.0974459648132324
16 1.0845329761505127
17 0.9402839541435242
18 1.2727962732315063
19 1.0575369596481323
20 1.2071832418441772
21 1.0860176086425781
22 0.9704567193984985
23 1.1838998794555664
24 1.1031001806259155
25 1.0131585597991943
26 1.1951464414596558
27 1.0912585258483887
28 1.0935271978378296
29 1.0587939023971558
30 1.02861487865448
31 1.1035147905349731
32 1.0320158004760742
33 1.1120494604110718
34 1.0694648027420044
35 1.1764543056488037
36 1.0330787897109985
37 1.097042202949524
38 1.009467363357544
39 1.149991750717163
40 0.9622319936752319
41 1.145294427871704
42 1.3046236038208008
43 0.9116467237472534
44 0.9547529816627502
45 1.0946967601776123
46 1.2186046838760376
47 1.097089409828186
48 1.283598780632019
49 1.1449062824249268
50 1.1505858898162842
51 1.0754194259643555
52 1.1866868734359741
53 1.0234084129333496
54 0.9102556705474854
55 1.023486852645874
56 1.1369801759719849
57 1.053676724433899
58 1.117881417274475
59 1.1602264642715454
60 1.1759542226791382
61 1.1212151050567627
62 1.1510138511657715
63 1.0015764236450195
64 1.0750627517700195
65 1.0970865488052368
66 0.9675913453102112
67 1.1978813409805298
68 1.0233467817306519
69 1.1226645708084106
70 1.0351732969284058
71 1.2866015434265137
72 1.1007484197616577
73 1.0532151460647583
74 1.1711549758911133
75 1.079300045967102
76 0.9754107594490051
77 0.9428405165672302
78 1.1949273347854614
79 1.111707329750061
80 1.071697473526001
81 1.0794180631637573
82 1.1481744050979614
83 0.9846160411834717
84 0.9714871048927307
85 1.1410330533981323
86 1.2708804607391357
87 1.0013855695724487
88 0.9069886207580566
89 1.078891634941101
90 1.262789011001587
91 1.368647575378418
92 1.2661315202713013
93 1.1411571502685547
94 1.1902984380722046
95 0.9956057667732239
96 1.1362122297286987
97 1.276903510093689
98 1.10100519657135
99 1.1657546758651733
100 0.9924153685569763
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.992415
101 1.2209917306900024
102 1.072034239768982
103 1.0908337831497192
104 1.143273949623108
105 1.1302845478057861
106 1.100846767425537
107 1.1284481287002563
108 0.9915829300880432
109 1.280760645866394
110 1.0997798442840576
111 1.0312057733535767
112 1.1460957527160645
113 1.1907703876495361
114 0.9507095217704773
115 1.1531226634979248
116 1.1654046773910522
117 1.1372928619384766
118 0.9840205311775208
119 0.9856378436088562
120 1.200060248374939
121 1.237213373184204
122 1.0669273138046265
123 1.2698678970336914
124 1.0423822402954102
125 1.1772074699401855
126 1.2002954483032227
127 1.2193934917449951
128 0.9492002129554749
129 1.1711746454238892
130 1.1026118993759155
131 1.0486674308776855
132 1.0222512483596802
133 0.990114152431488
134 1.0222820043563843
135 0.9772840142250061
136 1.1275510787963867
137 0.8856443166732788
138 1.2455613613128662
139 1.0989192724227905
140 1.0452955961227417
141 1.2323826551437378
142 1.173119306564331
143 1.1153429746627808
144 1.1910091638565063
145 1.1289194822311401
146 1.1969879865646362
147 1.0024046897888184
148 1.1839754581451416
149 0.9165794253349304
150 1.1073031425476074
151 1.1428731679916382
152 1.1614383459091187
153 1.0699630975723267
154 1.0848420858383179
155 1.095564842224121
156 1.3070385456085205
157 1.076930284500122
158 1.1480777263641357
159 1.099797248840332
160 1.1870932579040527
161 0.950002133846283
162 0.9205703139305115
163 1.0047343969345093
164 1.0445303916931152
165 1.0467909574508667
166 0.9325183033943176
167 1.0399975776672363
168 0.9793487787246704
169 0.960774302482605
170 1.1627404689788818
171 1.0077435970306396
172 0.9837466478347778
173 0.9981268048286438
174 1.005653977394104
175 0.9775558710098267
176 1.163140058517456
177 1.1856486797332764
178 1.1914138793945312
179 1.171452283859253
180 1.0440677404403687
181 1.0049209594726562
182 1.0580048561096191
183 1.0973196029663086
184 0.9659109711647034
185 1.0176090002059937
186 0.9211061000823975
187 1.073575496673584
188 1.0531595945358276
189 1.1214320659637451
190 1.0560857057571411
191 1.0145522356033325
192 1.110367774963379
193 1.2046078443527222
194 1.188942551612854
195 0.9351007342338562
196 0.9085237383842468
197 1.051550269126892
198 0.9901899099349976
199 1.1377898454666138
.............
3670.469166442752 671.017728112638 179.23886422626674
.............
0 1.0912326574325562
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.091233
1 1.272607684135437
2 1.1938437223434448
3 1.1267224550247192
4 1.0478054285049438
5 1.3041589260101318
6 0.8804835677146912
7 1.147116780281067
8 0.966478705406189
9 1.1910465955734253
10 0.910431981086731
11 1.0420641899108887
12 1.0391051769256592
13 0.998324990272522
14 1.1306138038635254
15 1.069427490234375
16 1.0605368614196777
17 0.9127315282821655
18 1.2653779983520508
19 1.0251796245574951
20 1.1925314664840698
21 1.0683265924453735
22 0.9408043622970581
23 1.1543934345245361
24 1.0918240547180176
25 0.9857118725776672
26 1.1736149787902832
27 1.055242896080017
28 1.0570858716964722
29 1.0400516986846924
30 0.9775508642196655
31 1.0371177196502686
32 1.0105193853378296
33 1.0731617212295532
34 1.033928632736206
35 1.1670126914978027
36 0.9859223961830139
37 1.0713316202163696
38 0.9990902543067932
39 1.175075888633728
40 0.942959189414978
41 1.096928596496582
42 1.2772053480148315
43 0.9012031555175781
44 0.9295912981033325
45 1.0811084508895874
46 1.2350401878356934
47 1.0549542903900146
48 1.2847065925598145
49 1.1201976537704468
50 1.1819320917129517
51 1.088919997215271
52 1.1267673969268799
53 1.025259256362915
54 0.9141309261322021
55 1.0037670135498047
56 1.1112635135650635
57 1.046374797821045
58 1.125451683998108
59 1.1460840702056885
60 1.140937328338623
61 1.117182970046997
62 1.1544854640960693
63 0.9548502564430237
64 1.0380539894104004
65 1.0611718893051147
66 0.9469925165176392
67 1.1726601123809814
68 1.0113563537597656
69 1.0870429277420044
70 0.9693962931632996
71 1.2563892602920532
72 1.0900120735168457
73 1.0194311141967773
74 1.1519296169281006
75 1.0501888990402222
76 0.9679128527641296
77 0.929785966873169
78 1.1703778505325317
79 1.0887349843978882
80 1.0582990646362305
81 1.0615350008010864
82 1.1293319463729858
83 0.973085880279541
84 0.9031725525856018
85 1.1200566291809082
86 1.217014193534851
87 0.966420590877533
88 0.8850902318954468
89 1.0512659549713135
90 1.2293486595153809
91 1.3442579507827759
92 1.2650961875915527
93 1.1079550981521606
94 1.1559733152389526
95 0.9594710469245911
96 1.0944231748580933
97 1.2156351804733276
98 1.0741910934448242
99 1.124089241027832
100 0.9452273845672607
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.945227
101 1.1882736682891846
102 1.0316557884216309
103 1.0550355911254883
104 1.1200685501098633
105 1.097926378250122
106 1.0751957893371582
107 1.1049479246139526
108 0.9809439182281494
109 1.2515838146209717
110 1.0746746063232422
111 1.0220602750778198
112 1.0972553491592407
113 1.1842718124389648
114 0.9465599060058594
115 1.1308475732803345
116 1.1335872411727905
117 1.1052260398864746
118 0.9807376265525818
119 0.9572690725326538
120 1.1493048667907715
121 1.2570947408676147
122 1.0464608669281006
123 1.2054834365844727
124 1.028979778289795
125 1.1686047315597534
126 1.1869927644729614
127 1.1766430139541626
128 0.9474449157714844
129 1.1130386590957642
130 1.0793483257293701
131 1.0307798385620117
132 0.9702029228210449
133 0.9523237347602844
134 1.0057334899902344
135 0.9583844542503357
136 1.1050161123275757
137 0.8650344610214233
138 1.2156109809875488
139 1.054773211479187
140 1.0149210691452026
141 1.224237084388733
142 1.1422604322433472
143 1.0838065147399902
144 1.1554020643234253
145 1.1057296991348267
146 1.182405710220337
147 0.9748137593269348
148 1.1704829931259155
149 0.8855812549591064
150 1.0836552381515503
151 1.137296199798584
152 1.1287022829055786
153 1.046600341796875
154 1.0678027868270874
155 1.0545696020126343
156 1.2993240356445312
157 1.0962111949920654
158 1.1179683208465576
159 1.068172574043274
160 1.1888960599899292
161 0.9469487071037292
162 0.9002230167388916
163 1.0436701774597168
164 1.0703071355819702
165 1.0256962776184082
166 0.9184322953224182
167 1.0258539915084839
168 0.9747281074523926
169 0.9558649659156799
170 1.1497842073440552
171 1.011298656463623
172 0.9874569773674011
173 0.9673109650611877
174 0.9811432361602783
175 0.959017813205719
176 1.1458710432052612
177 1.1671018600463867
178 1.1776057481765747
179 1.153647541999817
180 1.040105938911438
181 0.9794263243675232
182 1.0417838096618652
183 1.0762267112731934
184 0.9587974548339844
185 0.993859052658081
186 0.9056999087333679
187 1.0539084672927856
188 1.0375193357467651
189 1.1148382425308228
190 1.0407310724258423
191 0.9966918230056763
192 1.08036208152771
193 1.1743865013122559
194 1.1654062271118164
195 0.9143674373626709
196 0.8920982480049133
197 1.0150574445724487
198 0.9645687937736511
199 1.1156222820281982
.............
7604.795764356852 1387.4569658488035 369.74226956442
.............
11 7604.795764356852 1387.4569658488035 369.74226956442
.............
