./lab1.pytorch:83: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 2.828749895095825
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.828750
1 2.8046376705169678
2 2.7608182430267334
3 2.7075040340423584
4 2.670125722885132
5 2.6030728816986084
6 2.481374979019165
7 2.427041530609131
8 2.2839674949645996
9 2.184567928314209
10 1.994214653968811
11 1.8802802562713623
12 1.7555906772613525
13 1.9747520685195923
14 1.8780633211135864
15 1.9471147060394287
16 1.8218008279800415
17 1.9341081380844116
18 1.7246824502944946
19 1.832145094871521
20 1.9897451400756836
21 1.834182858467102
22 1.5338897705078125
23 1.7959753274917603
24 1.6621185541152954
25 1.7796194553375244
26 1.8706436157226562
27 1.794081449508667
28 1.4915252923965454
29 1.6854456663131714
30 1.6670445203781128
31 1.7041938304901123
32 1.6369677782058716
33 1.6150686740875244
34 1.645655632019043
35 1.7471466064453125
36 1.5469529628753662
37 1.6468815803527832
38 1.5555225610733032
39 1.5724586248397827
40 1.579281210899353
41 1.6237688064575195
42 1.654875636100769
43 1.542332410812378
44 1.4332396984100342
45 1.4765079021453857
46 1.7234411239624023
47 1.615747332572937
48 1.6857028007507324
49 1.6116241216659546
50 1.5272303819656372
51 1.532114863395691
52 1.582377314567566
53 1.50606107711792
54 1.3066939115524292
55 1.5501056909561157
56 1.7595491409301758
57 1.580549955368042
58 1.4758960008621216
59 1.5941593647003174
60 1.653293251991272
61 1.624058723449707
62 1.5602515935897827
63 1.5296510457992554
64 1.597938895225525
65 1.5653630495071411
66 1.4523932933807373
67 1.5313787460327148
68 1.5058653354644775
69 1.547339677810669
70 1.4166481494903564
71 1.618444561958313
72 1.4346497058868408
73 1.4216430187225342
74 1.3866993188858032
75 1.405875563621521
76 1.4021644592285156
77 1.3260074853897095
78 1.4882835149765015
79 1.5202401876449585
80 1.3731184005737305
81 1.4460715055465698
82 1.5329947471618652
83 1.478371262550354
84 1.255853295326233
85 1.5078074932098389
86 1.6291062831878662
87 1.4623700380325317
88 1.3272818326950073
89 1.432409644126892
90 1.5428903102874756
91 1.6257156133651733
92 1.5962004661560059
93 1.4680691957473755
94 1.4598760604858398
95 1.332218050956726
96 1.409192681312561
97 1.6374123096466064
98 1.5992389917373657
99 1.4138449430465698
100 1.3657780885696411
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.365778
101 1.5174390077590942
102 1.3849657773971558
103 1.319688320159912
104 1.4896759986877441
105 1.5604958534240723
106 1.4120439291000366
107 1.472873330116272
108 1.3973537683486938
109 1.5689997673034668
110 1.4698173999786377
111 1.3672378063201904
112 1.5607141256332397
113 1.4875483512878418
114 1.2163596153259277
115 1.4086664915084839
116 1.356955885887146
117 1.4263556003570557
118 1.271647334098816
119 1.27242112159729
120 1.4851574897766113
121 1.4274569749832153
122 1.353234052658081
123 1.499437689781189
124 1.2610687017440796
125 1.3456612825393677
126 1.3332771062850952
127 1.439335584640503
128 1.170465350151062
129 1.4151034355163574
130 1.2711989879608154
131 1.3260847330093384
132 1.290193796157837
133 1.2983100414276123
134 1.3293758630752563
135 1.2261834144592285
136 1.2492793798446655
137 1.149346947669983
138 1.4145277738571167
139 1.1344711780548096
140 1.3225017786026
141 1.3735769987106323
142 1.4309011697769165
143 1.3166093826293945
144 1.295578122138977
145 1.3207411766052246
146 1.3290302753448486
147 1.2979429960250854
148 1.4401134252548218
149 1.2006217241287231
150 1.1970535516738892
151 1.3255608081817627
152 1.3228797912597656
153 1.253618836402893
154 1.3523361682891846
155 1.3889061212539673
156 1.4171557426452637
157 1.1923640966415405
158 1.4000128507614136
159 1.202948808670044
160 1.3514610528945923
161 1.1512994766235352
162 1.1250962018966675
163 1.1361268758773804
164 1.2358160018920898
165 1.187832236289978
166 1.1208595037460327
167 1.256981611251831
168 1.2102329730987549
169 1.1859906911849976
170 1.3316476345062256
171 1.202560305595398
172 1.2290226221084595
173 1.1865936517715454
174 1.1536825895309448
175 1.1322792768478394
176 1.30122709274292
177 1.4272034168243408
178 1.3495738506317139
179 1.332725167274475
180 1.2314664125442505
181 1.1967159509658813
182 1.2257105112075806
183 1.336746335029602
184 1.2427898645401
185 1.1867483854293823
186 1.121071219444275
187 1.2284297943115234
188 1.257931113243103
189 1.3174126148223877
190 1.2822043895721436
191 1.2818881273269653
192 1.3776006698608398
193 1.3594361543655396
194 1.2945040464401245
195 1.2972148656845093
196 1.082492470741272
197 1.2811781167984009
198 1.1816259622573853
199 1.3458313941955566
1 14.461336655542254 57.28685002028942 1.7471611499786377e-05
.............
