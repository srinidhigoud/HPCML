0 2.813394546508789
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.813395
1 2.7941067218780518
2 2.7473363876342773
3 2.695422887802124
4 2.6358883380889893
5 2.5717825889587402
6 2.422205924987793
7 2.37430739402771
8 2.2181291580200195
9 2.126678466796875
10 1.9408154487609863
11 1.8574802875518799
12 1.7525004148483276
13 1.9782966375350952
14 1.8776121139526367
15 1.9250508546829224
16 1.7586027383804321
17 1.8499172925949097
18 1.7655510902404785
19 1.8471852540969849
20 1.9987863302230835
21 1.7842234373092651
22 1.507624626159668
23 1.8203823566436768
24 1.6652330160140991
25 1.7896405458450317
26 1.8692421913146973
27 1.7779933214187622
28 1.492733120918274
29 1.6671026945114136
30 1.6463308334350586
31 1.6707321405410767
32 1.6457419395446777
33 1.6167489290237427
34 1.6411229372024536
35 1.7412998676300049
36 1.5459588766098022
37 1.6361886262893677
38 1.5293214321136475
39 1.5662673711776733
40 1.5796120166778564
41 1.6165449619293213
42 1.6516364812850952
43 1.5326954126358032
44 1.4259604215621948
45 1.4766536951065063
46 1.7177876234054565
47 1.6063826084136963
48 1.676722526550293
49 1.602048635482788
50 1.5213890075683594
51 1.5304880142211914
52 1.574535846710205
53 1.4866855144500732
54 1.2911763191223145
55 1.564439296722412
56 1.7672364711761475
57 1.5655735731124878
58 1.4636443853378296
59 1.6217057704925537
60 1.6728953123092651
61 1.6057478189468384
62 1.5588135719299316
63 1.5524083375930786
64 1.6273558139801025
65 1.5898181200027466
66 1.4521186351776123
67 1.5354198217391968
68 1.509921669960022
69 1.5736093521118164
70 1.4239922761917114
71 1.6195026636123657
72 1.427493929862976
73 1.4207866191864014
74 1.4028509855270386
75 1.4079582691192627
76 1.40024995803833
77 1.32218337059021
78 1.4798972606658936
79 1.5119602680206299
80 1.3688340187072754
81 1.4360324144363403
82 1.5235780477523804
83 1.4649815559387207
84 1.2483608722686768
85 1.494978666305542
86 1.6234022378921509
87 1.4536470174789429
88 1.3132911920547485
89 1.4147007465362549
90 1.5362013578414917
91 1.6167402267456055
92 1.5895342826843262
93 1.455105185508728
94 1.4533028602600098
95 1.3224531412124634
96 1.3989962339401245
97 1.6239237785339355
98 1.5824155807495117
99 1.4011354446411133
100 1.3497309684753418
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.349731
101 1.505887746810913
102 1.3682812452316284
103 1.3081750869750977
104 1.4766539335250854
105 1.5447192192077637
106 1.3979393243789673
107 1.458624005317688
108 1.3768553733825684
109 1.5490132570266724
110 1.4528775215148926
111 1.3508158922195435
112 1.557524561882019
113 1.4764691591262817
114 1.198249340057373
115 1.3920246362686157
116 1.345415472984314
117 1.4049290418624878
118 1.2616766691207886
119 1.2589232921600342
120 1.4717200994491577
121 1.4118846654891968
122 1.340643286705017
123 1.4827308654785156
124 1.2470415830612183
125 1.3344308137893677
126 1.3231492042541504
127 1.4164881706237793
128 1.1473890542984009
129 1.407837986946106
130 1.2607194185256958
131 1.308849811553955
132 1.2851868867874146
133 1.290767788887024
134 1.3088918924331665
135 1.207208514213562
136 1.2399802207946777
137 1.1341023445129395
138 1.4080872535705566
139 1.1292017698287964
140 1.3032188415527344
141 1.3612666130065918
142 1.4137691259384155
143 1.3173295259475708
144 1.2863175868988037
145 1.3096204996109009
146 1.3239532709121704
147 1.2895666360855103
148 1.4314459562301636
149 1.186334490776062
150 1.1936537027359009
151 1.321302056312561
152 1.312361240386963
153 1.2423288822174072
154 1.3328378200531006
155 1.3679172992706299
156 1.4127329587936401
157 1.1857389211654663
158 1.3991845846176147
159 1.1971594095230103
160 1.333765983581543
161 1.1383365392684937
162 1.1114600896835327
163 1.1298669576644897
164 1.2267731428146362
165 1.1772300004959106
166 1.1138261556625366
167 1.245384931564331
168 1.2007055282592773
169 1.1780208349227905
170 1.3287227153778076
171 1.1985307931900024
172 1.2159982919692993
173 1.1819149255752563
174 1.1462947130203247
175 1.1304287910461426
176 1.2992252111434937
177 1.4268977642059326
178 1.3465975522994995
179 1.3395251035690308
180 1.2188067436218262
181 1.1928021907806396
182 1.225164532661438
183 1.3210253715515137
184 1.2207117080688477
185 1.1802877187728882
186 1.1058646440505981
187 1.2128788232803345
188 1.247613549232483
189 1.3280818462371826
190 1.2725316286087036
191 1.2687948942184448
192 1.3846142292022705
193 1.3595651388168335
194 1.3074123859405518
195 1.3000065088272095
196 1.0769935846328735
197 1.2899258136749268
198 1.1912906169891357
199 1.3388842344284058
.............
20.135684741660953 64.79261990450323 35.47049114480615
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3312419652938843
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.331242
1 1.3934788703918457
2 1.3283450603485107
3 1.259129285812378
4 1.2272753715515137
5 1.487782597541809
6 1.0948184728622437
7 1.332692265510559
8 1.1794770956039429
9 1.3474736213684082
10 1.0700929164886475
11 1.25633704662323
12 1.1702028512954712
13 1.19778573513031
14 1.2071633338928223
15 1.2642159461975098
16 1.2622963190078735
17 1.2273842096328735
18 1.341249942779541
19 1.191399335861206
20 1.377140998840332
21 1.2642009258270264
22 1.1869345903396606
23 1.343125343322754
24 1.2908565998077393
25 1.1529184579849243
26 1.3288681507110596
27 1.270179033279419
28 1.1550884246826172
29 1.148644208908081
30 1.1540862321853638
31 1.2247896194458008
32 1.1991583108901978
33 1.2381670475006104
34 1.1822341680526733
35 1.3047065734863281
36 1.146773338317871
37 1.1707841157913208
38 1.1337589025497437
39 1.2937865257263184
40 1.0753291845321655
41 1.2801135778427124
42 1.3852734565734863
43 1.0767430067062378
44 1.0759830474853516
45 1.1946219205856323
46 1.3224475383758545
47 1.1885818243026733
48 1.4180525541305542
49 1.2798242568969727
50 1.2934234142303467
51 1.1980654001235962
52 1.2604273557662964
53 1.1832927465438843
54 1.0105286836624146
55 1.0929148197174072
56 1.3071238994598389
57 1.1834568977355957
58 1.2385883331298828
59 1.2748066186904907
60 1.2935631275177002
61 1.1903990507125854
62 1.2955708503723145
63 1.1902652978897095
64 1.1707510948181152
65 1.168003797531128
66 1.1469106674194336
67 1.4028958082199097
68 1.1029165983200073
69 1.2570005655288696
70 1.242250919342041
71 1.4294188022613525
72 1.192794919013977
73 1.1017329692840576
74 1.2348332405090332
75 1.2428478002548218
76 1.158154010772705
77 1.0122581720352173
78 1.2566989660263062
79 1.2805640697479248
80 1.2906787395477295
81 1.2198195457458496
82 1.2582197189331055
83 1.2422332763671875
84 0.9702167510986328
85 1.269363284111023
86 1.3843783140182495
87 1.1853711605072021
88 1.015275239944458
89 1.1938444375991821
90 1.3299931287765503
91 1.4276586771011353
92 1.3425086736679077
93 1.2301652431488037
94 1.2789379358291626
95 1.097221851348877
96 1.1742327213287354
97 1.3684208393096924
98 1.2031488418579102
99 1.2373614311218262
100 1.1045721769332886
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.104572
101 1.3076854944229126
102 1.0865384340286255
103 1.0987778902053833
104 1.2501556873321533
105 1.2198994159698486
106 1.1660053730010986
107 1.2235137224197388
108 1.1054786443710327
109 1.351359248161316
110 1.245802640914917
111 1.136448860168457
112 1.2980773448944092
113 1.225407600402832
114 1.105729103088379
115 1.268393874168396
116 1.1955467462539673
117 1.2230961322784424
118 1.1638978719711304
119 1.0809017419815063
120 1.2867263555526733
121 1.2982302904129028
122 1.213600993156433
123 1.3501591682434082
124 1.1249017715454102
125 1.2042748928070068
126 1.2272610664367676
127 1.292384147644043
128 0.9997546672821045
129 1.2711347341537476
130 1.108410358428955
131 1.1648138761520386
132 1.1287288665771484
133 1.084357500076294
134 1.1118168830871582
135 1.0291969776153564
136 1.1515188217163086
137 0.9743978977203369
138 1.2900699377059937
139 1.0728765726089478
140 1.1071583032608032
141 1.2226210832595825
142 1.2580466270446777
143 1.1955549716949463
144 1.1860158443450928
145 1.1637004613876343
146 1.2238370180130005
147 1.121749997138977
148 1.2858572006225586
149 0.9933487772941589
150 1.1154648065567017
151 1.1709370613098145
152 1.1980292797088623
153 1.1141623258590698
154 1.1804940700531006
155 1.1708699464797974
156 1.3452650308609009
157 1.0924304723739624
158 1.2306034564971924
159 1.1122664213180542
160 1.203796625137329
161 1.0163789987564087
162 0.955849289894104
163 1.023206353187561
164 1.1049163341522217
165 1.0990591049194336
166 1.0086987018585205
167 1.112593412399292
168 1.0409460067749023
169 1.0094447135925293
170 1.1909598112106323
171 1.0700764656066895
172 1.0671417713165283
173 1.0769171714782715
174 1.0068614482879639
175 1.0090112686157227
176 1.167930245399475
177 1.2842109203338623
178 1.2578586339950562
179 1.2325880527496338
180 1.098442554473877
181 1.0352003574371338
182 1.1399139165878296
183 1.1746163368225098
184 1.0122581720352173
185 1.0542300939559937
186 0.9779412150382996
187 1.1041232347488403
188 1.1098320484161377
189 1.161812424659729
190 1.115888237953186
191 1.0852322578430176
192 1.1829874515533447
193 1.288709282875061
194 1.2527830600738525
195 1.0183062553405762
196 0.9543922543525696
197 1.1498414278030396
198 1.0462536811828613
199 1.1868029832839966
.............
61.38296418823302 195.9387255758047 107.12982741370797
.............
0 1.1624879837036133
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.162488
1 1.304674506187439
2 1.2501146793365479
3 1.1429810523986816
4 1.1381025314331055
5 1.369612455368042
6 0.9237850904464722
7 1.2001662254333496
8 1.0194830894470215
9 1.2486636638641357
10 0.9490023255348206
11 1.1162004470825195
12 1.0809688568115234
13 1.0462595224380493
14 1.1436806917190552
15 1.1270641088485718
16 1.1303166151046753
17 0.9967264533042908
18 1.2814167737960815
19 1.1047745943069458
20 1.2402397394180298
21 1.127536654472351
22 1.0190062522888184
23 1.2232656478881836
24 1.1332439184188843
25 1.0503994226455688
26 1.220929741859436
27 1.1340646743774414
28 1.0962713956832886
29 1.0712769031524658
30 1.0495190620422363
31 1.1030281782150269
32 1.0665537118911743
33 1.1388485431671143
34 1.0921661853790283
35 1.2078280448913574
36 1.0092867612838745
37 1.0962177515029907
38 1.0169110298156738
39 1.1655250787734985
40 0.9783236980438232
41 1.1730941534042358
42 1.3385255336761475
43 0.9299008846282959
44 0.9562280774116516
45 1.1167607307434082
46 1.2363674640655518
47 1.1345196962356567
48 1.2981185913085938
49 1.1873201131820679
50 1.1531987190246582
51 1.0996609926223755
52 1.2338768243789673
53 1.0247235298156738
54 0.9185225963592529
55 1.0608491897583008
56 1.2012832164764404
57 1.0831687450408936
58 1.142593264579773
59 1.2140976190567017
60 1.237511157989502
61 1.1407493352890015
62 1.1927169561386108
63 1.0591299533843994
64 1.1288025379180908
65 1.1407526731491089
66 0.9922903180122375
67 1.228469967842102
68 1.0680044889450073
69 1.1598546504974365
70 1.0742305517196655
71 1.3189888000488281
72 1.1230965852737427
73 1.0751310586929321
74 1.1878068447113037
75 1.101507306098938
76 1.0017168521881104
77 0.9611825346946716
78 1.2066999673843384
79 1.1314291954040527
80 1.0950244665145874
81 1.1139674186706543
82 1.1673673391342163
83 1.010797142982483
84 0.9743152856826782
85 1.1714670658111572
86 1.323746919631958
87 1.0368937253952026
88 0.9054186940193176
89 1.0992146730422974
90 1.2677490711212158
91 1.378494381904602
92 1.277037501335144
93 1.1470775604248047
94 1.1722816228866577
95 1.0285754203796387
96 1.1501177549362183
97 1.3739137649536133
98 1.128899335861206
99 1.1904224157333374
100 0.9984104633331299
Train Epoch: 3 [10000/20000 (50%)]	Loss: 0.998410
101 1.256127119064331
102 1.1067321300506592
103 1.1166315078735352
104 1.1626577377319336
105 1.1422096490859985
106 1.1345738172531128
107 1.1516335010528564
108 1.0011283159255981
109 1.3192739486694336
110 1.1337236166000366
111 1.052193284034729
112 1.1793895959854126
113 1.2226557731628418
114 0.9762009978294373
115 1.188336968421936
116 1.1962755918502808
117 1.1718021631240845
118 0.9847107529640198
119 1.0258607864379883
120 1.248966097831726
121 1.2347686290740967
122 1.1138304471969604
123 1.3056950569152832
124 1.091371774673462
125 1.2116023302078247
126 1.2088927030563354
127 1.2444911003112793
128 0.9428685903549194
129 1.2395715713500977
130 1.1098271608352661
131 1.0869389772415161
132 1.1060452461242676
133 1.0293418169021606
134 1.061245322227478
135 0.9914184808731079
136 1.1363276243209839
137 0.9082232117652893
138 1.266043782234192
139 1.144240140914917
140 1.0693013668060303
141 1.2345821857452393
142 1.2033705711364746
143 1.1504244804382324
144 1.199753999710083
145 1.1390948295593262
146 1.195128321647644
147 1.040061116218567
148 1.2105209827423096
149 0.9479575157165527
150 1.1122431755065918
151 1.1362570524215698
152 1.161535620689392
153 1.0896656513214111
154 1.1325690746307373
155 1.1187562942504883
156 1.3163355588912964
157 1.061260461807251
158 1.161146879196167
159 1.0854240655899048
160 1.1667776107788086
161 0.9613708257675171
162 0.9088786244392395
163 0.975497305393219
164 1.0512539148330688
165 1.0661956071853638
166 0.947334349155426
167 1.0561670064926147
168 0.9795148372650146
169 0.969897449016571
170 1.1711534261703491
171 1.0183309316635132
172 1.003019094467163
173 1.0199573040008545
174 0.9948226809501648
175 0.9851210713386536
176 1.1589730978012085
177 1.2124590873718262
178 1.2191343307495117
179 1.1752560138702393
180 1.059788703918457
181 1.016528606414795
182 1.0861741304397583
183 1.1244051456451416
184 0.9806935787200928
185 1.0214736461639404
186 0.9349260926246643
187 1.0758739709854126
188 1.0624020099639893
189 1.1408337354660034
190 1.0753226280212402
191 1.036142349243164
192 1.1341769695281982
193 1.248531699180603
194 1.2148315906524658
195 0.9622082710266113
196 0.9201145172119141
197 1.081112027168274
198 1.0138615369796753
199 1.1588109731674194
.............
144.27891817502677 459.14145641587675 251.20613009482622
.............
0 1.1141360998153687
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.114136
1 1.2778384685516357
2 1.2062443494796753
3 1.1306568384170532
4 1.080012321472168
5 1.3309673070907593
6 0.8935098052024841
7 1.1715388298034668
8 0.9870672821998596
9 1.2051335573196411
10 0.9176635146141052
11 1.0682703256607056
12 1.0448952913284302
13 1.0117926597595215
14 1.1393011808395386
15 1.0935397148132324
16 1.0848580598831177
17 0.9411929845809937
18 1.2760915756225586
19 1.059303641319275
20 1.1977781057357788
21 1.0853962898254395
22 0.9713014960289001
23 1.1850860118865967
24 1.1036187410354614
25 1.015001654624939
26 1.2018263339996338
27 1.0886085033416748
28 1.095521330833435
29 1.0691208839416504
30 1.0203278064727783
31 1.1090792417526245
32 1.0251692533493042
33 1.1114022731781006
34 1.0760782957077026
35 1.184424877166748
36 1.0232348442077637
37 1.0963752269744873
38 1.0075596570968628
39 1.1598553657531738
40 0.9570830464363098
41 1.1586582660675049
42 1.309781789779663
43 0.9088037014007568
44 0.9581451416015625
45 1.0956569910049438
46 1.2301547527313232
47 1.101934790611267
48 1.2794482707977295
49 1.1506052017211914
50 1.1489415168762207
51 1.078220009803772
52 1.1898162364959717
53 1.0237493515014648
54 0.9115731120109558
55 1.0329465866088867
56 1.1494739055633545
57 1.0579650402069092
58 1.1214152574539185
59 1.1672320365905762
60 1.1914491653442383
61 1.125440239906311
62 1.1540303230285645
63 1.0102540254592896
64 1.0924304723739624
65 1.0938596725463867
66 0.9662870168685913
67 1.2043251991271973
68 1.0406359434127808
69 1.11479914188385
70 1.0326757431030273
71 1.2887016534805298
72 1.110966444015503
73 1.063187599182129
74 1.1728558540344238
75 1.0723758935928345
76 0.9789156913757324
77 0.9495787620544434
78 1.2000714540481567
79 1.1120930910110474
80 1.0754201412200928
81 1.0896273851394653
82 1.1579090356826782
83 0.9837566614151001
84 0.9588636159896851
85 1.147241234779358
86 1.261488676071167
87 1.0065289735794067
88 0.906383216381073
89 1.073815941810608
90 1.2662850618362427
91 1.3674768209457397
92 1.2669804096221924
93 1.1396293640136719
94 1.1818960905075073
95 0.9933425188064575
96 1.1378287076950073
97 1.297927737236023
98 1.109078288078308
99 1.1649249792099
100 0.9814386963844299
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.981439
101 1.2245924472808838
102 1.0835564136505127
103 1.0982310771942139
104 1.139865756034851
105 1.1267673969268799
106 1.0959513187408447
107 1.127197265625
108 0.9879106283187866
109 1.2864030599594116
110 1.1018589735031128
111 1.0306776762008667
112 1.147475242614746
113 1.192967176437378
114 0.9520317912101746
115 1.1599125862121582
116 1.1597174406051636
117 1.1401008367538452
118 0.9847249388694763
119 0.9896087646484375
120 1.2067437171936035
121 1.2325999736785889
122 1.0680029392242432
123 1.2674493789672852
124 1.0438933372497559
125 1.1754727363586426
126 1.1999502182006836
127 1.2229819297790527
128 0.9485201239585876
129 1.1851162910461426
130 1.1003504991531372
131 1.047590732574463
132 1.0330337285995483
133 1.000458002090454
134 1.0240408182144165
135 0.9823256731033325
136 1.134610652923584
137 0.8857735395431519
138 1.2470066547393799
139 1.1150306463241577
140 1.054075837135315
141 1.2391078472137451
142 1.1713402271270752
143 1.1216216087341309
144 1.194597840309143
145 1.1347265243530273
146 1.1998809576034546
147 1.0067105293273926
148 1.1832830905914307
149 0.9205083250999451
150 1.1056782007217407
151 1.142163872718811
152 1.1682616472244263
153 1.0736240148544312
154 1.0752511024475098
155 1.0969514846801758
156 1.3110573291778564
157 1.0827866792678833
158 1.143753170967102
159 1.0921622514724731
160 1.190198540687561
161 0.9542067646980286
162 0.9092376232147217
163 1.0038886070251465
164 1.0520204305648804
165 1.0402029752731323
166 0.9281563758850098
167 1.0401803255081177
168 0.9893918633460999
169 0.9623322486877441
170 1.1632659435272217
171 1.0074738264083862
172 0.9905100464820862
173 0.9969645142555237
174 1.0054938793182373
175 0.9763012528419495
176 1.1624541282653809
177 1.1857268810272217
178 1.1865547895431519
179 1.1713495254516602
180 1.0528380870819092
181 1.0101826190948486
182 1.057644009590149
183 1.1050165891647339
184 0.9683998227119446
185 1.0171418190002441
186 0.922624945640564
187 1.0726052522659302
188 1.0497217178344727
189 1.1226636171340942
190 1.0593658685684204
191 1.018333077430725
192 1.113687515258789
193 1.2058836221694946
194 1.1961194276809692
195 0.9365569949150085
196 0.9123739004135132
197 1.0499993562698364
198 0.9957011342048645
199 1.1405928134918213
.............
309.33912427537143 983.8919857181609 538.4798856358975
.............
0 1.0927051305770874
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.092705
1 1.271787405014038
2 1.1937568187713623
3 1.1225963830947876
4 1.046562910079956
5 1.3056241273880005
6 0.8789190649986267
7 1.153719425201416
8 0.9701535701751709
9 1.194687843322754
10 0.9161319136619568
11 1.050950527191162
12 1.0396651029586792
13 1.006554365158081
14 1.1347123384475708
15 1.0740115642547607
16 1.0646532773971558
17 0.9161908030509949
18 1.2692779302597046
19 1.032629370689392
20 1.1884745359420776
21 1.065393090248108
22 0.9452338218688965
23 1.1664420366287231
24 1.097020149230957
25 0.9914891719818115
26 1.1919139623641968
27 1.058050513267517
28 1.06306791305542
29 1.0615718364715576
30 0.9780504107475281
31 1.0498281717300415
32 1.006219506263733
33 1.0751687288284302
34 1.0410712957382202
35 1.179257869720459
36 0.9798638820648193
37 1.0703423023223877
38 0.9977530837059021
39 1.184883713722229
40 0.9470254778862
41 1.1056530475616455
42 1.2913236618041992
43 0.9134982824325562
44 0.924679160118103
45 1.0853561162948608
46 1.251359224319458
47 1.0695202350616455
48 1.29538893699646
49 1.1256051063537598
50 1.1904528141021729
51 1.1020054817199707
52 1.1227185726165771
53 1.0308500528335571
54 0.9258795380592346
55 1.0163534879684448
56 1.1241837739944458
57 1.0436078310012817
58 1.1237109899520874
59 1.1528892517089844
60 1.1358802318572998
61 1.119645595550537
62 1.1513550281524658
63 0.9444219470024109
64 1.0243065357208252
65 1.0610305070877075
66 0.9450339674949646
67 1.1731330156326294
68 1.0191386938095093
69 1.088903546333313
70 0.9656503796577454
71 1.2681654691696167
72 1.1007589101791382
73 1.0307204723358154
74 1.1529433727264404
75 1.0604346990585327
76 0.9671787023544312
77 0.9360124468803406
78 1.1799044609069824
79 1.0897737741470337
80 1.0714428424835205
81 1.0636248588562012
82 1.1196813583374023
83 0.9734811186790466
84 0.9006075263023376
85 1.1230885982513428
86 1.2226446866989136
87 0.9761940240859985
88 0.8878048658370972
89 1.057490348815918
90 1.241814136505127
91 1.3458524942398071
92 1.2600561380386353
93 1.1062369346618652
94 1.1501731872558594
95 0.9702706336975098
96 1.0927923917770386
97 1.2356653213500977
98 1.0863277912139893
99 1.1294118165969849
100 0.9468255639076233
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.946826
101 1.191108226776123
102 1.0489166975021362
103 1.0684936046600342
104 1.1188946962356567
105 1.1058132648468018
106 1.0706210136413574
107 1.1033488512039185
108 0.9756928086280823
109 1.2601816654205322
110 1.0873132944107056
111 1.0269899368286133
112 1.1150474548339844
113 1.1954702138900757
114 0.9535683989524841
115 1.1306651830673218
116 1.1317263841629028
117 1.117842197418213
118 0.9966510534286499
119 0.9672867655754089
120 1.1641879081726074
121 1.2483572959899902
122 1.0472803115844727
123 1.2225699424743652
124 1.0262271165847778
125 1.162257432937622
126 1.1818959712982178
127 1.1919784545898438
128 0.9490048885345459
129 1.1404515504837036
130 1.0776408910751343
131 1.033703088760376
132 0.976661205291748
133 0.9673715233802795
134 1.0155764818191528
135 0.9684958457946777
136 1.118262529373169
137 0.8676687479019165
138 1.222451090812683
139 1.0723066329956055
140 1.0290554761886597
141 1.2282605171203613
142 1.1486084461212158
143 1.0980473756790161
144 1.175052523612976
145 1.1207581758499146
146 1.1865423917770386
147 0.9804502129554749
148 1.1677590608596802
149 0.8894544839859009
150 1.101555347442627
151 1.1447422504425049
152 1.1562933921813965
153 1.0581743717193604
154 1.059260606765747
155 1.058485984802246
156 1.3036810159683228
157 1.10530424118042
158 1.1318279504776
159 1.0734349489212036
160 1.203121304512024
161 0.9526993036270142
162 0.8946555256843567
163 1.0429247617721558
164 1.0821932554244995
165 1.0205841064453125
166 0.9068385362625122
167 1.0213967561721802
168 0.9826234579086304
169 0.9618884921073914
170 1.1590213775634766
171 1.0132548809051514
172 0.9975681900978088
173 0.9594805240631104
174 0.973232626914978
175 0.9708636999130249
176 1.1586856842041016
177 1.1626169681549072
178 1.1656590700149536
179 1.1453909873962402
180 1.044904351234436
181 1.0083034038543701
182 1.0399667024612427
183 1.0811283588409424
184 0.9690548181533813
185 0.9951954483985901
186 0.9150342345237732
187 1.0598225593566895
188 1.0350100994110107
189 1.1253411769866943
190 1.0432183742523193
191 1.0009703636169434
192 1.0903797149658203
193 1.171181559562683
194 1.1903525590896606
195 0.9313198328018188
196 0.904201865196228
197 1.0211924314498901
198 0.9722626209259033
199 1.1270922422409058
.............
640.3754719737917 2034.8818957395852 1113.991049895063
.............
2 640.3754719737917 2034.8818957395852 1113.991049895063
.............
