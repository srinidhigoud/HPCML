0 2.8261001110076904
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.826100
1 2.807039499282837
2 2.772291898727417
3 2.705749273300171
4 2.6717987060546875
5 2.5962576866149902
6 2.459617853164673
7 2.4198851585388184
8 2.26914119720459
9 2.184453010559082
10 2.009697914123535
11 1.8832199573516846
12 1.7861157655715942
13 2.0069758892059326
14 1.8903911113739014
15 1.9304938316345215
16 1.8034417629241943
17 1.8967851400375366
18 1.7182104587554932
19 1.8649672269821167
20 1.9734495878219604
21 1.8119227886199951
22 1.539435863494873
23 1.8112133741378784
24 1.659753680229187
25 1.7625000476837158
26 1.8517969846725464
27 1.7866977453231812
28 1.4958761930465698
29 1.6734730005264282
30 1.658725619316101
31 1.7040725946426392
32 1.6176141500473022
33 1.608330488204956
34 1.6449335813522339
35 1.7464549541473389
36 1.5408116579055786
37 1.6366673707962036
38 1.5417311191558838
39 1.5696182250976562
40 1.565232753753662
41 1.6125596761703491
42 1.6431676149368286
43 1.5167964696884155
44 1.419532299041748
45 1.4663949012756348
46 1.7244431972503662
47 1.6050928831100464
48 1.6793299913406372
49 1.6010454893112183
50 1.5222105979919434
51 1.5367767810821533
52 1.5695561170578003
53 1.4777356386184692
54 1.2909599542617798
55 1.5841189622879028
56 1.7660179138183594
57 1.545972466468811
58 1.463076114654541
59 1.6592919826507568
60 1.68748140335083
61 1.5872387886047363
62 1.5722012519836426
63 1.5803859233856201
64 1.6556812524795532
65 1.6113020181655884
66 1.4581429958343506
67 1.5375022888183594
68 1.5156440734863281
69 1.5984317064285278
70 1.4339725971221924
71 1.6256963014602661
72 1.4278641939163208
73 1.424037218093872
74 1.4120434522628784
75 1.412958025932312
76 1.4072341918945312
77 1.3292524814605713
78 1.4808447360992432
79 1.5119757652282715
80 1.3594601154327393
81 1.4403572082519531
82 1.5299577713012695
83 1.465091586112976
84 1.245442509651184
85 1.489182949066162
86 1.6170058250427246
87 1.4491583108901978
88 1.3160113096237183
89 1.4137451648712158
90 1.535253643989563
91 1.6135107278823853
92 1.5855451822280884
93 1.4502102136611938
94 1.4528692960739136
95 1.3178869485855103
96 1.3852143287658691
97 1.6147470474243164
98 1.5615824460983276
99 1.393941044807434
100 1.3402577638626099
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.340258
101 1.4998186826705933
102 1.351597785949707
103 1.2980149984359741
104 1.4692742824554443
105 1.5387654304504395
106 1.391761302947998
107 1.4481251239776611
108 1.3703879117965698
109 1.5364184379577637
110 1.4389528036117554
111 1.3360449075698853
112 1.5420891046524048
113 1.4619314670562744
114 1.1916252374649048
115 1.3874328136444092
116 1.3443619012832642
117 1.3972649574279785
118 1.2533930540084839
119 1.248029112815857
120 1.4740034341812134
121 1.4017170667648315
122 1.3358477354049683
123 1.4860767126083374
124 1.2300761938095093
125 1.3267524242401123
126 1.311982274055481
127 1.4076522588729858
128 1.131711483001709
129 1.4100029468536377
130 1.2515264749526978
131 1.301384687423706
132 1.2779514789581299
133 1.2826794385910034
134 1.302823543548584
135 1.1979622840881348
136 1.2368203401565552
137 1.128231167793274
138 1.3964523077011108
139 1.1144863367080688
140 1.2979680299758911
141 1.3560158014297485
142 1.4090958833694458
143 1.3172520399093628
144 1.284388542175293
145 1.3079428672790527
146 1.32444167137146
147 1.2807432413101196
148 1.4266648292541504
149 1.177505612373352
150 1.1931800842285156
151 1.3143366575241089
152 1.3089178800582886
153 1.2312030792236328
154 1.324161410331726
155 1.3582241535186768
156 1.4155131578445435
157 1.1865496635437012
158 1.388299822807312
159 1.1994231939315796
160 1.3297878503799438
161 1.1373388767242432
162 1.0976786613464355
163 1.1327005624771118
164 1.227209210395813
165 1.1714767217636108
166 1.103944182395935
167 1.243815541267395
168 1.1897859573364258
169 1.1752406358718872
170 1.3299716711044312
171 1.202897071838379
172 1.2206313610076904
173 1.1712899208068848
174 1.1294093132019043
175 1.1231569051742554
176 1.2991610765457153
177 1.416159987449646
178 1.3418083190917969
179 1.3309779167175293
180 1.2216705083847046
181 1.1856223344802856
182 1.2255967855453491
183 1.3117679357528687
184 1.2187219858169556
185 1.1793993711471558
186 1.103968620300293
187 1.206557035446167
188 1.2476211786270142
189 1.3271746635437012
190 1.259777545928955
191 1.2710379362106323
192 1.392274022102356
193 1.3704078197479248
194 1.297398328781128
195 1.2916370630264282
196 1.0747135877609253
197 1.2770564556121826
198 1.1807516813278198
199 1.3386682271957397
./lab1.pytorch:83: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
slurmstepd: error: *** JOB 4861729 ON c33-12 CANCELLED AT 2018-03-12T02:53:53 ***
