0 2.864229440689087
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.864229
1 2.8333029747009277
2 2.799795627593994
3 2.7437210083007812
4 2.71162486076355
5 2.64546275138855
6 2.53843355178833
7 2.498614549636841
8 2.3692407608032227
9 2.263880729675293
10 2.091550588607788
11 1.9565104246139526
12 1.8322317600250244
13 2.034712553024292
14 1.932602047920227
15 1.9954890012741089
16 1.8894071578979492
17 2.023380994796753
18 1.7026351690292358
19 1.7972187995910645
20 1.9614379405975342
21 1.862267255783081
22 1.604812741279602
23 1.7645776271820068
24 1.6584842205047607
25 1.7263588905334473
26 1.85105299949646
27 1.803504228591919
28 1.496096134185791
29 1.692136287689209
30 1.670815110206604
31 1.7348544597625732
32 1.6070998907089233
33 1.60097074508667
34 1.6513131856918335
35 1.7516926527023315
36 1.5496766567230225
37 1.6485323905944824
38 1.5617430210113525
39 1.5728199481964111
40 1.564940333366394
41 1.6010090112686157
42 1.6423819065093994
43 1.5255671739578247
44 1.427778720855713
45 1.457918405532837
46 1.7296377420425415
47 1.6144001483917236
48 1.6801426410675049
49 1.6064378023147583
50 1.526065468788147
51 1.542646050453186
52 1.5699584484100342
53 1.5008289813995361
54 1.3105043172836304
55 1.5516347885131836
56 1.7494474649429321
57 1.563433051109314
58 1.4692007303237915
59 1.6084847450256348
60 1.670911431312561
61 1.613407850265503
62 1.5547174215316772
63 1.5329010486602783
64 1.601282000541687
65 1.5765341520309448
66 1.455744981765747
67 1.5313845872879028
68 1.495298147201538
69 1.551484227180481
70 1.4123529195785522
71 1.6140594482421875
72 1.4305448532104492
73 1.406124234199524
74 1.3856374025344849
75 1.401681661605835
76 1.3908014297485352
77 1.3121592998504639
78 1.4767102003097534
79 1.5126090049743652
80 1.3616596460342407
81 1.4392914772033691
82 1.5228300094604492
83 1.4646236896514893
84 1.2444649934768677
85 1.492468237876892
86 1.6194764375686646
87 1.4479362964630127
88 1.3076679706573486
89 1.4175331592559814
90 1.5337179899215698
91 1.6111390590667725
92 1.5803462266921997
93 1.4524234533309937
94 1.455011248588562
95 1.3179928064346313
96 1.3838472366333008
97 1.6192001104354858
98 1.5675982236862183
99 1.3951624631881714
100 1.3460012674331665
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.346001
101 1.5032293796539307
102 1.3602279424667358
103 1.3006401062011719
104 1.4704809188842773
105 1.5402313470840454
106 1.3899717330932617
107 1.4597234725952148
108 1.3810734748840332
109 1.5413410663604736
110 1.4435876607894897
111 1.3413479328155518
112 1.5512783527374268
113 1.4740885496139526
114 1.1958348751068115
115 1.3920207023620605
116 1.3519325256347656
117 1.406030297279358
118 1.2565104961395264
119 1.251211404800415
120 1.4780797958374023
121 1.4016470909118652
122 1.3362735509872437
123 1.4910423755645752
124 1.2377911806106567
125 1.3321025371551514
126 1.316962480545044
127 1.4128113985061646
128 1.1405709981918335
129 1.4081135988235474
130 1.2518025636672974
131 1.3071397542953491
132 1.2819480895996094
133 1.2832506895065308
134 1.3100165128707886
135 1.2045284509658813
136 1.2363067865371704
137 1.136236548423767
138 1.3998284339904785
139 1.1197110414505005
140 1.3027350902557373
141 1.3665446043014526
142 1.41288423538208
143 1.312605381011963
144 1.2815182209014893
145 1.310734748840332
146 1.3232221603393555
147 1.2789214849472046
148 1.4262151718139648
149 1.1810840368270874
150 1.1957118511199951
151 1.3129870891571045
152 1.3097723722457886
153 1.2384394407272339
154 1.3314390182495117
155 1.366671085357666
156 1.4167473316192627
157 1.1883543729782104
158 1.3922196626663208
159 1.1983848810195923
160 1.3346856832504272
161 1.1386058330535889
162 1.1098930835723877
163 1.1364933252334595
164 1.233970046043396
165 1.1733840703964233
166 1.1077709197998047
167 1.2476325035095215
168 1.1947227716445923
169 1.1802016496658325
170 1.3312079906463623
171 1.1956907510757446
172 1.2194604873657227
173 1.1744985580444336
174 1.1374890804290771
175 1.1244808435440063
176 1.2980353832244873
177 1.424605369567871
178 1.3392459154129028
179 1.3306900262832642
180 1.2271161079406738
181 1.1895123720169067
182 1.2214975357055664
183 1.3148994445800781
184 1.2317472696304321
185 1.185988187789917
186 1.1082640886306763
187 1.2145403623580933
188 1.250191569328308
189 1.3229460716247559
190 1.267163872718811
191 1.2714680433273315
192 1.381650686264038
193 1.3632681369781494
194 1.2872270345687866
195 1.2832320928573608
196 1.0813918113708496
197 1.2804229259490967
198 1.1728403568267822
199 1.338474988937378
.............
406.8433762062341 92.73718843236566 24.71146923303604
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3355354070663452
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.335535
1 1.4040184020996094
2 1.3240265846252441
3 1.2459352016448975
4 1.2261897325515747
5 1.4940247535705566
6 1.0822017192840576
7 1.335872769355774
8 1.1788967847824097
9 1.3484585285186768
10 1.0636833906173706
11 1.2519587278366089
12 1.170268177986145
13 1.1959443092346191
14 1.205723524093628
15 1.271930456161499
16 1.2610167264938354
17 1.2343357801437378
18 1.3371609449386597
19 1.1845489740371704
20 1.3668795824050903
21 1.265296220779419
22 1.1832594871520996
23 1.344874620437622
24 1.2930150032043457
25 1.1540937423706055
26 1.328282117843628
27 1.269076943397522
28 1.1502944231033325
29 1.1401616334915161
30 1.1542564630508423
31 1.227562427520752
32 1.1966038942337036
33 1.2393347024917603
34 1.180345892906189
35 1.3036255836486816
36 1.1381804943084717
37 1.165561556816101
38 1.1319026947021484
39 1.2943315505981445
40 1.0746575593948364
41 1.2664037942886353
42 1.377506136894226
43 1.0810524225234985
44 1.0817666053771973
45 1.1941429376602173
46 1.3291046619415283
47 1.1897804737091064
48 1.4232144355773926
49 1.2813544273376465
50 1.2974401712417603
51 1.1975390911102295
52 1.2535512447357178
53 1.1853946447372437
54 1.014396071434021
55 1.0914173126220703
56 1.2981399297714233
57 1.1779876947402954
58 1.2372581958770752
59 1.2646880149841309
60 1.2887159585952759
61 1.1881977319717407
62 1.291318655014038
63 1.1777838468551636
64 1.1638365983963013
65 1.168062448501587
66 1.145126223564148
67 1.3839367628097534
68 1.097724199295044
69 1.2545799016952515
70 1.227522611618042
71 1.4200059175491333
72 1.1871528625488281
73 1.1025923490524292
74 1.2309558391571045
75 1.244822382926941
76 1.1565725803375244
77 1.0090824365615845
78 1.2570254802703857
79 1.2827956676483154
80 1.276798129081726
81 1.219745397567749
82 1.2624149322509766
83 1.2441679239273071
84 0.9658377766609192
85 1.2635501623153687
86 1.3841493129730225
87 1.1798056364059448
88 1.0260049104690552
89 1.187218189239502
90 1.332700490951538
91 1.423423171043396
92 1.3414244651794434
93 1.225070595741272
94 1.2767834663391113
95 1.0950113534927368
96 1.159183144569397
97 1.3825105428695679
98 1.1996252536773682
99 1.2406052350997925
100 1.1080994606018066
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.108099
101 1.3060067892074585
102 1.0828769207000732
103 1.096354603767395
104 1.2543251514434814
105 1.2234705686569214
106 1.1665420532226562
107 1.2270655632019043
108 1.1196177005767822
109 1.3413381576538086
110 1.2372124195098877
111 1.135172963142395
112 1.2989920377731323
113 1.2207903861999512
114 1.0977530479431152
115 1.2660354375839233
116 1.2011032104492188
117 1.2176358699798584
118 1.1455566883087158
119 1.078391432762146
120 1.2826701402664185
121 1.294399380683899
122 1.2074187994003296
123 1.3562614917755127
124 1.127255916595459
125 1.1990238428115845
126 1.2234232425689697
127 1.2830623388290405
128 0.9980215430259705
129 1.2704261541366577
130 1.102152943611145
131 1.1569364070892334
132 1.1252858638763428
133 1.0843935012817383
134 1.106629729270935
135 1.028972864151001
136 1.1483447551727295
137 0.9740877747535706
138 1.291203260421753
139 1.0614464282989502
140 1.1060289144515991
141 1.2251386642456055
142 1.2517266273498535
143 1.1886837482452393
144 1.1770256757736206
145 1.1653704643249512
146 1.2312724590301514
147 1.1216410398483276
148 1.2868837118148804
149 0.9972959160804749
150 1.1079155206680298
151 1.166771411895752
152 1.1983078718185425
153 1.1079185009002686
154 1.1731693744659424
155 1.1705546379089355
156 1.3493354320526123
157 1.0968950986862183
158 1.2244240045547485
159 1.1072661876678467
160 1.2060810327529907
161 1.016547441482544
162 0.9539932012557983
163 1.0318946838378906
164 1.109026312828064
165 1.0907201766967773
166 1.006433129310608
167 1.1165642738342285
168 1.046675205230713
169 1.0100082159042358
170 1.192621111869812
171 1.0734314918518066
172 1.064782738685608
173 1.078525185585022
174 1.0112175941467285
175 1.0095213651657104
176 1.1686233282089233
177 1.2871983051300049
178 1.2566357851028442
179 1.2359110116958618
180 1.100131630897522
181 1.037135362625122
182 1.1411255598068237
183 1.1653670072555542
184 1.006311297416687
185 1.0507680177688599
186 0.9764367938041687
187 1.1021713018417358
188 1.1065521240234375
189 1.1574174165725708
190 1.1149345636367798
191 1.0825612545013428
192 1.1727831363677979
193 1.277589201927185
194 1.248395323753357
195 1.018085241317749
196 0.951962947845459
197 1.14667546749115
198 1.0399922132492065
199 1.1865710020065308
.............
983.2735045384616 400.56203108653426 67.72093203105032
.............
0 1.1515179872512817
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.151518
1 1.3063310384750366
2 1.2414391040802002
3 1.1402782201766968
4 1.133741021156311
5 1.3669341802597046
6 0.923570454120636
7 1.200395941734314
8 1.0161762237548828
9 1.2461447715759277
10 0.9424324035644531
11 1.113015055656433
12 1.0816149711608887
13 1.042323112487793
14 1.1430270671844482
15 1.1261365413665771
16 1.1270439624786377
17 0.9982149004936218
18 1.2790844440460205
19 1.0922834873199463
20 1.2394660711288452
21 1.126254916191101
22 1.0113786458969116
23 1.2175384759902954
24 1.126643180847168
25 1.045994520187378
26 1.2179116010665894
27 1.1299219131469727
28 1.0902060270309448
29 1.0706156492233276
30 1.0578093528747559
31 1.093754529953003
32 1.0661277770996094
33 1.1300104856491089
34 1.0900664329528809
35 1.2025196552276611
36 1.0094190835952759
37 1.0847498178482056
38 1.019286036491394
39 1.165332317352295
40 0.9790060520172119
41 1.1655958890914917
42 1.3227555751800537
43 0.9305387139320374
44 0.9598808288574219
45 1.1132235527038574
46 1.2461671829223633
47 1.1258488893508911
48 1.2953577041625977
49 1.1952500343322754
50 1.1583287715911865
51 1.0971964597702026
52 1.2271029949188232
53 1.028873324394226
54 0.9172813296318054
55 1.0570300817489624
56 1.1984758377075195
57 1.0766884088516235
58 1.1446038484573364
59 1.2111872434616089
60 1.2330206632614136
61 1.1334503889083862
62 1.195239543914795
63 1.0648443698883057
64 1.1307121515274048
65 1.124220848083496
66 0.9896647930145264
67 1.241715908050537
68 1.0649317502975464
69 1.1605184078216553
70 1.074641227722168
71 1.3144431114196777
72 1.1173416376113892
73 1.0833766460418701
74 1.1728123426437378
75 1.1059987545013428
76 1.011926293373108
77 0.9637088775634766
78 1.2072632312774658
79 1.1286540031433105
80 1.0913820266723633
81 1.1255667209625244
82 1.1719539165496826
83 1.008266806602478
84 0.9754989147186279
85 1.1731089353561401
86 1.327052116394043
87 1.036139726638794
88 0.9156688451766968
89 1.0985838174819946
90 1.2711459398269653
91 1.3758963346481323
92 1.2743812799453735
93 1.1390002965927124
94 1.1755738258361816
95 1.0161981582641602
96 1.147666573524475
97 1.3639110326766968
98 1.1288985013961792
99 1.183615803718567
100 1.001857876777649
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.001858
101 1.2483644485473633
102 1.1023062467575073
103 1.107285499572754
104 1.164211392402649
105 1.1442255973815918
106 1.14024019241333
107 1.150591492652893
108 1.0010446310043335
109 1.3097115755081177
110 1.130407691001892
111 1.0491241216659546
112 1.1696972846984863
113 1.207836627960205
114 0.9637426733970642
115 1.1872063875198364
116 1.2017884254455566
117 1.1619555950164795
118 0.9724826812744141
119 1.021730899810791
120 1.2253249883651733
121 1.243938684463501
122 1.1057063341140747
123 1.2829846143722534
124 1.0826916694641113
125 1.2052456140518188
126 1.2158509492874146
127 1.2410228252410889
128 0.9418059587478638
129 1.1969647407531738
130 1.0923117399215698
131 1.074483871459961
132 1.091321587562561
133 1.0072050094604492
134 1.0589836835861206
135 0.9901382923126221
136 1.1222336292266846
137 0.8985172510147095
138 1.2626421451568604
139 1.1288700103759766
140 1.0608606338500977
141 1.2433562278747559
142 1.196699857711792
143 1.1558425426483154
144 1.1953223943710327
145 1.1394448280334473
146 1.2040923833847046
147 1.0445315837860107
148 1.2213019132614136
149 0.9393206834793091
150 1.1181151866912842
151 1.1482815742492676
152 1.1645358800888062
153 1.0908446311950684
154 1.1495403051376343
155 1.1265875101089478
156 1.3154759407043457
157 1.0630183219909668
158 1.153753399848938
159 1.0992003679275513
160 1.1725187301635742
161 0.9631803035736084
162 0.9137722253799438
163 0.9676862955093384
164 1.0460669994354248
165 1.0687028169631958
166 0.9492490291595459
167 1.0570322275161743
168 0.9774059057235718
169 0.9750843644142151
170 1.1621270179748535
171 1.0171937942504883
172 1.0009177923202515
173 1.0127125978469849
174 0.9884259104728699
175 0.971320390701294
176 1.1541845798492432
177 1.2137649059295654
178 1.209667682647705
179 1.1723443269729614
180 1.056002140045166
181 1.0061484575271606
182 1.1010127067565918
183 1.1133872270584106
184 0.9741652607917786
185 1.016240119934082
186 0.9282150864601135
187 1.066942572593689
188 1.0550220012664795
189 1.1363846063613892
190 1.0669893026351929
191 1.032283902168274
192 1.1185106039047241
193 1.2432101964950562
194 1.210388422012329
195 0.9574208855628967
196 0.9167549014091492
197 1.0785880088806152
198 1.0103532075881958
199 1.1561163663864136
.............
2144.361183060333 1024.1102786175907 154.23234610073268
.............
0 1.1021058559417725
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.102106
1 1.2769891023635864
2 1.1917232275009155
3 1.1184029579162598
4 1.0810039043426514
5 1.315710425376892
6 0.895709216594696
7 1.1649984121322632
8 0.9808528423309326
9 1.201992154121399
10 0.913664698600769
11 1.0595365762710571
12 1.041576862335205
13 1.0042246580123901
14 1.139413595199585
15 1.0854928493499756
16 1.0721220970153809
17 0.9402546882629395
18 1.2740123271942139
19 1.0459882020950317
20 1.1921327114105225
21 1.0848069190979004
22 0.9578844308853149
23 1.1690266132354736
24 1.0972537994384766
25 1.0039252042770386
26 1.1958645582199097
27 1.085828185081482
28 1.0862493515014648
29 1.0588328838348389
30 1.0155611038208008
31 1.0873684883117676
32 1.0243141651153564
33 1.1035211086273193
34 1.063522219657898
35 1.1740596294403076
36 1.0095584392547607
37 1.0791771411895752
38 1.003187894821167
39 1.1526312828063965
40 0.955976128578186
41 1.1238332986831665
42 1.2907177209854126
43 0.8968814611434937
44 0.9557672739028931
45 1.0916171073913574
46 1.2327114343643188
47 1.088067650794983
48 1.2709057331085205
49 1.1449518203735352
50 1.1512222290039062
51 1.0703688859939575
52 1.1780309677124023
53 1.0186766386032104
54 0.9057250022888184
55 1.018043041229248
56 1.1339387893676758
57 1.0524227619171143
58 1.1236687898635864
59 1.1614983081817627
60 1.168866515159607
61 1.1157374382019043
62 1.1517417430877686
63 1.0037118196487427
64 1.0771790742874146
65 1.0662983655929565
66 0.9638645052909851
67 1.2139103412628174
68 1.0279022455215454
69 1.1056005954742432
70 1.0354251861572266
71 1.2770801782608032
72 1.0979974269866943
73 1.0586392879486084
74 1.153520107269287
75 1.0696979761123657
76 0.9823462963104248
77 0.9476508498191833
78 1.188369870185852
79 1.0977438688278198
80 1.0529319047927856
81 1.0958948135375977
82 1.1525477170944214
83 0.9792341589927673
84 0.9512470960617065
85 1.1485974788665771
86 1.264474630355835
87 1.0027508735656738
88 0.9105628728866577
89 1.0631468296051025
90 1.2622101306915283
91 1.3632243871688843
92 1.2647161483764648
93 1.1242738962173462
94 1.175227165222168
95 0.9758151769638062
96 1.1187434196472168
97 1.281355381011963
98 1.1012786626815796
99 1.1498571634292603
100 0.9730870127677917
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.973087
101 1.216545820236206
102 1.069525122642517
103 1.0713756084442139
104 1.1389741897583008
105 1.1208230257034302
106 1.098262071609497
107 1.120355248451233
108 0.9914787411689758
109 1.2867404222488403
110 1.1036512851715088
111 1.0315632820129395
112 1.1193413734436035
113 1.1835441589355469
114 0.938607931137085
115 1.161841869354248
116 1.1692442893981934
117 1.122439980506897
118 0.9657911062240601
119 0.9833982586860657
120 1.1810786724090576
121 1.2486279010772705
122 1.0593010187149048
123 1.2314496040344238
124 1.0352150201797485
125 1.1917147636413574
126 1.2034893035888672
127 1.2020522356033325
128 0.9449907541275024
129 1.1241817474365234
130 1.0951182842254639
131 1.0282714366912842
132 1.0050352811813354
133 0.9674015641212463
134 1.0095294713974
135 0.9718744158744812
136 1.109055519104004
137 0.8728452324867249
138 1.2269338369369507
139 1.067236065864563
140 1.0174392461776733
141 1.2297320365905762
142 1.161865234375
143 1.1049896478652954
144 1.154693365097046
145 1.113196611404419
146 1.1891889572143555
147 0.9963476657867432
148 1.1835740804672241
149 0.8972257375717163
150 1.0874617099761963
151 1.133488655090332
152 1.1315957307815552
153 1.0609148740768433
154 1.1030991077423096
155 1.0736907720565796
156 1.2947986125946045
157 1.0535824298858643
158 1.1224159002304077
159 1.0923172235488892
160 1.1817612648010254
161 0.9515525102615356
162 0.9045673608779907
163 1.0035006999969482
164 1.0421944856643677
165 1.0393092632293701
166 0.9410600066184998
167 1.0311229228973389
168 0.9679022431373596
169 0.9613194465637207
170 1.1525200605392456
171 1.0015404224395752
172 0.9952757954597473
173 0.9831414222717285
174 0.980109691619873
175 0.9500526189804077
176 1.1415866613388062
177 1.1872055530548096
178 1.1772983074188232
179 1.1582114696502686
180 1.039316177368164
181 0.9898777604103088
182 1.067055106163025
183 1.0822176933288574
184 0.955091655254364
185 1.0058306455612183
186 0.9096461534500122
187 1.0465683937072754
188 1.0342696905136108
189 1.1023083925247192
190 1.0381683111190796
191 1.0049742460250854
192 1.081887125968933
193 1.1894538402557373
194 1.1666855812072754
195 0.9220758080482483
196 0.9023891687393188
197 1.0371531248092651
198 0.9755418300628662
199 1.125535249710083
.............
4450.347817512229 2252.921037727967 326.6654283795506
.............
0 1.0714712142944336
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.071471
1 1.269980788230896
2 1.1827683448791504
3 1.0973161458969116
4 1.045354962348938
5 1.2788784503936768
6 0.8749906420707703
7 1.1305322647094727
8 0.9571990966796875
9 1.1776511669158936
10 0.9054318070411682
11 1.026503324508667
12 1.0326051712036133
13 0.9954204559326172
14 1.1219067573547363
15 1.0579601526260376
16 1.038458228111267
17 0.9052034020423889
18 1.2621650695800781
19 1.0065170526504517
20 1.166634440422058
21 1.051947832107544
22 0.9156546592712402
23 1.1293654441833496
24 1.0795845985412598
25 0.967163622379303
26 1.1696922779083252
27 1.035341739654541
28 1.0411072969436646
29 1.0263901948928833
30 0.9525351524353027
31 1.0120868682861328
32 0.9952743053436279
33 1.0555236339569092
34 1.0156272649765015
35 1.148661494255066
36 0.9465396404266357
37 1.0434231758117676
38 0.9741982817649841
39 1.1852129697799683
40 0.9345073103904724
41 1.0503040552139282
42 1.2557628154754639
43 0.8892613053321838
44 0.9142558574676514
45 1.0640041828155518
46 1.2498799562454224
47 1.0386028289794922
48 1.2719072103500366
49 1.1114342212677002
50 1.1767789125442505
51 1.0905067920684814
52 1.08391273021698
53 1.0105056762695312
54 0.8942380547523499
55 0.985592246055603
56 1.1159677505493164
57 1.0374515056610107
58 1.1078130006790161
59 1.1374086141586304
60 1.1192876100540161
61 1.1066840887069702
62 1.137798547744751
63 0.9553787112236023
64 1.014440894126892
65 1.0364069938659668
66 0.9419370293617249
67 1.1707873344421387
68 1.0021134614944458
69 1.079890251159668
70 0.9718208909034729
71 1.2235417366027832
72 1.0586129426956177
73 1.0146489143371582
74 1.1180346012115479
75 1.035996913909912
76 0.9699448347091675
77 0.9151042699813843
78 1.1432666778564453
79 1.064803123474121
80 1.0258774757385254
81 1.0560641288757324
82 1.1223183870315552
83 0.9498181343078613
84 0.8889791369438171
85 1.1000052690505981
86 1.2046302556991577
87 0.962565004825592
88 0.90380859375
89 1.0341092348098755
90 1.2283313274383545
91 1.3319278955459595
92 1.244745135307312
93 1.0796992778778076
94 1.1369999647140503
95 0.938392162322998
96 1.0497056245803833
97 1.1824266910552979
98 1.0799771547317505
99 1.1006544828414917
100 0.9167197346687317
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.916720
101 1.1598098278045654
102 1.0052317380905151
103 1.0177446603775024
104 1.105972409248352
105 1.0678194761276245
106 1.0350204706192017
107 1.1037358045578003
108 0.9587543606758118
109 1.2593445777893066
110 1.0778179168701172
111 1.0096954107284546
112 1.0630549192428589
113 1.1880711317062378
114 0.940237283706665
115 1.1144064664840698
116 1.1091784238815308
117 1.0678282976150513
118 0.9630534648895264
119 0.9562227725982666
120 1.111433506011963
121 1.2247636318206787
122 1.0139447450637817
123 1.1113770008087158
124 1.054863691329956
125 1.1564245223999023
126 1.1469401121139526
127 1.17245352268219
128 0.9710559844970703
129 1.064334750175476
130 1.039589285850525
131 1.024186611175537
132 0.9350398182868958
133 0.9236198663711548
134 1.0011991262435913
135 0.9552192091941833
136 1.0947070121765137
137 0.864966869354248
138 1.1942946910858154
139 0.9989100694656372
140 0.9754183888435364
141 1.2129008769989014
142 1.1316412687301636
143 1.0538407564163208
144 1.0922986268997192
145 1.0796581506729126
146 1.1667805910110474
147 0.9580789804458618
148 1.173729419708252
149 0.8723177313804626
150 1.0160269737243652
151 1.1155979633331299
152 1.0821442604064941
153 1.0347245931625366
154 1.0634976625442505
155 1.0006879568099976
156 1.2718946933746338
157 1.0489275455474854
158 1.0482772588729858
159 1.028287410736084
160 1.1509675979614258
161 0.936448872089386
162 0.8833915591239929
163 1.0256668329238892
164 1.0342658758163452
165 0.9919750094413757
166 0.9427681565284729
167 1.0010414123535156
168 0.9397111535072327
169 0.9244500994682312
170 1.1191909313201904
171 0.9896041750907898
172 0.9806351661682129
173 0.9301223754882812
174 0.9117112159729004
175 0.919176459312439
176 1.114205241203308
177 1.161271572113037
178 1.1730546951293945
179 1.104352593421936
180 1.0259473323822021
181 0.9655107855796814
182 1.0724393129348755
183 1.0442020893096924
184 0.9759772419929504
185 0.9769202470779419
186 0.8830432891845703
187 1.0051074028015137
188 1.0041230916976929
189 1.0872080326080322
190 0.998049259185791
191 0.9841358065605164
192 1.0416710376739502
193 1.1740036010742188
194 1.1485340595245361
195 0.8972430229187012
196 0.884104311466217
197 0.9942983984947205
198 0.9455881714820862
199 1.0898469686508179
.............
9072.39012331143 4722.070106090978 671.9208536539227
.............
13 9072.39012331143 4722.070106090978 671.9208536539227
.............
