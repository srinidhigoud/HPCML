0 2.8211655616760254
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.821166
1 2.7983052730560303
2 2.7615389823913574
3 2.7052266597747803
4 2.651658296585083
5 2.592315435409546
6 2.447908878326416
7 2.396329164505005
8 2.247326612472534
9 2.1738617420196533
10 1.9994990825653076
11 1.8659825325012207
12 1.7895829677581787
13 1.9549349546432495
14 1.8757081031799316
15 1.9064631462097168
16 1.82646906375885
17 1.8040639162063599
18 1.707757830619812
19 1.8647979497909546
20 2.036759614944458
21 1.8212472200393677
22 1.5263681411743164
23 1.7777115106582642
24 1.6450692415237427
25 1.7499220371246338
26 1.8640568256378174
27 1.7849531173706055
28 1.4900866746902466
29 1.6619340181350708
30 1.6438188552856445
31 1.691115379333496
32 1.6063897609710693
33 1.6082926988601685
34 1.6366997957229614
35 1.739617109298706
36 1.5447860956192017
37 1.6397966146469116
38 1.5466680526733398
39 1.566440463066101
40 1.564602017402649
41 1.5930339097976685
42 1.6377607583999634
43 1.5276581048965454
44 1.4269219636917114
45 1.4604676961898804
46 1.7274376153945923
47 1.6152749061584473
48 1.6820993423461914
49 1.6014572381973267
50 1.5193238258361816
51 1.544359564781189
52 1.5716592073440552
53 1.4947402477264404
54 1.2988871335983276
55 1.5646557807922363
56 1.7693864107131958
57 1.5588078498840332
58 1.4654792547225952
59 1.6313223838806152
60 1.6880371570587158
61 1.6057014465332031
62 1.561099648475647
63 1.5446572303771973
64 1.6186015605926514
65 1.590559959411621
66 1.4636837244033813
67 1.5352087020874023
68 1.504908800125122
69 1.5592448711395264
70 1.4242417812347412
71 1.6192922592163086
72 1.435469150543213
73 1.4136710166931152
74 1.385778784751892
75 1.4057681560516357
76 1.4043641090393066
77 1.3309458494186401
78 1.4851471185684204
79 1.5199624300003052
80 1.3656960725784302
81 1.4422160387039185
82 1.5358636379241943
83 1.4838998317718506
84 1.2520054578781128
85 1.5071349143981934
86 1.6269530057907104
87 1.4601430892944336
88 1.3260401487350464
89 1.4366602897644043
90 1.5434626340866089
91 1.6155157089233398
92 1.591784954071045
93 1.4594817161560059
94 1.4497302770614624
95 1.3329155445098877
96 1.3988964557647705
97 1.6398130655288696
98 1.5851695537567139
99 1.4119640588760376
100 1.360029697418213
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.360030
101 1.5182158946990967
102 1.3803249597549438
103 1.3169646263122559
104 1.4878255128860474
105 1.5655624866485596
106 1.4173938035964966
107 1.4685968160629272
108 1.382572054862976
109 1.562190294265747
110 1.4694476127624512
111 1.3663326501846313
112 1.5540579557418823
113 1.478572964668274
114 1.2075363397598267
115 1.4041950702667236
116 1.352258563041687
117 1.4145393371582031
118 1.270565390586853
119 1.2683892250061035
120 1.4793086051940918
121 1.4324212074279785
122 1.354109525680542
123 1.4938342571258545
124 1.2536698579788208
125 1.3399330377578735
126 1.3313660621643066
127 1.4251396656036377
128 1.1712390184402466
129 1.4085296392440796
130 1.2683594226837158
131 1.3182276487350464
132 1.278026819229126
133 1.2936078310012817
134 1.3174052238464355
135 1.210260272026062
136 1.2475581169128418
137 1.1381199359893799
138 1.4126821756362915
139 1.1293662786483765
140 1.3182237148284912
141 1.3673449754714966
142 1.4198393821716309
143 1.323363184928894
144 1.2954728603363037
145 1.3220442533493042
146 1.3341188430786133
147 1.3020766973495483
148 1.4386297464370728
149 1.1985154151916504
150 1.1988188028335571
151 1.3314566612243652
152 1.3237152099609375
153 1.2492972612380981
154 1.3481930494308472
155 1.3826838731765747
156 1.4149879217147827
157 1.1952420473098755
158 1.3973060846328735
159 1.2035722732543945
160 1.3397772312164307
161 1.1473506689071655
162 1.1204807758331299
163 1.140154242515564
164 1.236517071723938
165 1.1828875541687012
166 1.1177324056625366
167 1.249964714050293
168 1.2016688585281372
169 1.1885684728622437
170 1.3328121900558472
171 1.198392629623413
172 1.234220266342163
173 1.1728066205978394
174 1.1376125812530518
175 1.131629228591919
176 1.3062684535980225
177 1.4261107444763184
178 1.3466624021530151
179 1.3358008861541748
180 1.231136441230774
181 1.1855725049972534
182 1.2281569242477417
183 1.331598162651062
184 1.235547661781311
185 1.1835182905197144
186 1.1197710037231445
187 1.2169585227966309
188 1.2485606670379639
189 1.3161276578903198
190 1.265116810798645
191 1.277154803276062
192 1.387711763381958
193 1.3692840337753296
194 1.304274320602417
195 1.3069732189178467
196 1.0801784992218018
197 1.274299144744873
198 1.1909300088882446
199 1.3406143188476562
.............
28.142032196745276 73.66023650951684 26.622297890484333
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3398326635360718
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.339833
1 1.4091399908065796
2 1.332089900970459
3 1.249791145324707
4 1.2343292236328125
5 1.4787832498550415
6 1.0882534980773926
7 1.3347114324569702
8 1.1855292320251465
9 1.3546212911605835
10 1.072743535041809
11 1.2633143663406372
12 1.174201488494873
13 1.1961404085159302
14 1.2130126953125
15 1.2769639492034912
16 1.267669439315796
17 1.264113426208496
18 1.3446590900421143
19 1.1932053565979004
20 1.3589602708816528
21 1.2786678075790405
22 1.1876505613327026
23 1.3582634925842285
24 1.3118826150894165
25 1.1603745222091675
26 1.3355299234390259
27 1.269506812095642
28 1.1642186641693115
29 1.1419867277145386
30 1.1589696407318115
31 1.2330782413482666
32 1.2135083675384521
33 1.2495840787887573
34 1.1804927587509155
35 1.3117178678512573
36 1.1423872709274292
37 1.1712276935577393
38 1.1381665468215942
39 1.3094547986984253
40 1.0782361030578613
41 1.2762479782104492
42 1.3838510513305664
43 1.0931259393692017
44 1.089827537536621
45 1.2014832496643066
46 1.338303804397583
47 1.200799584388733
48 1.4406956434249878
49 1.2897038459777832
50 1.3069512844085693
51 1.2001667022705078
52 1.2632548809051514
53 1.1969995498657227
54 1.0310921669006348
55 1.0935908555984497
56 1.3034765720367432
57 1.1885253190994263
58 1.2492897510528564
59 1.2685481309890747
60 1.297133207321167
61 1.1922900676727295
62 1.30217707157135
63 1.1803829669952393
64 1.1799904108047485
65 1.1796095371246338
66 1.1429632902145386
67 1.3910009860992432
68 1.1130789518356323
69 1.2596385478973389
70 1.2243962287902832
71 1.4333540201187134
72 1.200373888015747
73 1.1163135766983032
74 1.2356899976730347
75 1.2465320825576782
76 1.1872957944869995
77 1.0238378047943115
78 1.2555242776870728
79 1.2827893495559692
80 1.2957489490509033
81 1.2464085817337036
82 1.2631198167800903
83 1.2438124418258667
84 0.9849119782447815
85 1.2830244302749634
86 1.4056614637374878
87 1.191188097000122
88 1.040408968925476
89 1.2198127508163452
90 1.3456737995147705
91 1.421865701675415
92 1.3485252857208252
93 1.2427988052368164
94 1.2948882579803467
95 1.1115931272506714
96 1.160876750946045
97 1.3954522609710693
98 1.2070488929748535
99 1.2460733652114868
100 1.1197078227996826
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.119708
101 1.3232247829437256
102 1.084947109222412
103 1.1060094833374023
104 1.2710611820220947
105 1.2426152229309082
106 1.1842492818832397
107 1.2470710277557373
108 1.1361676454544067
109 1.3522591590881348
110 1.2616207599639893
111 1.1704676151275635
112 1.3379807472229004
113 1.2364084720611572
114 1.1281073093414307
115 1.2953068017959595
116 1.214731216430664
117 1.2320698499679565
118 1.1978061199188232
119 1.1077858209609985
120 1.302152395248413
121 1.3018172979354858
122 1.2383843660354614
123 1.380116581916809
124 1.1317044496536255
125 1.2168220281600952
126 1.2310210466384888
127 1.315003514289856
128 1.0020318031311035
129 1.2890725135803223
130 1.1187692880630493
131 1.1871428489685059
132 1.1512082815170288
133 1.111393690109253
134 1.1082531213760376
135 1.0418392419815063
136 1.1568790674209595
137 0.9937702417373657
138 1.3093403577804565
139 1.0262951850891113
140 1.1311132907867432
141 1.2447710037231445
142 1.2718162536621094
143 1.2224934101104736
144 1.192150354385376
145 1.1781693696975708
146 1.2441822290420532
147 1.1390442848205566
148 1.3104300498962402
149 1.0276378393173218
150 1.091616153717041
151 1.1812494993209839
152 1.2099634408950806
153 1.127392292022705
154 1.1709741353988647
155 1.18423593044281
156 1.35490083694458
157 1.1132084131240845
158 1.2438714504241943
159 1.0995171070098877
160 1.2254034280776978
161 1.0339384078979492
162 0.9709514379501343
163 1.0430961847305298
164 1.1359467506408691
165 1.1022511720657349
166 1.0139679908752441
167 1.1213423013687134
168 1.073142647743225
169 1.0325140953063965
170 1.2390307188034058
171 1.1195193529129028
172 1.0869392156600952
173 1.0877768993377686
174 1.0342274904251099
175 1.0714489221572876
176 1.2037092447280884
177 1.2874782085418701
178 1.285464882850647
179 1.2759957313537598
180 1.1299515962600708
181 1.0658669471740723
182 1.1464003324508667
183 1.2080341577529907
184 1.0327485799789429
185 1.0706640481948853
186 0.9996318817138672
187 1.1410634517669678
188 1.1523152589797974
189 1.198388934135437
190 1.1639580726623535
191 1.0994162559509277
192 1.1850463151931763
193 1.2869738340377808
194 1.296299934387207
195 1.055776596069336
196 0.9718043804168701
197 1.161615014076233
198 1.0506644248962402
199 1.204439640045166
.............
83.741958392784 220.46345725655556 79.16012991592288
.............
0 1.1904168128967285
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.190417
1 1.3238933086395264
2 1.2574867010116577
3 1.1559183597564697
4 1.1404398679733276
5 1.3902572393417358
6 0.9259903430938721
7 1.2311691045761108
8 1.0501304864883423
9 1.2852027416229248
10 0.9745296239852905
11 1.1443287134170532
12 1.1142910718917847
13 1.0893616676330566
14 1.1591509580612183
15 1.12458074092865
16 1.1490329504013062
17 1.0332621335983276
18 1.2815274000167847
19 1.1152997016906738
20 1.2431070804595947
21 1.1537063121795654
22 1.036935806274414
23 1.2383122444152832
24 1.1638555526733398
25 1.0588915348052979
26 1.222852349281311
27 1.1527791023254395
28 1.0902862548828125
29 1.0774011611938477
30 1.0571633577346802
31 1.0831575393676758
32 1.0707279443740845
33 1.1448098421096802
34 1.1104482412338257
35 1.2308217287063599
36 0.9980698227882385
37 1.0966782569885254
38 1.0366926193237305
39 1.2135682106018066
40 1.0235753059387207
41 1.1720787286758423
42 1.336208462715149
43 1.0043282508850098
44 0.986498236656189
45 1.1262133121490479
46 1.3027321100234985
47 1.1613215208053589
48 1.3712397813796997
49 1.2082027196884155
50 1.2474932670593262
51 1.1702326536178589
52 1.1565378904342651
53 1.1009955406188965
54 0.9364533424377441
55 1.0607925653457642
56 1.2623333930969238
57 1.1164183616638184
58 1.1751445531845093
59 1.2246768474578857
60 1.2167412042617798
61 1.1580424308776855
62 1.2071281671524048
63 1.0050522089004517
64 1.0867689847946167
65 1.1303389072418213
66 1.0034462213516235
67 1.1936951875686646
68 1.038622260093689
69 1.151198148727417
70 1.0423591136932373
71 1.3051549196243286
72 1.1108702421188354
73 1.0360357761383057
74 1.1894954442977905
75 1.0858302116394043
76 1.0169894695281982
77 0.9530028700828552
78 1.1863903999328613
79 1.1385083198547363
80 1.1332331895828247
81 1.109451174736023
82 1.1835472583770752
83 1.045578122138977
84 0.9456402063369751
85 1.1843854188919067
86 1.278177261352539
87 1.0388360023498535
88 0.9255940914154053
89 1.097288727760315
90 1.2634769678115845
91 1.3717540502548218
92 1.2844139337539673
93 1.1248207092285156
94 1.1771050691604614
95 1.0123871564865112
96 1.109438419342041
97 1.3536186218261719
98 1.119227409362793
99 1.1943470239639282
100 1.0078742504119873
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.007874
101 1.2493504285812378
102 1.081498146057129
103 1.0716311931610107
104 1.1760109663009644
105 1.1620161533355713
106 1.156114935874939
107 1.1502280235290527
108 1.0153216123580933
109 1.3024532794952393
110 1.1510566473007202
111 1.067020058631897
112 1.1819840669631958
113 1.2273958921432495
114 1.0071390867233276
115 1.1875495910644531
116 1.2179945707321167
117 1.190048336982727
118 1.001658320426941
119 1.0112359523773193
120 1.2363587617874146
121 1.2793999910354614
122 1.112297534942627
123 1.2824441194534302
124 1.0594754219055176
125 1.1961148977279663
126 1.222230315208435
127 1.234075903892517
128 0.9871510863304138
129 1.187187910079956
130 1.102529525756836
131 1.0762778520584106
132 1.0557608604431152
133 1.0095953941345215
134 1.0310450792312622
135 1.0018081665039062
136 1.1168612241744995
137 0.9096181988716125
138 1.2749420404434204
139 1.076717495918274
140 1.0522273778915405
141 1.2197338342666626
142 1.2015302181243896
143 1.1501402854919434
144 1.1770656108856201
145 1.1326240301132202
146 1.2111366987228394
147 1.0470222234725952
148 1.2166646718978882
149 0.9380765557289124
150 1.1057302951812744
151 1.1188939809799194
152 1.153793454170227
153 1.0993000268936157
154 1.1422066688537598
155 1.1267554759979248
156 1.3148647546768188
157 1.0513173341751099
158 1.152932047843933
159 1.0801246166229248
160 1.1669776439666748
161 0.9561991691589355
162 0.8968358039855957
163 0.9584367871284485
164 1.0426419973373413
165 1.0756328105926514
166 0.9478509426116943
167 1.0587501525878906
168 0.9706297516822815
169 0.9799895286560059
170 1.1630367040634155
171 1.0147274732589722
172 1.011669397354126
173 1.0162715911865234
174 0.9886049032211304
175 0.9616453051567078
176 1.1504548788070679
177 1.2105766534805298
178 1.2095208168029785
179 1.1634223461151123
180 1.063639760017395
181 1.0062878131866455
182 1.1022367477416992
183 1.1186747550964355
184 0.984312117099762
185 1.0204975605010986
186 0.9294248819351196
187 1.0721545219421387
188 1.0627182722091675
189 1.1455225944519043
190 1.0709418058395386
191 1.032463788986206
192 1.1395295858383179
193 1.2456673383712769
194 1.2179666757583618
195 0.9646263122558594
196 0.9224005341529846
197 1.0839409828186035
198 1.01072359085083
199 1.162109375
.............
195.16405462659895 514.7173333298415 185.11032707057893
.............
0 1.1141570806503296
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.114157
1 1.2836105823516846
2 1.2048346996307373
3 1.1299952268600464
4 1.0892179012298584
5 1.3353970050811768
6 0.8996286988258362
7 1.1672664880752563
8 0.9873540997505188
9 1.208064317703247
10 0.9144824147224426
11 1.0640047788619995
12 1.0470370054244995
13 1.0086349248886108
14 1.139273762702942
15 1.089347004890442
16 1.0811823606491089
17 0.9475471377372742
18 1.2752715349197388
19 1.05329167842865
20 1.195829153060913
21 1.1012121438980103
22 0.9665735363960266
23 1.1794465780258179
24 1.0975090265274048
25 1.0070457458496094
26 1.1996526718139648
27 1.079957127571106
28 1.090828537940979
29 1.0633208751678467
30 1.0215600728988647
31 1.088473916053772
32 1.030619740486145
33 1.1121108531951904
34 1.0657131671905518
35 1.1848067045211792
36 1.0227278470993042
37 1.0890827178955078
38 1.0107616186141968
39 1.1489373445510864
40 0.9600404500961304
41 1.1425226926803589
42 1.2965282201766968
43 0.8993715643882751
44 0.9515315294265747
45 1.0922218561172485
46 1.2241171598434448
47 1.0992302894592285
48 1.280733585357666
49 1.1373341083526611
50 1.1410466432571411
51 1.076577067375183
52 1.1714099645614624
53 1.014264702796936
54 0.902534544467926
55 1.023091197013855
56 1.1425548791885376
57 1.0503928661346436
58 1.127401351928711
59 1.1704285144805908
60 1.179444670677185
61 1.1153504848480225
62 1.1599220037460327
63 1.0116976499557495
64 1.0884768962860107
65 1.084168553352356
66 0.9599718451499939
67 1.2070186138153076
68 1.028205394744873
69 1.1212304830551147
70 1.029149055480957
71 1.2828351259231567
72 1.1038882732391357
73 1.0627198219299316
74 1.158769130706787
75 1.0758943557739258
76 0.9858354926109314
77 0.9495628476142883
78 1.193579912185669
79 1.10771644115448
80 1.0643417835235596
81 1.101853370666504
82 1.1556813716888428
83 0.9851149916648865
84 0.9590065479278564
85 1.1478564739227295
86 1.2783509492874146
87 1.0057655572891235
88 0.9063000679016113
89 1.0755542516708374
90 1.2607133388519287
91 1.3640080690383911
92 1.265621304512024
93 1.129349946975708
94 1.169520378112793
95 0.9879397749900818
96 1.127132773399353
97 1.2908412218093872
98 1.0893900394439697
99 1.1578627824783325
100 0.9748440384864807
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.974844
101 1.2189412117004395
102 1.0800235271453857
103 1.0854833126068115
104 1.1356950998306274
105 1.1163173913955688
106 1.1000255346298218
107 1.1191846132278442
108 0.9965324401855469
109 1.2855002880096436
110 1.1013691425323486
111 1.0345567464828491
112 1.134344220161438
113 1.1842910051345825
114 0.9442083239555359
115 1.1622473001480103
116 1.1678764820098877
117 1.1331313848495483
118 0.9715957045555115
119 0.9850581288337708
120 1.18876314163208
121 1.2477045059204102
122 1.0614451169967651
123 1.2366549968719482
124 1.0365428924560547
125 1.1932666301727295
126 1.2097688913345337
127 1.2146259546279907
128 0.9507076740264893
129 1.1450905799865723
130 1.1019489765167236
131 1.0363060235977173
132 1.0181454420089722
133 0.975109875202179
134 1.0168330669403076
135 0.9738019704818726
136 1.116844892501831
137 0.8742734789848328
138 1.236055850982666
139 1.081847071647644
140 1.024695873260498
141 1.2311862707138062
142 1.1724623441696167
143 1.115017294883728
144 1.1682904958724976
145 1.1203224658966064
146 1.190610647201538
147 0.9987228512763977
148 1.1869971752166748
149 0.9009944796562195
150 1.097985029220581
151 1.1341525316238403
152 1.129370927810669
153 1.074218988418579
154 1.1081628799438477
155 1.0892133712768555
156 1.2949572801589966
157 1.0504047870635986
158 1.1296725273132324
159 1.090061068534851
160 1.175717830657959
161 0.9521227478981018
162 0.9049221277236938
163 0.9863826036453247
164 1.0292401313781738
165 1.0548607110977173
166 0.9444673657417297
167 1.029240369796753
168 0.9636619091033936
169 0.9661996364593506
170 1.1549873352050781
171 0.9905542135238647
172 0.9991536736488342
173 0.9837234616279602
174 0.9831452965736389
175 0.9434272646903992
176 1.1457648277282715
177 1.1886223554611206
178 1.1809799671173096
179 1.149799108505249
180 1.0412096977233887
181 0.9918991327285767
182 1.0754189491271973
183 1.0891109704971313
184 0.964652419090271
185 1.0141092538833618
186 0.9095708727836609
187 1.0546306371688843
188 1.0417556762695312
189 1.1097157001495361
190 1.042639136314392
191 1.0062319040298462
192 1.0979005098342896
193 1.1965173482894897
194 1.1743663549423218
195 0.928677499294281
196 0.9064650535583496
197 1.0451405048370361
198 0.9800564050674438
199 1.1271356344223022
.............
416.8481323234737 1102.8097727354616 395.75848856009543
.............
0 1.0806500911712646
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.080650
1 1.2709312438964844
2 1.1857972145080566
3 1.103400707244873
4 1.0528771877288818
5 1.2935904264450073
6 0.8799988031387329
7 1.136495590209961
8 0.9644709825515747
9 1.181193232536316
10 0.9019761085510254
11 1.032271146774292
12 1.030473232269287
13 0.9949638843536377
14 1.1256475448608398
15 1.0657466650009155
16 1.0444879531860352
17 0.9136558771133423
18 1.2608401775360107
19 1.0166041851043701
20 1.1710413694381714
21 1.0710629224777222
22 0.9282829165458679
23 1.147888422012329
24 1.0802559852600098
25 0.9767848253250122
26 1.1860325336456299
27 1.0378789901733398
28 1.0556652545928955
29 1.0466525554656982
30 0.9666469097137451
31 1.0284045934677124
32 0.9963071942329407
33 1.0676954984664917
34 1.030653715133667
35 1.163026213645935
36 0.9703600406646729
37 1.0615791082382202
38 0.9844774603843689
39 1.1771628856658936
40 0.9434324502944946
41 1.0773483514785767
42 1.2744472026824951
43 0.8916220664978027
44 0.9250010848045349
45 1.0676299333572388
46 1.2423762083053589
47 1.048557162284851
48 1.2714061737060547
49 1.1128827333450317
50 1.17238187789917
51 1.0816283226013184
52 1.1065924167633057
53 1.0183401107788086
54 0.8947275280952454
55 0.9911821484565735
56 1.115405559539795
57 1.0420329570770264
58 1.1103113889694214
59 1.1405913829803467
60 1.14085054397583
61 1.1102824211120605
62 1.1440651416778564
63 0.9619140625
64 1.0410228967666626
65 1.041466474533081
66 0.9359668493270874
67 1.1633903980255127
68 1.0080353021621704
69 1.0829945802688599
70 0.9806087017059326
71 1.230262279510498
72 1.080654501914978
73 1.0255444049835205
74 1.129639983177185
75 1.0419191122055054
76 0.969521701335907
77 0.9218229651451111
78 1.1510275602340698
79 1.0668405294418335
80 1.0383760929107666
81 1.0666441917419434
82 1.1313673257827759
83 0.9653023481369019
84 0.9066193103790283
85 1.1187973022460938
86 1.2228422164916992
87 0.9705557823181152
88 0.8955240845680237
89 1.0457817316055298
90 1.2342456579208374
91 1.3361108303070068
92 1.253217339515686
93 1.0924005508422852
94 1.135454535484314
95 0.9437625408172607
96 1.0724217891693115
97 1.2075790166854858
98 1.0703845024108887
99 1.1120306253433228
100 0.9301671385765076
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.930167
101 1.1766693592071533
102 1.0187695026397705
103 1.0426946878433228
104 1.108093500137329
105 1.0800368785858154
106 1.0496174097061157
107 1.0978370904922485
108 0.978943943977356
109 1.2591300010681152
110 1.0799736976623535
111 1.0136219263076782
112 1.0774019956588745
113 1.185522437095642
114 0.9399930834770203
115 1.1275007724761963
116 1.1257007122039795
117 1.087164044380188
118 0.9654698371887207
119 0.9496966004371643
120 1.1274173259735107
121 1.2488479614257812
122 1.0171842575073242
123 1.1479343175888062
124 1.0336041450500488
125 1.163042426109314
126 1.1636601686477661
127 1.1596157550811768
128 0.9533060193061829
129 1.0816494226455688
130 1.0696492195129395
131 1.0226101875305176
132 0.9528428912162781
133 0.930417001247406
134 0.9918119311332703
135 0.9554026126861572
136 1.0901885032653809
137 0.8462055921554565
138 1.1949704885482788
139 1.0142310857772827
140 0.9806153774261475
141 1.2098948955535889
142 1.1410741806030273
143 1.066900610923767
144 1.1095603704452515
145 1.086517572402954
146 1.1670385599136353
147 0.9725295901298523
148 1.1718661785125732
149 0.8745700120925903
150 1.0418248176574707
151 1.1230616569519043
152 1.083796501159668
153 1.0454744100570679
154 1.0762757062911987
155 1.023040771484375
156 1.2735531330108643
157 1.04703688621521
158 1.0607550144195557
159 1.0545735359191895
160 1.1562409400939941
161 0.9413657188415527
162 0.8872169256210327
163 1.020485520362854
164 1.0287771224975586
165 1.0090235471725464
166 0.9445606470108032
167 1.009939193725586
168 0.9420392513275146
169 0.9426931738853455
170 1.1219133138656616
171 0.9736273288726807
172 0.9874216318130493
173 0.9391266703605652
174 0.9330224394798279
175 0.9065998196601868
176 1.114805817604065
177 1.1718837022781372
178 1.171981930732727
179 1.1108628511428833
180 1.0225117206573486
181 0.9534745216369629
182 1.0723991394042969
183 1.0563727617263794
184 0.9542497992515564
185 0.9836286902427673
186 0.8855840563774109
187 1.0123260021209717
188 1.0093847513198853
189 1.0780692100524902
190 1.001626968383789
191 0.9790342450141907
192 1.0454713106155396
193 1.160752296447754
194 1.1380536556243896
195 0.9007454514503479
196 0.8851220607757568
197 0.9925522804260254
198 0.9516953825950623
199 1.082168698310852
.............
860.3701880984008 2277.9590607769787 816.5354732517153
.............
3 860.3701880984008 2277.9590607769787 816.5354732517153
.............
