0 2.8148043155670166
Train Epoch: 1 [0/20000 (0%)]	Loss: 2.814804
1 2.7961068153381348
2 2.7586183547973633
3 2.70381236076355
4 2.656569242477417
5 2.58831787109375
6 2.4571027755737305
7 2.411665439605713
8 2.268389940261841
9 2.173398017883301
10 1.9821853637695312
11 1.8721462488174438
12 1.752858281135559
13 1.9756931066513062
14 1.870060682296753
15 1.9258671998977661
16 1.7932161092758179
17 1.9266700744628906
18 1.72759211063385
19 1.829622745513916
20 1.9751988649368286
21 1.8146476745605469
22 1.5162129402160645
23 1.8079369068145752
24 1.6630090475082397
25 1.77516770362854
26 1.8557478189468384
27 1.776034951210022
28 1.4880043268203735
29 1.6628832817077637
30 1.6473332643508911
31 1.6786515712738037
32 1.6340606212615967
33 1.6060400009155273
34 1.637221097946167
35 1.735978603363037
36 1.5346119403839111
37 1.62534761428833
38 1.5378108024597168
39 1.5640612840652466
40 1.5671272277832031
41 1.6168524026870728
42 1.6510471105575562
43 1.5306100845336914
44 1.4200419187545776
45 1.472131371498108
46 1.7105813026428223
47 1.6022874116897583
48 1.6743578910827637
49 1.6019457578659058
50 1.5198222398757935
51 1.5249782800674438
52 1.5745196342468262
53 1.487019658088684
54 1.286207675933838
55 1.5592024326324463
56 1.766076922416687
57 1.5592607259750366
58 1.460720181465149
59 1.6137876510620117
60 1.6670666933059692
61 1.6049972772598267
62 1.5552607774734497
63 1.5458699464797974
64 1.6186848878860474
65 1.5837337970733643
66 1.4563727378845215
67 1.527244210243225
68 1.5004273653030396
69 1.5636849403381348
70 1.4195441007614136
71 1.6260584592819214
72 1.4276249408721924
73 1.4193329811096191
74 1.3943257331848145
75 1.4026997089385986
76 1.3975564241409302
77 1.31846284866333
78 1.481730341911316
79 1.509310007095337
80 1.367679238319397
81 1.4323254823684692
82 1.5255515575408936
83 1.470499873161316
84 1.2438011169433594
85 1.4969369173049927
86 1.6221259832382202
87 1.4525409936904907
88 1.3121386766433716
89 1.4203550815582275
90 1.534241795539856
91 1.616847276687622
92 1.587094783782959
93 1.4599404335021973
94 1.4537663459777832
95 1.3204803466796875
96 1.4009724855422974
97 1.6216622591018677
98 1.5828299522399902
99 1.3995847702026367
100 1.350636601448059
Train Epoch: 1 [10000/20000 (50%)]	Loss: 1.350637
101 1.505794882774353
102 1.3675616979599
103 1.3056761026382446
104 1.4786959886550903
105 1.5434015989303589
106 1.3985116481781006
107 1.456346035003662
108 1.3786181211471558
109 1.5594717264175415
110 1.456021785736084
111 1.350257158279419
112 1.5573877096176147
113 1.4749053716659546
114 1.2002352476119995
115 1.4003866910934448
116 1.3499455451965332
117 1.415969967842102
118 1.2634735107421875
119 1.2584110498428345
120 1.4790557622909546
121 1.4080052375793457
122 1.3424955606460571
123 1.4903401136398315
124 1.2480456829071045
125 1.3359395265579224
126 1.3229693174362183
127 1.423506259918213
128 1.1496195793151855
129 1.4173591136932373
130 1.2614823579788208
131 1.3183940649032593
132 1.290879487991333
133 1.2935278415679932
134 1.317762851715088
135 1.213143229484558
136 1.2406511306762695
137 1.137211799621582
138 1.40513277053833
139 1.124946117401123
140 1.3096098899841309
141 1.3646122217178345
142 1.4232968091964722
143 1.311768651008606
144 1.2864488363265991
145 1.3109949827194214
146 1.3206294775009155
147 1.2863065004348755
148 1.4320684671401978
149 1.1920133829116821
150 1.1953099966049194
151 1.3136625289916992
152 1.3164669275283813
153 1.244253158569336
154 1.3370965719223022
155 1.3695847988128662
156 1.414950966835022
157 1.1869415044784546
158 1.3904222249984741
159 1.1971957683563232
160 1.3370987176895142
161 1.1423873901367188
162 1.1115448474884033
163 1.132370948791504
164 1.2323588132858276
165 1.1768231391906738
166 1.1124016046524048
167 1.2478214502334595
168 1.2035349607467651
169 1.1793209314346313
170 1.3310884237289429
171 1.1955246925354004
172 1.221988558769226
173 1.177130937576294
174 1.1457496881484985
175 1.1242923736572266
176 1.3000308275222778
177 1.4239588975906372
178 1.3470335006713867
179 1.3385190963745117
180 1.2269073724746704
181 1.1890451908111572
182 1.223483681678772
183 1.3246198892593384
184 1.231243371963501
185 1.1860371828079224
186 1.1112964153289795
187 1.2163337469100952
188 1.2511886358261108
189 1.3147393465042114
190 1.2751410007476807
191 1.271907091140747
192 1.3795957565307617
193 1.3551580905914307
194 1.3018299341201782
195 1.2985502481460571
196 1.072892427444458
197 1.2771856784820557
198 1.1847683191299438
199 1.3395988941192627
.............
196.88504621200264 54.69040075875819 40.14947872236371
.............
./lab1.pytorch:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  output = F.log_softmax(self.linear3(z))
0 1.3360737562179565
Train Epoch: 2 [0/20000 (0%)]	Loss: 1.336074
1 1.4029557704925537
2 1.3343112468719482
3 1.2488707304000854
4 1.232439637184143
5 1.4801517724990845
6 1.0889586210250854
7 1.3319580554962158
8 1.1816388368606567
9 1.3484822511672974
10 1.0700372457504272
11 1.2525513172149658
12 1.1623563766479492
13 1.1902657747268677
14 1.2018810510635376
15 1.2576547861099243
16 1.2593404054641724
17 1.2484766244888306
18 1.341802716255188
19 1.1989802122116089
20 1.368942141532898
21 1.2741048336029053
22 1.180856704711914
23 1.3525230884552002
24 1.3028874397277832
25 1.1621429920196533
26 1.327986240386963
27 1.2898870706558228
28 1.160782814025879
29 1.1451219320297241
30 1.1655855178833008
31 1.2526428699493408
32 1.1946264505386353
33 1.244735598564148
34 1.1912760734558105
35 1.3180415630340576
36 1.1410921812057495
37 1.1722222566604614
38 1.1427537202835083
39 1.3094267845153809
40 1.072756290435791
41 1.2762342691421509
42 1.3835780620574951
43 1.0835752487182617
44 1.0826349258422852
45 1.2022640705108643
46 1.336031436920166
47 1.1979131698608398
48 1.4284902811050415
49 1.2938593626022339
50 1.307714581489563
51 1.202279806137085
52 1.2641465663909912
53 1.197176218032837
54 1.020999550819397
55 1.0934449434280396
56 1.3005499839782715
57 1.18637216091156
58 1.2401387691497803
59 1.2750236988067627
60 1.2994587421417236
61 1.1915839910507202
62 1.2990015745162964
63 1.1838382482528687
64 1.1767096519470215
65 1.1702300310134888
66 1.1449341773986816
67 1.4000767469406128
68 1.1056852340698242
69 1.2608696222305298
70 1.2337535619735718
71 1.4317699670791626
72 1.192564845085144
73 1.105135440826416
74 1.2405186891555786
75 1.2459241151809692
76 1.158655047416687
77 1.0082374811172485
78 1.2673475742340088
79 1.2888920307159424
80 1.2852815389633179
81 1.2185078859329224
82 1.2728943824768066
83 1.2564623355865479
84 0.9706143140792847
85 1.2635208368301392
86 1.3898766040802002
87 1.1866835355758667
88 1.0282820463180542
89 1.1916849613189697
90 1.333398461341858
91 1.4253051280975342
92 1.3423489332199097
93 1.2377961874008179
94 1.2913204431533813
95 1.1071032285690308
96 1.1722915172576904
97 1.3734867572784424
98 1.2028995752334595
99 1.2398735284805298
100 1.11289644241333
Train Epoch: 2 [10000/20000 (50%)]	Loss: 1.112896
101 1.3154189586639404
102 1.0802338123321533
103 1.0980993509292603
104 1.2578692436218262
105 1.223564624786377
106 1.1700453758239746
107 1.2269889116287231
108 1.114548683166504
109 1.3504486083984375
110 1.2517269849777222
111 1.1485273838043213
112 1.311276912689209
113 1.2308241128921509
114 1.1142340898513794
115 1.2791286706924438
116 1.204581379890442
117 1.229871153831482
118 1.1813592910766602
119 1.0865106582641602
120 1.2926359176635742
121 1.3059072494506836
122 1.225826382637024
123 1.3752162456512451
124 1.1291937828063965
125 1.2100447416305542
126 1.235851764678955
127 1.2941765785217285
128 1.0063318014144897
129 1.2785842418670654
130 1.1096059083938599
131 1.1825201511383057
132 1.1304734945297241
133 1.0860940217971802
134 1.1241824626922607
135 1.0367140769958496
136 1.156611442565918
137 0.9874730706214905
138 1.2897719144821167
139 1.0499413013458252
140 1.113446593284607
141 1.229454755783081
142 1.2677704095840454
143 1.1967313289642334
144 1.180047631263733
145 1.164432406425476
146 1.225784420967102
147 1.125152349472046
148 1.2960394620895386
149 1.005600094795227
150 1.1017495393753052
151 1.1698215007781982
152 1.2029595375061035
153 1.1155972480773926
154 1.169929027557373
155 1.1716469526290894
156 1.347487211227417
157 1.0984492301940918
158 1.241004228591919
159 1.1028430461883545
160 1.2124539613723755
161 1.0189319849014282
162 0.9574854373931885
163 1.038291335105896
164 1.1253807544708252
165 1.0865676403045654
166 0.9973689317703247
167 1.116256833076477
168 1.0580769777297974
169 1.0095173120498657
170 1.2018914222717285
171 1.0815491676330566
172 1.0722728967666626
173 1.0865799188613892
174 1.0225080251693726
175 1.0301399230957031
176 1.1757389307022095
177 1.2810451984405518
178 1.2751027345657349
179 1.2499401569366455
180 1.1056511402130127
181 1.0492440462112427
182 1.1378883123397827
183 1.1751712560653687
184 1.0187840461730957
185 1.0624268054962158
186 0.990497350692749
187 1.1194660663604736
188 1.1196675300598145
189 1.174754023551941
190 1.1245245933532715
191 1.089410424232483
192 1.190222144126892
193 1.2917296886444092
194 1.2707210779190063
195 1.0277131795883179
196 0.9579688906669617
197 1.1617587804794312
198 1.051613688468933
199 1.192610502243042
.............
438.7745131775737 214.90385952964425 100.45125078782439
.............
0 1.1729615926742554
Train Epoch: 3 [0/20000 (0%)]	Loss: 1.172962
1 1.3062530755996704
2 1.2551246881484985
3 1.1487607955932617
4 1.1523514986038208
5 1.3752202987670898
6 0.9196302890777588
7 1.2009556293487549
8 1.0215564966201782
9 1.2532339096069336
10 0.9499655365943909
11 1.125370740890503
12 1.0914567708969116
13 1.0527081489562988
14 1.139255166053772
15 1.1408082246780396
16 1.1437188386917114
17 1.0041698217391968
18 1.2821203470230103
19 1.1084555387496948
20 1.253255009651184
21 1.132175087928772
22 1.0206749439239502
23 1.2252171039581299
24 1.133460283279419
25 1.062076449394226
26 1.2203874588012695
27 1.139418601989746
28 1.0917667150497437
29 1.067122459411621
30 1.0513912439346313
31 1.0946160554885864
32 1.0651119947433472
33 1.1404576301574707
34 1.0913318395614624
35 1.2104439735412598
36 1.003973126411438
37 1.096472144126892
38 1.0285120010375977
39 1.1720489263534546
40 0.9795846343040466
41 1.1760365962982178
42 1.3334970474243164
43 0.9370896816253662
44 0.9636594653129578
45 1.1144688129425049
46 1.248275876045227
47 1.1317700147628784
48 1.3092750310897827
49 1.2128077745437622
50 1.1697317361831665
51 1.1048781871795654
52 1.2384618520736694
53 1.0354843139648438
54 0.9144603610038757
55 1.0582280158996582
56 1.2033034563064575
57 1.0882930755615234
58 1.1465084552764893
59 1.2114845514297485
60 1.242730736732483
61 1.1394141912460327
62 1.1893757581710815
63 1.058139681816101
64 1.1266257762908936
65 1.1466846466064453
66 1.0011459589004517
67 1.2196897268295288
68 1.0552892684936523
69 1.1738886833190918
70 1.0878362655639648
71 1.3119902610778809
72 1.1084884405136108
73 1.0714373588562012
74 1.2000097036361694
75 1.0959315299987793
76 0.9927214980125427
77 0.9489866495132446
78 1.2002719640731812
79 1.1340904235839844
80 1.102463960647583
81 1.1011385917663574
82 1.170322060585022
83 1.0152651071548462
84 0.9885662794113159
85 1.1623610258102417
86 1.3259130716323853
87 1.0397214889526367
88 0.9113336205482483
89 1.1047160625457764
90 1.2758870124816895
91 1.376094102859497
92 1.2738909721374512
93 1.1565985679626465
94 1.1927766799926758
95 1.0287338495254517
96 1.158848524093628
97 1.3413472175598145
98 1.1279243230819702
99 1.1930439472198486
100 1.0084148645401
Train Epoch: 3 [10000/20000 (50%)]	Loss: 1.008415
101 1.2542670965194702
102 1.0976539850234985
103 1.1079117059707642
104 1.169137954711914
105 1.1423449516296387
106 1.1288999319076538
107 1.1522976160049438
108 0.996700644493103
109 1.313439130783081
110 1.1333746910095215
111 1.0460011959075928
112 1.1760132312774658
113 1.2088792324066162
114 0.964377760887146
115 1.1897467374801636
116 1.1935893297195435
117 1.1726067066192627
118 0.9778556823730469
119 1.0192432403564453
120 1.2372092008590698
121 1.2365607023239136
122 1.1103745698928833
123 1.296190619468689
124 1.0949366092681885
125 1.211190938949585
126 1.2143691778182983
127 1.2438493967056274
128 0.945939302444458
129 1.2236911058425903
130 1.107799768447876
131 1.0899779796600342
132 1.1112051010131836
133 1.035808801651001
134 1.0558871030807495
135 0.9867174029350281
136 1.1298930644989014
137 0.9092616438865662
138 1.2621608972549438
139 1.1376363039016724
140 1.0650572776794434
141 1.234307050704956
142 1.2099525928497314
143 1.1571235656738281
144 1.1976670026779175
145 1.134778380393982
146 1.1951556205749512
147 1.0422630310058594
148 1.2152376174926758
149 0.9423536658287048
150 1.1112124919891357
151 1.1373553276062012
152 1.1594035625457764
153 1.099340796470642
154 1.153878927230835
155 1.1259474754333496
156 1.3187528848648071
157 1.0605326890945435
158 1.1540606021881104
159 1.0846641063690186
160 1.1651045083999634
161 0.9584522843360901
162 0.9079539775848389
163 0.9743210077285767
164 1.054333209991455
165 1.0606485605239868
166 0.9394632577896118
167 1.0592525005340576
168 0.9789995551109314
169 0.9650717973709106
170 1.1718353033065796
171 1.0239487886428833
172 1.005255937576294
173 1.0207302570343018
174 0.9945473670959473
175 0.9828414916992188
176 1.1510272026062012
177 1.2094954252243042
178 1.2294145822525024
179 1.1763415336608887
180 1.059445858001709
181 1.0238734483718872
182 1.0910238027572632
183 1.118210792541504
184 0.9850870370864868
185 1.0220301151275635
186 0.9391842484474182
187 1.074914574623108
188 1.0605382919311523
189 1.1355311870574951
190 1.0725783109664917
191 1.0343983173370361
192 1.1362619400024414
193 1.2414220571517944
194 1.2225857973098755
195 0.9637061953544617
196 0.9197319746017456
197 1.0788052082061768
198 1.0132126808166504
199 1.1623084545135498
.............
924.1223729290068 539.7607074044645 222.45014648139477
.............
0 1.1191198825836182
Train Epoch: 4 [0/20000 (0%)]	Loss: 1.119120
1 1.2777495384216309
2 1.212020754814148
3 1.1283453702926636
4 1.0855896472930908
5 1.3324815034866333
6 0.8929133415222168
7 1.164005994796753
8 0.977762758731842
9 1.2135683298110962
10 0.9109224081039429
11 1.0679125785827637
12 1.0472220182418823
13 1.0074877738952637
14 1.1338108777999878
15 1.1013180017471313
16 1.0865675210952759
17 0.9471633434295654
18 1.2763671875
19 1.0566014051437378
20 1.1983410120010376
21 1.089961290359497
22 0.9672759771347046
23 1.1809176206588745
24 1.1003899574279785
25 1.015549659729004
26 1.1919673681259155
27 1.0829946994781494
28 1.087785005569458
29 1.0592502355575562
30 1.0266869068145752
31 1.0984489917755127
32 1.0263569355010986
33 1.114652156829834
34 1.0631085634231567
35 1.1801950931549072
36 1.0300229787826538
37 1.0972466468811035
38 1.011164903640747
39 1.1476171016693115
40 0.9634029269218445
41 1.1424126625061035
42 1.2962028980255127
43 0.9076924324035645
44 0.9558390974998474
45 1.0948361158370972
46 1.2191245555877686
47 1.099902868270874
48 1.290196418762207
49 1.1492093801498413
50 1.1489131450653076
51 1.074573278427124
52 1.177436351776123
53 1.0168817043304443
54 0.9032970666885376
55 1.0247310400009155
56 1.1388306617736816
57 1.0539528131484985
58 1.1192057132720947
59 1.1624441146850586
60 1.1705999374389648
61 1.1151126623153687
62 1.1551141738891602
63 1.0011154413223267
64 1.0740163326263428
65 1.088537335395813
66 0.9615647196769714
67 1.1951411962509155
68 1.0205578804016113
69 1.11565363407135
70 1.0273066759109497
71 1.2918788194656372
72 1.1009106636047363
73 1.0543299913406372
74 1.1630134582519531
75 1.0848461389541626
76 0.9735655784606934
77 0.9446443319320679
78 1.1916145086288452
79 1.105581283569336
80 1.0655701160430908
81 1.0858263969421387
82 1.1426548957824707
83 0.9826992154121399
84 0.9561777710914612
85 1.1371300220489502
86 1.2845081090927124
87 1.00227689743042
88 0.896348774433136
89 1.0755364894866943
90 1.2556108236312866
91 1.3596729040145874
92 1.2659249305725098
93 1.1334898471832275
94 1.159505844116211
95 0.9995068907737732
96 1.1423959732055664
97 1.2844855785369873
98 1.0925805568695068
99 1.1611926555633545
100 0.9809974431991577
Train Epoch: 4 [10000/20000 (50%)]	Loss: 0.980997
101 1.2187402248382568
102 1.0726523399353027
103 1.1027519702911377
104 1.1425386667251587
105 1.1180726289749146
106 1.0966060161590576
107 1.1233057975769043
108 0.9872375726699829
109 1.2869317531585693
110 1.0956453084945679
111 1.028765082359314
112 1.1382423639297485
113 1.1905180215835571
114 0.9478803873062134
115 1.1550755500793457
116 1.1558640003204346
117 1.1354914903640747
118 0.981031596660614
119 0.9813868999481201
120 1.1950260400772095
121 1.2345967292785645
122 1.0563727617263794
123 1.2535382509231567
124 1.0437012910842896
125 1.183440923690796
126 1.1944442987442017
127 1.2207772731781006
128 0.9376213550567627
129 1.1616193056106567
130 1.0995107889175415
131 1.0432002544403076
132 1.027870535850525
133 0.980742335319519
134 1.0260542631149292
135 0.9726405143737793
136 1.1284847259521484
137 0.878002941608429
138 1.2391037940979004
139 1.1044963598251343
140 1.0361292362213135
141 1.2419787645339966
142 1.177375316619873
143 1.1223835945129395
144 1.1835880279541016
145 1.1243376731872559
146 1.1919010877609253
147 1.002302885055542
148 1.190826177597046
149 0.9109391570091248
150 1.1017366647720337
151 1.1452770233154297
152 1.1505295038223267
153 1.0747321844100952
154 1.1017448902130127
155 1.0992555618286133
156 1.3071306943893433
157 1.0688542127609253
158 1.1381381750106812
159 1.107909083366394
160 1.1825848817825317
161 0.9521685838699341
162 0.9216082692146301
163 1.0012987852096558
164 1.0402787923812866
165 1.0401806831359863
166 0.9371078014373779
167 1.0385682582855225
168 0.9801999926567078
169 0.9579663276672363
170 1.1548049449920654
171 0.9992985725402832
172 0.9823093414306641
173 0.9949661493301392
174 0.9997653365135193
175 0.970879077911377
176 1.158443570137024
177 1.1824272871017456
178 1.2015959024429321
179 1.1700031757354736
180 1.043489694595337
181 1.0023136138916016
182 1.0629770755767822
183 1.0942258834838867
184 0.9686176776885986
185 1.0151447057724
186 0.9222744107246399
187 1.068909764289856
188 1.0427837371826172
189 1.1138838529586792
190 1.0531482696533203
191 1.0110788345336914
192 1.1028414964675903
193 1.2032657861709595
194 1.1853581666946411
195 0.9332858920097351
196 0.9070488214492798
197 1.0465576648712158
198 0.9823095798492432
199 1.1356581449508667
.............
1894.8685434125364 1185.5780600942671 465.00997553020716
.............
0 1.0847746133804321
Train Epoch: 5 [0/20000 (0%)]	Loss: 1.084775
1 1.2737051248550415
2 1.1862319707870483
3 1.1053156852722168
4 1.0525935888290405
5 1.2930893898010254
6 0.8795543909072876
7 1.1344695091247559
8 0.9606495499610901
9 1.1891019344329834
10 0.8975467681884766
11 1.0408527851104736
12 1.0328950881958008
13 0.9993528127670288
14 1.1236188411712646
15 1.070533275604248
16 1.0576907396316528
17 0.916748583316803
18 1.2654775381088257
19 1.0232093334197998
20 1.1778652667999268
21 1.059433937072754
22 0.9365365505218506
23 1.153905987739563
24 1.0827641487121582
25 0.9850142002105713
26 1.1875535249710083
27 1.0457710027694702
28 1.062228798866272
29 1.0472418069839478
30 0.971640408039093
31 1.0430666208267212
32 0.9970473647117615
33 1.0687034130096436
34 1.0280004739761353
35 1.1653913259506226
36 0.9736358523368835
37 1.0723837614059448
38 0.9924210906028748
39 1.1748675107955933
40 0.9394258260726929
41 1.094435691833496
42 1.2697769403457642
43 0.8897117376327515
44 0.9368136525154114
45 1.0766130685806274
46 1.2281467914581299
47 1.0512622594833374
48 1.273128628730774
49 1.1243230104446411
50 1.1756579875946045
51 1.076820731163025
52 1.128136157989502
53 1.0182305574417114
54 0.902353823184967
55 0.9985100626945496
56 1.1081488132476807
57 1.0455119609832764
58 1.1165310144424438
59 1.135508418083191
60 1.1368262767791748
61 1.1065772771835327
62 1.1407132148742676
63 0.9611884951591492
64 1.039135456085205
65 1.0527974367141724
66 0.9381492137908936
67 1.168349027633667
68 1.009969711303711
69 1.0743827819824219
70 0.9765731692314148
71 1.2452489137649536
72 1.0869207382202148
73 1.0215502977371216
74 1.1353142261505127
75 1.044050931930542
76 0.9731719493865967
77 0.9261616468429565
78 1.162978172302246
79 1.073306679725647
80 1.0414111614227295
81 1.0709987878799438
82 1.128861427307129
83 0.9633958339691162
84 0.9044146537780762
85 1.118754267692566
86 1.2296372652053833
87 0.9696521162986755
88 0.8847571611404419
89 1.047203540802002
90 1.2342880964279175
91 1.3386967182159424
92 1.249444842338562
93 1.101295828819275
94 1.1364539861679077
95 0.9552145600318909
96 1.0878541469573975
97 1.218397617340088
98 1.081770896911621
99 1.1162017583847046
100 0.9366928339004517
Train Epoch: 5 [10000/20000 (50%)]	Loss: 0.936693
101 1.1812620162963867
102 1.0297755002975464
103 1.0502305030822754
104 1.115689754486084
105 1.0852116346359253
106 1.0584875345230103
107 1.102887511253357
108 0.9745060801506042
109 1.2567497491836548
110 1.0761994123458862
111 1.012495517730713
112 1.0825722217559814
113 1.1819592714309692
114 0.9364109039306641
115 1.1297115087509155
116 1.1229820251464844
117 1.0902022123336792
118 0.9682120680809021
119 0.9512571096420288
120 1.135878086090088
121 1.2472169399261475
122 1.0238134860992432
123 1.1692241430282593
124 1.0296534299850464
125 1.1686129570007324
126 1.174818754196167
127 1.1677833795547485
128 0.9448063373565674
129 1.0901199579238892
130 1.0745474100112915
131 1.0188854932785034
132 0.9614614844322205
133 0.9438983798027039
134 0.9980931878089905
135 0.9498001933097839
136 1.087386131286621
137 0.8567401170730591
138 1.2008056640625
139 1.026986002922058
140 0.9874262809753418
141 1.2141156196594238
142 1.136027455329895
143 1.0818425416946411
144 1.1283828020095825
145 1.084949016571045
146 1.1721514463424683
147 0.9705736041069031
148 1.1781039237976074
149 0.8684563636779785
150 1.0508651733398438
151 1.1171702146530151
152 1.091295599937439
153 1.0547693967819214
154 1.0836299657821655
155 1.031934380531311
156 1.2775100469589233
157 1.0541588068008423
158 1.0648952722549438
159 1.070980429649353
160 1.1569079160690308
161 0.9366253018379211
162 0.8950209021568298
163 1.023460030555725
164 1.0287048816680908
165 1.0073715448379517
166 0.9435457587242126
167 1.0190287828445435
168 0.9457587599754333
169 0.9386917948722839
170 1.124700903892517
171 0.9850090742111206
172 0.9768924713134766
173 0.9445146918296814
174 0.9471076726913452
175 0.9303556680679321
176 1.1249914169311523
177 1.1642513275146484
178 1.1812548637390137
179 1.1264728307724
180 1.0266591310501099
181 0.9628114104270935
182 1.0596513748168945
183 1.0595818758010864
184 0.9646075367927551
185 0.990333080291748
186 0.891685426235199
187 1.0246654748916626
188 1.016581416130066
189 1.0850528478622437
190 1.0207633972167969
191 0.9867700338363647
192 1.0496816635131836
193 1.160272479057312
194 1.144476056098938
195 0.9064968824386597
196 0.8844452500343323
197 0.9971367716789246
198 0.950685441493988
199 1.095202922821045
.............
3830.9213193636388 2470.3090314585716 947.6363795883954
.............
5 3830.9213193636388 2470.3090314585716 947.6363795883954
.............
